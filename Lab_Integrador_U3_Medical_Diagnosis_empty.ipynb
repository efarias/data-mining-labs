{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/efarias/data-mining-labs/blob/main/Lab_Integrador_U3_Medical_Diagnosis_empty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_0"
      },
      "source": [
        "# üè• Laboratorio Integrador U3: Medical Diagnosis Classification\n",
        "## Stack Profesional Completo de Machine Learning\n",
        "\n",
        "**Docente:** Eduardo Far√≠as Reyes  \n",
        "**Curso:** Miner√≠a de Datos (IEI-097)  \n",
        "**Instituci√≥n:** Instituto Profesional Santo Tom√°s\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Objetivos del Laboratorio\n",
        "\n",
        "1. ‚úÖ Realizar **EDA exhaustivo** de datos m√©dicos\n",
        "2. ‚úÖ Implementar **preprocesamiento** sin data leakage\n",
        "3. ‚úÖ Entrenar modelos **b√°sicos y complejos** (5 modelos)\n",
        "4. ‚úÖ Usar **Optuna** para optimizaci√≥n de hiperpar√°metros\n",
        "5. ‚úÖ Registrar experimentos con **MLflow** (UI con ngrok)\n",
        "6. ‚úÖ Generar visualizaciones con **Yellowbrick**\n",
        "7. ‚úÖ Crear reportes con **Evidently AI**\n",
        "8. ‚úÖ Interpretar con **SHAP**\n",
        "9. ‚úÖ Validar con **Deepchecks**\n",
        "\n",
        "‚è±Ô∏è **Tiempo estimado:** 25-35 minutos\n",
        "\n",
        "üìä **Dataset:** UCI Heart Disease (303 pacientes, 13 features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WO_5A_ip5NpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üè• Medical Diagnosis Classification - Laboratorio Integrador\n",
        "\n",
        "**Clasificaci√≥n binaria de enfermedad cardiovascular usando UCI Heart Disease Dataset**\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Metadatos del Notebook\n",
        "\n",
        "| Campo | Valor |\n",
        "|-------|-------|\n",
        "| **Versi√≥n** | `v3.1.0` |\n",
        "| **√öltima Actualizaci√≥n** | `2025-11-14` |\n",
        "| **Estado** | ‚úÖ `Estable`\n",
        "| **Tipo** | Laboratorio Educativo / Experimento / Producci√≥n |\n",
        "| **Nivel** | Intermedio-Avanzado |\n",
        "| **Duraci√≥n Estimada** | 2 horas (ejecuci√≥n completa) |\n",
        "\n",
        "---\n",
        "\n",
        "## üë• Autor√≠a y Propiedad\n",
        "\n",
        "| Campo | Valor |\n",
        "|-------|-------|\n",
        "| **Autor Principal** | Eduardo Far√≠as Reyes |\n",
        "| **Instituci√≥n** | Instituto Profesional Santo Tom√°s, Arica |\n",
        "| **Curso** | Miner√≠a de Datos (IEI-097) |\n",
        "| **Contacto** | efarias4@santotomas.cl |\n",
        "| **Licencia** | `MIT / CC BY-NC-SA 4.0` |\n",
        "| **Repositorio** | https://github.com/efarias/data-mining-labs|\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Objetivos de Aprendizaje\n",
        "\n",
        "1. **Comparaci√≥n de modelos**: Evaluar 5 algoritmos de clasificaci√≥n\n",
        "2. **Optimizaci√≥n**: Aplicar Optuna para HPO autom√°tico  \n",
        "3. **Experiment Tracking**: Usar MLflow para gesti√≥n de experimentos\n",
        "4. **Interpretabilidad**: Generar explicaciones con SHAP\n",
        "5. **Reporting**: Crear reportes autom√°ticos con Evidently AI\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Historial de Versiones\n",
        "\n",
        "| Versi√≥n | Fecha | Cambios | Autor |\n",
        "|---------|-------|---------|-------|\n",
        "| `v3.3.1` | 2025-11-16 | üîß FASE 6: Agregar y_train_final e y_test_final para consistencia | EFR |\n",
        "| `v3.3.0` | 2025-11-16 | ‚úÖ Corregir Deepchecks y Evidently: conversi√≥n robusta a DataFrames | EFR |\n",
        "| `v3.2.0` | 2025-11-16 | ‚úÖ Corregir SHAP: manejo robusto de arrays/DataFrames | EFR |\n",
        "| `v3.1.0` | 2025-11-16 | ‚úÖ Agregar celda configuraci√≥n global (suprimir warnings) | EFR |\n",
        "| `v3.0.0` | 2025-11-16 | üîß **PREPROCESAMIENTO**: Manejo robusto de NaN con SimpleImputer, verificaciones exhaustivas | EFR |\n",
        "| `v2.9.0` | 2025-11-16 | ‚úÖ FASE 5.9: Implementaci√≥n manual de PCA/t-SNE/UMAP (m√°s robusto que Yellowbrick) | EFR |\n",
        "| `v2.8.0` | 2025-11-16 | ‚úÖ FASE 5.9: Agregar imputaci√≥n de NaN para visualizaciones multivariadas | EFR |\n",
        "| `v2.7.0` | 2025-11-15 | ‚úÖ FASE 5.6: Corregir ranking features (workaround pandas.nlargest sin key) | EFR |\n",
        "| `v2.6.0` | 2025-11-15 | üîÑ EDA: Omitir YData-Profiling/Sweetviz por incompatibilidad NumPy 2.0 | EFR |\n",
        "| `v2.5.0` | 2025-11-15 | ‚úÖ Agregar instalaci√≥n UMAP para visualizaciones Manifold | EFR |\n",
        "| `v2.4.0` | 2025-11-15 | üìù Mejorar documentaci√≥n pedag√≥gica en todas las fases | EFR |\n",
        "| `v2.3.0` | 2025-11-14 | ‚úÖ Agregar reportes autom√°ticos EDA (YData-Profiling + Sweetviz) | EFR |\n",
        "| `v2.2.0` | 2025-11-14 | ‚úÖ Implementar an√°lisis cl√≠nico espec√≠fico (FASE 5.7) | EFR |\n",
        "| `v2.1.0` | 2025-11-14 | ‚úÖ Corregir Evidently (usar best_model), agregar SHAP completo | EFR |\n",
        "| `v2.0.0` | 2025-11-13 | ‚úÖ Implementar MLflow con timestamps, corregir warnings | EFR |\n",
        "| `v1.5.0` | 2025-11-13 | ‚úÖ Agregar identificaci√≥n autom√°tica mejor modelo | EFR |\n",
        "| `v1.0.0` | 2025-11-13 | üéâ Versi√≥n inicial con 5 modelos + Optuna | EFR |\n",
        "\n",
        "### Convenci√≥n de Versionamiento (Semantic Versioning)\n",
        "- **MAJOR** (X.0.0): Cambios que rompen compatibilidad\n",
        "- **MINOR** (0.X.0): Nuevas funcionalidades compatibles\n",
        "- **PATCH** (0.0.X): Correcciones de bugs\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Stack Tecnol√≥gico\n",
        "\n",
        "### Modelos\n",
        "- Logistic Regression (baseline)\n",
        "- Random Forest (optimizado con Optuna)\n",
        "- XGBoost (optimizado con Optuna)\n",
        "- Neural Network (Keras/TensorFlow)\n",
        "- Ensemble (Voting Classifier)\n",
        "\n",
        "### Herramientas MLOps\n",
        "| Herramienta | Versi√≥n | Prop√≥sito |\n",
        "|-------------|---------|-----------|\n",
        "| **MLflow** | ‚â•2.10 | Experiment tracking, model registry |\n",
        "| **Optuna** | ‚â•3.5 | Hyperparameter optimization (TPE) |\n",
        "| **SHAP** | ‚â•0.44 | Model interpretability |\n",
        "| **Yellowbrick** | ‚â•1.5 | Visualization |\n",
        "| **Evidently** | ‚â•0.7 | Model evaluation reports |\n",
        "\n",
        "### Core Libraries\n",
        "| Librer√≠a | Versi√≥n M√≠nima | Notas |\n",
        "|----------|----------------|-------|\n",
        "| Python | 3.10+ | Requerido |\n",
        "| scikit-learn | 1.3+ | Core ML |\n",
        "| xgboost | 2.0+ | Gradient boosting |\n",
        "| tensorflow | 2.15+ | Deep learning |\n",
        "| pandas | 2.0+ | Compatibilidad |\n",
        "| numpy | >2.0 | Compatibilidad, ‚ö†Ô∏è issues con Deepchecks |\n",
        "\n",
        "---\n",
        "\n",
        "## üíª Entorno de Ejecuci√≥n\n",
        "\n",
        "### Recomendado\n",
        "- **Plataforma**: Google Colab (GPU T4)\n",
        "- **RAM**: 12GB m√≠nimo\n",
        "- **Tiempo GPU**: ~20 min (Optuna + Neural Network)\n",
        "- **Almacenamiento**: 2GB libres\n",
        "\n",
        "### Alternativas Compatibles\n",
        "- ‚úÖ Jupyter Notebook local (con GPU CUDA)\n",
        "- ‚úÖ Kaggle Kernels\n",
        "- ‚úÖ SageMaker Studio\n",
        "- ‚ö†Ô∏è Databricks (requiere ajustes en MLflow)\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Dataset\n",
        "\n",
        "| Campo | Valor |\n",
        "|-------|-------|\n",
        "| **Nombre** | UCI Heart Disease Dataset |\n",
        "| **Fuente** | UCI Machine Learning Repository |\n",
        "| **Instancias** | 303 pacientes |\n",
        "| **Features** | 13 cl√≠nicos + 1 target |\n",
        "| **Target** | Binario (0: No disease, 1: Disease) |\n",
        "| **Desbalance** | 54% positivos / 46% negativos |\n",
        "| **Missing Values** | Ninguno |\n",
        "| **Link** | [UCI Repository](https://archive.ics.uci.edu/dataset/45/) |\n",
        "\n",
        "---\n",
        "\n",
        "## üî¨ Configuraci√≥n de Experimentos\n",
        "\n",
        "### MLflow\n",
        "```python\n",
        "EXPERIMENT_NAME = \"Heart_Disease_Classification\"\n",
        "MLFLOW_TRACKING_URI = \"ngrok_dynamic\"  # Se genera en runtime\n",
        "ARTIFACT_LOCATION = \"./mlruns\"\n",
        "```\n",
        "\n",
        "### Reproducibilidad\n",
        "```python\n",
        "RANDOM_STATE = 42  # Seed global\n",
        "TRAIN_TEST_SPLIT = 0.2  # 80/20\n",
        "OPTUNA_TRIALS_RF = 30\n",
        "OPTUNA_TRIALS_XGB = 30\n",
        "OPTIMIZATION_METRIC = \"f1_score\"  # M√©trica objetivo\n",
        "```\n",
        "\n",
        "### Performance Targets\n",
        "| M√©trica | Baseline | Target | SOTA |\n",
        "|---------|----------|--------|------|\n",
        "| Accuracy | 0.85 | 0.90 | 0.95+ |\n",
        "| F1-Score | 0.80 | 0.88 | 0.93+ |\n",
        "| ROC-AUC | 0.90 | 0.94 | 0.98+ |\n",
        "\n",
        "---\n",
        "\n",
        "## üìÅ Estructura de Outputs\n",
        "```\n",
        "outputs/\n",
        "‚îú‚îÄ‚îÄ mlruns/                          # MLflow artifacts\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ <experiment_id>/\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ <run_id>/\n",
        "‚îÇ           ‚îú‚îÄ‚îÄ artifacts/\n",
        "‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ model/           # Modelos guardados\n",
        "‚îÇ           ‚îú‚îÄ‚îÄ metrics/             # M√©tricas por step\n",
        "‚îÇ           ‚îî‚îÄ‚îÄ params/              # Hiperpar√°metros\n",
        "‚îú‚îÄ‚îÄ evidently_report.html            # Reporte Evidently\n",
        "‚îú‚îÄ‚îÄ shap_*.png                       # Visualizaciones SHAP\n",
        "‚îú‚îÄ‚îÄ model_comparison.png             # Comparaci√≥n modelos\n",
        "‚îî‚îÄ‚îÄ resumen_ejecutivo.md             # Resumen final\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Limitaciones Conocidas\n",
        "\n",
        "### T√©cnicas\n",
        "1. **Evidently AI**: Cambios en versiones nuevas requieren estudio de implementaci√≥n con documentaci√≥n actualizada.\n",
        "   \n",
        "2. **Deepchecks**: No compatible con NumPy 2.0\n",
        "   - **Estado**: Omitido del notebook (FASE 15 eliminada)\n",
        "   \n",
        "3. **Optuna Trials**: 30 trials puede ser lento sin GPU\n",
        "   - **Workaround**: Reducir a 10-15 trials si T4 no disponible\n",
        "\n",
        "---\n",
        "\n",
        "## üîê Consideraciones de Seguridad\n",
        "\n",
        "| Aspecto | Estado |\n",
        "|---------|--------|\n",
        "| **ngrok Token** | ‚ö†Ô∏è Requerido - No compartir p√∫blicamente |\n",
        "| **Datos sensibles** | ‚úÖ Dataset p√∫blico, sin PII |\n",
        "| **API Keys** | ‚úÖ No requeridas (MLflow local) |\n",
        "| **Outputs** | ‚úÖ Seguros para compartir |\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Referencias y Recursos\n",
        "\n",
        "### Documentaci√≥n Oficial\n",
        "- [MLflow Docs](https://mlflow.org/docs/latest/index.html)\n",
        "- [Optuna Documentation](https://optuna.readthedocs.io/)\n",
        "- [SHAP Documentation](https://shap.readthedocs.io/)\n",
        "- [Evidently AI Docs](https://docs.evidentlyai.com/)\n",
        "\n",
        "### Papers Relevantes\n",
        "1. Detrano et al. (1989) - \"International application of a new probability algorithm for the diagnosis of coronary artery disease\"\n",
        "2. Lundberg & Lee (2017) - \"A Unified Approach to Interpreting Model Predictions\" (SHAP)\n",
        "3. Akiba et al. (2019) - \"Optuna: A Next-generation Hyperparameter Optimization Framework\"\n",
        "\n",
        "### Tutoriales Complementarios\n",
        "- [MLflow Tutorial - Tracking Experiments](https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html)\n",
        "- [Hyperparameter Tuning with Optuna](https://optuna.readthedocs.io/en/stable/tutorial/index.html)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Prerequisites\n",
        "\n",
        "### Conocimientos Previos Requeridos\n",
        "- ‚úÖ Python intermedio (funciones, clases, manejo de errores)\n",
        "- ‚úÖ Pandas y NumPy b√°sico\n",
        "- ‚úÖ Conceptos de ML (train/test, overfitting, m√©tricas)\n",
        "- ‚úÖ Scikit-learn b√°sico\n",
        "\n",
        "### Nice to Have\n",
        "- üìò Experiencia con Jupyter/Colab\n",
        "- üìò Conocimiento de cross-validation\n",
        "- üìò Familiaridad con CLI b√°sico\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Resultados de Aprendizaje Esperados\n",
        "\n",
        "Al completar este laboratorio, los estudiantes podr√°n:\n",
        "\n",
        "1. ‚úÖ **Configurar** un pipeline MLOps completo con MLflow\n",
        "2. ‚úÖ **Implementar** optimizaci√≥n autom√°tica de hiperpar√°metros\n",
        "3. ‚úÖ **Comparar** m√∫ltiples modelos usando m√©tricas objetivas\n",
        "4. ‚úÖ **Interpretar** decisiones de modelos con SHAP\n",
        "5. ‚úÖ **Generar** reportes autom√°ticos de evaluaci√≥n\n",
        "6. ‚úÖ **Documentar** experimentos de forma reproducible\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Proceso de Actualizaci√≥n\n",
        "\n",
        "### Cuando actualizar versi√≥n:\n",
        "- **PATCH**: Correcci√≥n de typos, ajustes menores de c√≥digo\n",
        "- **MINOR**: Agregar nueva visualizaci√≥n, mejorar documentaci√≥n\n",
        "- **MAJOR**: Cambiar modelo principal, modificar estructura de fases\n",
        "\n",
        "### Checklist antes de release:\n",
        "- [ ] Ejecutar notebook completo sin errores\n",
        "- [ ] Actualizar tabla de versiones\n",
        "- [ ] Verificar enlaces externos\n",
        "- [ ] Probar en entorno limpio (runtime restart)\n",
        "- [ ] Actualizar duraci√≥n estimada si aplica\n",
        "- [ ] Documentar breaking changes\n",
        "\n",
        "---\n",
        "\n",
        "## üìû Soporte y Contacto\n",
        "\n",
        "### Para Estudiantes\n",
        "- üí¨ **Foro del curso**: Aula Virtual IEI-097\n",
        "- üìß **Email**: efarias4@santotomas.cl\n",
        "- ‚è∞ **Horario consultas**: Lunes y Martes 18:40-21:30\n",
        "\n",
        "### Para Colaboradores\n",
        "- üêõ **Reportar bugs**: Via email con output de error completo\n",
        "- üí° **Sugerencias**: Abrir issue en repositorio (si aplica)\n",
        "- ü§ù **Contribuciones**: Fork + Pull Request bienvenidos\n",
        "\n",
        "---\n",
        "\n",
        "## üìú Citaci√≥n\n",
        "\n",
        "Si utilizas este notebook en publicaciones acad√©micas:\n",
        "```bibtex\n",
        "@misc{farias2025medical,\n",
        "  title={Medical Diagnosis Classification: Laboratorio Integrador MLOps},\n",
        "  author={Far√≠as, Eduardo},\n",
        "  year={2025},\n",
        "  institution={Instituto Profesional Santo Tom√°s},\n",
        "  course={IEI-097: Miner√≠a de Datos},\n",
        "  version={2.1.0},\n",
        "  url={https://github.com/efarias/data-mining-labs}\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üèÅ Quick Start\n",
        "```python\n",
        "# 1. Abrir en Google Colab\n",
        "# 2. Runtime > Run all (Ctrl+F9)\n",
        "# 3. Esperar ~35 minutos\n",
        "# 4. Revisar MLflow UI (link ngrok en FASE 3)\n",
        "# 5. Descargar outputs desde /mnt/user-data/outputs/\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**√öltima revisi√≥n**: 2025-11-14 | **Pr√≥xima revisi√≥n planificada**: 2025-12-01"
      ],
      "metadata": {
        "id": "RXjOKox2-anG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gfKs8ZILejV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_1"
      },
      "source": [
        "# üì¶ FASE 1: Instalaci√≥n de Dependencias\n",
        "\n",
        "‚è±Ô∏è **Tiempo:** 2-3 minutos\n",
        "\n",
        "Instalando stack profesional de ML:\n",
        "- **MLflow** - Experiment tracking\n",
        "- **Optuna** - Hyperparameter optimization\n",
        "- **Yellowbrick** - ML visualization\n",
        "- **Evidently AI** - Model monitoring\n",
        "- **SHAP** - Model interpretability\n",
        "- **Deepchecks** - Model validation\n",
        "- **XGBoost** - Gradient boosting\n",
        "- **imbalanced-learn** - SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_2"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print(\"üöÄ Instalando dependencias del stack profesional de ML...\")\n",
        "print(\"‚è±Ô∏è Esto puede tomar 2-3 minutos...\\n\")\n",
        "\n",
        "# ============================================\n",
        "# SOLUCI√ìN PARA CONFLICTOS EN COLAB\n",
        "# ============================================\n",
        "# Colab tiene conflictos de numpy/pydantic preinstalados\n",
        "# Instalamos sin versiones espec√≠ficas\n",
        "# Downgrade NumPy para compatibilidad con deepchecks y otras libs\n",
        "\n",
        "import sys\n",
        "\n",
        "# Instalar stack de ML\n",
        "!{sys.executable} -m pip install -q --upgrade pip\n",
        "!{sys.executable} -m pip install -q optuna mlflow yellowbrick evidently\n",
        "!{sys.executable} -m pip install -q shap xgboost imbalanced-learn\n",
        "!{sys.executable} -m pip install -q pyngrok\n",
        "!{sys.executable} -m pip install --upgrade deepchecks\n",
        "\n",
        "print(\"üì¶ Instalando herramientas profesionales para EDA...\")\n",
        "\n",
        "print(\"\\n‚úÖ Instalaci√≥n completada!\")\n",
        "print(\"üìä Stack instalado correctamente\")\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_3"
      },
      "source": [
        "# üéØ FASE 2: Imports y Configuraci√≥n\n",
        "\n",
        "Importando todas las librer√≠as necesarias y configurando:\n",
        "- **Seeds** para reproducibilidad\n",
        "- **Estilos** de visualizaci√≥n\n",
        "- **Warnings** para output limpio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_4"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# CONFIGURACI√ìN GLOBAL - SUPRIMIR WARNINGS (MEJORADO)\n",
        "# ==========================================\n",
        "\n",
        "print(\"üîá Configurando supresi√≥n robusta de warnings...\")\n",
        "\n",
        "import warnings\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# ==========================================\n",
        "# SUPRESI√ìN MULTI-CAPA DE WARNINGS\n",
        "# ==========================================\n",
        "\n",
        "# Capa 1: Suprimir TODOS los warnings globalmente\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Capa 2: Suprimir por categor√≠a (por si acaso)\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=PendingDeprecationWarning)\n",
        "\n",
        "# Capa 3: Suprimir por m√≥dulo espec√≠fico\n",
        "warnings.filterwarnings('ignore', module='sklearn')\n",
        "warnings.filterwarnings('ignore', module='scipy')\n",
        "warnings.filterwarnings('ignore', module='jupyter_client')\n",
        "warnings.filterwarnings('ignore', module='IPython')\n",
        "warnings.filterwarnings('ignore', module='matplotlib')\n",
        "warnings.filterwarnings('ignore', module='seaborn')\n",
        "warnings.filterwarnings('ignore', module='tensorflow')\n",
        "warnings.filterwarnings('ignore', module='keras')\n",
        "\n",
        "# Capa 4: Suprimir warnings espec√≠ficos problem√°ticos\n",
        "warnings.filterwarnings('ignore', message='.*datetime.datetime.utcnow.*')\n",
        "warnings.filterwarnings('ignore', message='.*utcnow.*')\n",
        "warnings.filterwarnings('ignore', message='.*disp.*iprint.*L-BFGS-B.*')\n",
        "\n",
        "# Capa 5: Variables de entorno (nivel sistema)\n",
        "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # TensorFlow warnings\n",
        "\n",
        "# Capa 6: Configurar logging exhaustivo\n",
        "logging.getLogger('jupyter_client').setLevel(logging.CRITICAL)\n",
        "logging.getLogger('jupyter_client.session').setLevel(logging.CRITICAL)\n",
        "logging.getLogger('matplotlib').setLevel(logging.ERROR)\n",
        "logging.getLogger('PIL').setLevel(logging.ERROR)\n",
        "logging.getLogger('urllib3').setLevel(logging.ERROR)\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
        "\n",
        "print(\"‚úÖ Warnings suprimidos correctamente (multi-capa)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ==========================================\n",
        "# IMPORTS DE BIBLIOTECAS\n",
        "# ==========================================\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.float_ = np.float64\n",
        "np.NINF = -np.inf\n",
        "np.Inf = np.inf\n",
        "np.NaN = np.nan\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.io import show\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report,\n",
        "    roc_curve, precision_recall_curve\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn import datasets\n",
        "import sklearn  # Para obtener versi√≥n\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "\n",
        "# Imbalanced data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# MLflow\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import mlflow.keras\n",
        "import mlflow.xgboost\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "# Optuna\n",
        "import optuna\n",
        "from optuna.visualization import (\n",
        "    plot_optimization_history,\n",
        "    plot_param_importances,\n",
        "    plot_parallel_coordinate\n",
        ")\n",
        "\n",
        "# Yellowbrick\n",
        "from yellowbrick.classifier import (\n",
        "    ConfusionMatrix, ROCAUC, ClassificationReport,\n",
        "    PrecisionRecallCurve\n",
        ")\n",
        "\n",
        "# SHAP\n",
        "import shap\n",
        "\n",
        "# Evidently\n",
        "from evidently import Dataset\n",
        "from evidently import DataDefinition\n",
        "from evidently import Report\n",
        "from evidently.presets import DataDriftPreset, DataSummaryPreset\n",
        "\n",
        "# Deepchecks (comentado si causa problemas)\n",
        "# from deepchecks.tabular import Dataset\n",
        "# from deepchecks.tabular.suites import model_evaluation\n",
        "\n",
        "# Utils\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURACI√ìN ADICIONAL POST-IMPORT\n",
        "# ==========================================\n",
        "\n",
        "# Pandas: suprimir SettingWithCopyWarning\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "# NumPy: suprimir warnings num√©ricos\n",
        "np.seterr(all='ignore')\n",
        "\n",
        "# Pandas: opciones de display\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.float_format', '{:.4f}'.format)  # ‚úÖ CORREGIDO: sin comilla extra\n",
        "\n",
        "# NumPy: opciones de print\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURACI√ìN DE REPRODUCIBILIDAD\n",
        "# ==========================================\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "random.seed(RANDOM_STATE)\n",
        "np.random.seed(RANDOM_STATE)\n",
        "tf.random.set_seed(RANDOM_STATE)\n",
        "\n",
        "# Tambi√©n para Keras/TensorFlow\n",
        "try:\n",
        "    tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
        "except:\n",
        "    pass  # Por si la versi√≥n de TF no tiene esta funci√≥n\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURACI√ìN DE VISUALIZACI√ìN\n",
        "# ==========================================\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['figure.max_open_warning'] = 0  # Suprimir warning de figuras abiertas\n",
        "\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configuraci√≥n adicional de matplotlib\n",
        "plt.rcParams.update({\n",
        "    'font.size': 10,\n",
        "    'axes.labelsize': 11,\n",
        "    'axes.titlesize': 12,\n",
        "    'xtick.labelsize': 9,\n",
        "    'ytick.labelsize': 9,\n",
        "    'legend.fontsize': 9,\n",
        "    'figure.titlesize': 14,\n",
        "    'figure.dpi': 100,\n",
        "    'savefig.dpi': 150,\n",
        "    'savefig.bbox': 'tight'\n",
        "})\n",
        "\n",
        "# ==========================================\n",
        "# FUNCIONES UTILITARIAS PARA M√âTRICAS Y MLFLOW\n",
        "# ==========================================\n",
        "\n",
        "def log_model_metrics(y_true, y_pred, y_proba, prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Calcula y registra m√©tricas en MLflow\n",
        "\n",
        "    Args:\n",
        "        y_true: Valores reales\n",
        "        y_pred: Predicciones del modelo\n",
        "        y_proba: Probabilidades predichas\n",
        "        prefix: Prefijo para las m√©tricas (ej: 'train_', 'test_')\n",
        "\n",
        "    Returns:\n",
        "        dict: Diccionario con las m√©tricas calculadas\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        f'{prefix}accuracy': accuracy_score(y_true, y_pred),\n",
        "        f'{prefix}precision': precision_score(y_true, y_pred),\n",
        "        f'{prefix}recall': recall_score(y_true, y_pred),\n",
        "        f'{prefix}f1_score': f1_score(y_true, y_pred),\n",
        "        f'{prefix}roc_auc': roc_auc_score(y_true, y_proba)\n",
        "    }\n",
        "    mlflow.log_metrics(metrics)\n",
        "    return metrics\n",
        "\n",
        "def get_run_name(model_name):\n",
        "    \"\"\"\n",
        "    Genera nombre √∫nico con timestamp para runs de MLflow\n",
        "\n",
        "    Args:\n",
        "        model_name: Nombre base del modelo\n",
        "\n",
        "    Returns:\n",
        "        str: Nombre √∫nico con formato \"{model_name}_{timestamp}\"\n",
        "    \"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    return f\"{model_name}_{timestamp}\"\n",
        "\n",
        "# ==========================================\n",
        "# INFORMACI√ìN DEL ENTORNO\n",
        "# ==========================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üéì LABORATORIO INTEGRADOR U3: MEDICAL DIAGNOSIS CLASSIFICATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üî¢ Random State: {RANDOM_STATE}\")\n",
        "print(f\"üêç Python: {sys.version.split()[0]}\")\n",
        "print(f\"üìä NumPy: {np.__version__}\")\n",
        "print(f\"üìä Pandas: {pd.__version__}\")\n",
        "print(f\"üìä Scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"üìä TensorFlow: {tf.__version__}\")\n",
        "print(f\"üìä MLflow: {mlflow.__version__}\")\n",
        "print(f\"üìä Optuna: {optuna.__version__}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n‚úÖ CONFIGURACI√ìN COMPLETADA:\")\n",
        "print(\"   üîá Warnings suprimidos (6 capas)\")\n",
        "print(\"   üì¶ Bibliotecas importadas\")\n",
        "print(\"   üé≤ Reproducibilidad garantizada (seed=42)\")\n",
        "print(\"   üé® Visualizaci√≥n configurada\")\n",
        "print(\"   üîß Funciones utilitarias cargadas\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ LISTO PARA COMENZAR EL LABORATORIO\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üé® Paleta EF Brand + Yellowbrick (FORZANDO COLORMAPS)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import numpy as np\n",
        "\n",
        "# ============================================\n",
        "# üé® DEFINICI√ìN DE COLORES EF BRAND\n",
        "# ============================================\n",
        "EF_COLORS = {\n",
        "    # Core Brand\n",
        "    'azul_profundo': '#0E254E',\n",
        "    'azul_electrico': '#3BA4F2',\n",
        "    'cyan_luminico': '#5AE0DF',\n",
        "\n",
        "    # Neutros\n",
        "    'gris_carbon': '#1A1A1A',\n",
        "    'gris_grafito': '#272525',\n",
        "    'gris_claro': '#E5E7EB',\n",
        "    'blanco': '#FFFFFF',\n",
        "\n",
        "    # Extendidos\n",
        "    'azul_medio': '#4F8CC9',\n",
        "    'verde_datos': '#22C55E',\n",
        "    'naranja_warning': '#F97316',\n",
        "    'rojo_error': '#EF4444',\n",
        "    'purpura_ia': '#6366F1',\n",
        "    'celeste_claro': '#A7D8FF'\n",
        "}\n",
        "\n",
        "# ============================================\n",
        "# üåà COLORMAPS PERSONALIZADOS EF\n",
        "# ============================================\n",
        "\n",
        "# 1. Colormap para Confusion Matrix (Blanco ‚Üí Azul EF)\n",
        "ef_confusion_colors = [\n",
        "    EF_COLORS['blanco'],\n",
        "    EF_COLORS['celeste_claro'],\n",
        "    EF_COLORS['azul_medio'],\n",
        "    EF_COLORS['azul_electrico'],\n",
        "    EF_COLORS['azul_profundo']\n",
        "]\n",
        "ef_confusion_cmap = LinearSegmentedColormap.from_list('EF_Confusion', ef_confusion_colors, N=256)\n",
        "\n",
        "# 2. Colormap para Classification Report (Rojo ‚Üí Amarillo ‚Üí Verde)\n",
        "ef_classification_colors = [\n",
        "    EF_COLORS['rojo_error'],     # 0.0 - Malo\n",
        "    EF_COLORS['naranja_warning'], # 0.25\n",
        "    '#FCD34D',                    # 0.5 - Amarillo\n",
        "    '#86EFAC',                    # 0.75 - Verde claro\n",
        "    EF_COLORS['verde_datos']      # 1.0 - Excelente\n",
        "]\n",
        "ef_classification_cmap = LinearSegmentedColormap.from_list('EF_Classification', ef_classification_colors, N=256)\n",
        "\n",
        "# 3. Paleta discreta\n",
        "EF_DISCRETE_PALETTE = [\n",
        "    EF_COLORS['azul_electrico'],\n",
        "    EF_COLORS['cyan_luminico'],\n",
        "    EF_COLORS['purpura_ia'],\n",
        "    EF_COLORS['verde_datos'],\n",
        "    EF_COLORS['naranja_warning'],\n",
        "    EF_COLORS['azul_medio']\n",
        "]\n",
        "\n",
        "# ============================================\n",
        "# üé® CONFIGURACI√ìN GLOBAL\n",
        "# ============================================\n",
        "def setup_ef_style():\n",
        "    \"\"\"Configura el estilo global de matplotlib con branding EF\"\"\"\n",
        "    plt.rcParams.update({\n",
        "        'figure.facecolor': 'white',\n",
        "        'axes.facecolor': 'white',\n",
        "        'axes.edgecolor': EF_COLORS['gris_grafito'],\n",
        "        'axes.labelcolor': EF_COLORS['gris_carbon'],\n",
        "        'text.color': EF_COLORS['gris_carbon'],\n",
        "        'xtick.color': EF_COLORS['gris_grafito'],\n",
        "        'ytick.color': EF_COLORS['gris_grafito'],\n",
        "        'grid.color': EF_COLORS['gris_claro'],\n",
        "        'grid.linestyle': '--',\n",
        "        'grid.alpha': 0.6,\n",
        "        'axes.prop_cycle': plt.cycler(color=EF_DISCRETE_PALETTE),\n",
        "        'font.family': 'sans-serif',\n",
        "        'font.sans-serif': ['Arial', 'Helvetica', 'DejaVu Sans'],\n",
        "        'font.size': 11,\n",
        "        'axes.titlesize': 13,\n",
        "        'axes.labelsize': 11,\n",
        "        'figure.titlesize': 16\n",
        "    })\n",
        "\n",
        "setup_ef_style()\n",
        "\n",
        "# ============================================\n",
        "# üìä FUNCI√ìN YELLOWBRICK CON BRANDING EF\n",
        "# ============================================\n",
        "def yellowbrick_ef_brand(model, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Yellowbrick con paleta profesional EF Brand\n",
        "    \"\"\"\n",
        "    from yellowbrick.classifier import (\n",
        "        ConfusionMatrix, ROCAUC, ClassificationReport,\n",
        "        PrecisionRecallCurve, ClassPredictionError, DiscriminationThreshold\n",
        "    )\n",
        "    from yellowbrick.model_selection import LearningCurve, ValidationCurve\n",
        "\n",
        "    # ========================================\n",
        "    # GRID 1: M√©tricas B√°sicas (2x2)\n",
        "    # ========================================\n",
        "    fig1, axes1 = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig1.patch.set_facecolor('white')\n",
        "\n",
        "    fig1.suptitle(\n",
        "        f'üìä Evaluaci√≥n B√°sica - {model_name}',\n",
        "        fontsize=18, fontweight='bold', y=1.00,\n",
        "        color=EF_COLORS['azul_profundo']\n",
        "    )\n",
        "\n",
        "    # 1.1 Confusion Matrix\n",
        "    cm_viz = ConfusionMatrix(\n",
        "        model,\n",
        "        classes=['No Disease', 'Disease'],\n",
        "        ax=axes1[0,0],\n",
        "        cmap=ef_confusion_cmap,\n",
        "        fontsize=12\n",
        "    )\n",
        "    cm_viz.fit(X_train_final, y_train_final)\n",
        "    cm_viz.score(X_test_final, y_test)\n",
        "    cm_viz.finalize()\n",
        "\n",
        "    # ‚úÖ FORZAR cambio de colormap despu√©s de finalize()\n",
        "    for im in axes1[0,0].get_images():\n",
        "        im.set_cmap(ef_confusion_cmap)\n",
        "\n",
        "    axes1[0,0].set_title('Matriz de Confusi√≥n', fontweight='bold', pad=10,\n",
        "                          color=EF_COLORS['azul_profundo'])\n",
        "\n",
        "    # 1.2 ROC-AUC Curve\n",
        "    roc = ROCAUC(\n",
        "        model,\n",
        "        classes=['No Disease', 'Disease'],\n",
        "        ax=axes1[0,1],\n",
        "        micro=False,\n",
        "        macro=False,\n",
        "        per_class=True\n",
        "    )\n",
        "    roc.fit(X_train_final, y_train_final)\n",
        "    roc.score(X_test_scaled, y_test)\n",
        "    roc.finalize()\n",
        "\n",
        "    # ‚úÖ Cambiar colores DESPU√âS de finalize()\n",
        "    lines = axes1[0,1].get_lines()\n",
        "    if len(lines) >= 2:\n",
        "        lines[0].set_color(EF_COLORS['azul_electrico'])\n",
        "        lines[0].set_linewidth(2.5)\n",
        "        lines[1].set_color(EF_COLORS['cyan_luminico'])\n",
        "        lines[1].set_linewidth(2.5)\n",
        "\n",
        "    axes1[0,1].set_title('Curva ROC-AUC', fontweight='bold', pad=10,\n",
        "                          color=EF_COLORS['azul_profundo'])\n",
        "    axes1[0,1].grid(True, alpha=0.3, color=EF_COLORS['gris_claro'])\n",
        "\n",
        "    # 1.3 Precision-Recall Curve\n",
        "    prc = PrecisionRecallCurve(\n",
        "        model,\n",
        "        classes=['No Disease', 'Disease'],\n",
        "        ax=axes1[1,0],\n",
        "        micro=False,\n",
        "        per_class=True\n",
        "    )\n",
        "    prc.fit(X_train_final, y_train_final)\n",
        "    prc.score(X_test_scaled, y_test)\n",
        "    prc.finalize()\n",
        "\n",
        "    # ‚úÖ Cambiar colores\n",
        "    lines = axes1[1,0].get_lines()\n",
        "    if len(lines) >= 2:\n",
        "        lines[0].set_color(EF_COLORS['purpura_ia'])\n",
        "        lines[0].set_linewidth(2.5)\n",
        "        lines[1].set_color(EF_COLORS['verde_datos'])\n",
        "        lines[1].set_linewidth(2.5)\n",
        "\n",
        "    axes1[1,0].set_title('Curva Precision-Recall', fontweight='bold', pad=10,\n",
        "                          color=EF_COLORS['azul_profundo'])\n",
        "    axes1[1,0].grid(True, alpha=0.3, color=EF_COLORS['gris_claro'])\n",
        "\n",
        "    # 1.4 Classification Report\n",
        "    cr = ClassificationReport(\n",
        "        model,\n",
        "        classes=['No Disease', 'Disease'],\n",
        "        ax=axes1[1,1],\n",
        "        cmap=ef_classification_cmap,\n",
        "        support=True\n",
        "    )\n",
        "    cr.fit(X_train_final, y_train_final)\n",
        "    cr.score(X_test_scaled, y_test)\n",
        "    cr.finalize()\n",
        "\n",
        "    # ‚úÖ FORZAR cambio de colormap despu√©s de finalize()\n",
        "    for im in axes1[1,1].get_images():\n",
        "        im.set_cmap(ef_classification_cmap)\n",
        "\n",
        "    axes1[1,1].set_title('Reporte de Clasificaci√≥n', fontweight='bold', pad=10,\n",
        "                          color=EF_COLORS['azul_profundo'])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\n",
        "        f'{model_name.lower().replace(\" \", \"_\")}_yellowbrick_ef_basic.png',\n",
        "        dpi=300, bbox_inches='tight', facecolor='white'\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    # ========================================\n",
        "    # GRID 2: An√°lisis Avanzado (2x2)\n",
        "    # ========================================\n",
        "    fig2, axes2 = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig2.patch.set_facecolor('white')\n",
        "\n",
        "    fig2.suptitle(\n",
        "        f'üîç An√°lisis Avanzado - {model_name}',\n",
        "        fontsize=18, fontweight='bold', y=1.00,\n",
        "        color=EF_COLORS['azul_profundo']\n",
        "    )\n",
        "\n",
        "    # 2.1 Class Prediction Error\n",
        "    cpe = ClassPredictionError(\n",
        "        model,\n",
        "        classes=['No Disease', 'Disease'],\n",
        "        ax=axes2[0,0]\n",
        "    )\n",
        "    cpe.fit(X_train_final, y_train_final)\n",
        "    cpe.score(X_test_scaled, y_test)\n",
        "    cpe.finalize()\n",
        "\n",
        "    # ‚úÖ Cambiar colores de las barras\n",
        "    for i, bar_container in enumerate(axes2[0,0].containers):\n",
        "        if i == 0:\n",
        "            for bar in bar_container:\n",
        "                bar.set_facecolor(EF_COLORS['azul_electrico'])\n",
        "                bar.set_edgecolor(EF_COLORS['azul_profundo'])\n",
        "                bar.set_linewidth(1.5)\n",
        "        elif i == 1:\n",
        "            for bar in bar_container:\n",
        "                bar.set_facecolor(EF_COLORS['cyan_luminico'])\n",
        "                bar.set_edgecolor(EF_COLORS['azul_profundo'])\n",
        "                bar.set_linewidth(1.5)\n",
        "\n",
        "    axes2[0,0].set_title('Errores de Predicci√≥n por Clase', fontweight='bold', pad=10,\n",
        "                          color=EF_COLORS['azul_profundo'])\n",
        "\n",
        "    # 2.2 Discrimination Threshold\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        dt = DiscriminationThreshold(model, ax=axes2[0,1])\n",
        "        dt.fit(X_train_final, y_train_final)\n",
        "        dt.score(X_test_scaled, y_test)\n",
        "        dt.finalize()\n",
        "\n",
        "        # ‚úÖ Cambiar colores\n",
        "        for line in axes2[0,1].get_lines():\n",
        "            label = line.get_label().lower()\n",
        "            if 'precision' in label:\n",
        "                line.set_color(EF_COLORS['verde_datos'])\n",
        "                line.set_linewidth(2.5)\n",
        "            elif 'recall' in label or 'sensitivity' in label:\n",
        "                line.set_color(EF_COLORS['azul_electrico'])\n",
        "                line.set_linewidth(2.5)\n",
        "            elif 'f1' in label or 'fscore' in label:\n",
        "                line.set_color(EF_COLORS['purpura_ia'])\n",
        "                line.set_linewidth(2.5)\n",
        "            elif 'queue' in label:\n",
        "                line.set_color(EF_COLORS['cyan_luminico'])\n",
        "                line.set_linewidth(2.5)\n",
        "\n",
        "        axes2[0,1].set_title('An√°lisis de Umbrales de Decisi√≥n', fontweight='bold', pad=10,\n",
        "                              color=EF_COLORS['azul_profundo'])\n",
        "        axes2[0,1].grid(True, alpha=0.3, color=EF_COLORS['gris_claro'])\n",
        "    else:\n",
        "        axes2[0,1].text(0.5, 0.5, 'Modelo no soporta predict_proba',\n",
        "                        ha='center', va='center', fontsize=14,\n",
        "                        color=EF_COLORS['gris_grafito'])\n",
        "        axes2[0,1].axis('off')\n",
        "\n",
        "    # 2.3 Learning Curve\n",
        "    print(f\"   üîÑ Generando Learning Curve para {model_name}...\")\n",
        "    lc = LearningCurve(\n",
        "        model, cv=5, scoring='f1',\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "        ax=axes2[1,0], n_jobs=-1\n",
        "    )\n",
        "    lc.fit(X_train_final, y_train_final)\n",
        "    lc.finalize()\n",
        "\n",
        "    # ‚úÖ Cambiar colores\n",
        "    for line in axes2[1,0].get_lines():\n",
        "        label = line.get_label().lower()\n",
        "        if 'training' in label or 'train' in label:\n",
        "            line.set_color(EF_COLORS['azul_electrico'])\n",
        "            line.set_linewidth(2.5)\n",
        "        elif 'cross' in label or 'validation' in label or 'test' in label:\n",
        "            line.set_color(EF_COLORS['verde_datos'])\n",
        "            line.set_linewidth(2.5)\n",
        "\n",
        "    axes2[1,0].set_title('Curva de Aprendizaje (5-Fold CV)', fontweight='bold', pad=10,\n",
        "                          color=EF_COLORS['azul_profundo'])\n",
        "    axes2[1,0].grid(True, alpha=0.3, color=EF_COLORS['gris_claro'])\n",
        "\n",
        "    # 2.4 Validation Curve\n",
        "    if hasattr(model, 'max_depth'):\n",
        "        param_name = \"max_depth\"\n",
        "        param_range = np.arange(1, 21, 2)\n",
        "        print(f\"   üìà Generando Validation Curve para {param_name}...\")\n",
        "\n",
        "        vc = ValidationCurve(\n",
        "            model, param_name=param_name, param_range=param_range,\n",
        "            cv=5, scoring='f1', ax=axes2[1,1], n_jobs=-1\n",
        "        )\n",
        "        vc.fit(X_train_final, y_train_final)\n",
        "        vc.finalize()\n",
        "\n",
        "        # ‚úÖ Cambiar colores\n",
        "        for line in axes2[1,1].get_lines():\n",
        "            label = line.get_label().lower()\n",
        "            if 'training' in label or 'train' in label:\n",
        "                line.set_color(EF_COLORS['azul_electrico'])\n",
        "                line.set_linewidth(2.5)\n",
        "            elif 'cross' in label or 'validation' in label:\n",
        "                line.set_color(EF_COLORS['verde_datos'])\n",
        "                line.set_linewidth(2.5)\n",
        "\n",
        "        axes2[1,1].set_title(f'Validation Curve ({param_name})', fontweight='bold', pad=10,\n",
        "                              color=EF_COLORS['azul_profundo'])\n",
        "        axes2[1,1].grid(True, alpha=0.3, color=EF_COLORS['gris_claro'])\n",
        "    else:\n",
        "        axes2[1,1].text(0.5, 0.5, 'Validation Curve no aplicable',\n",
        "                        ha='center', va='center', fontsize=14,\n",
        "                        color=EF_COLORS['gris_grafito'])\n",
        "        axes2[1,1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\n",
        "        f'{model_name.lower().replace(\" \", \"_\")}_yellowbrick_ef_advanced.png',\n",
        "        dpi=300, bbox_inches='tight', facecolor='white'\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"‚úÖ Yellowbrick EF Brand completo para {model_name}\")\n",
        "\n",
        "# ============================================\n",
        "# üé® PREVIEW DE PALETA\n",
        "# ============================================\n",
        "def preview_ef_palette():\n",
        "    \"\"\"Muestra preview de colores EF\"\"\"\n",
        "    fig = plt.figure(figsize=(16, 10))\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)\n",
        "\n",
        "    fig.suptitle('üé® Sistema de Color EF Brand', fontsize=20, fontweight='bold',\n",
        "                 color=EF_COLORS['azul_profundo'], y=0.98)\n",
        "\n",
        "    # 1. Core Colors\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    core = ['azul_profundo', 'azul_electrico', 'cyan_luminico']\n",
        "    ax1.bar(range(len(core)), [1]*3, color=[EF_COLORS[c] for c in core], width=0.8)\n",
        "    ax1.set_xticks(range(3))\n",
        "    ax1.set_xticklabels(['Azul\\nProfundo', 'Azul\\nEl√©ctrico', 'Cyan\\nLum√≠nico'])\n",
        "    ax1.set_title('Core Brand', fontweight='bold', color=EF_COLORS['azul_profundo'])\n",
        "    ax1.set_ylim(0, 1.2)\n",
        "    ax1.set_yticks([])\n",
        "\n",
        "    # 2. Extended Palette\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    extended = ['azul_medio', 'verde_datos', 'purpura_ia', 'naranja_warning', 'rojo_error']\n",
        "    ax2.bar(range(len(extended)), [1]*5, color=[EF_COLORS[c] for c in extended], width=0.8)\n",
        "    ax2.set_xticks(range(5))\n",
        "    ax2.set_xticklabels(['Azul\\nMedio', 'Verde\\nDatos', 'P√∫rpura\\nIA', 'Naranja\\nAlert', 'Rojo\\nError'], fontsize=9)\n",
        "    ax2.set_title('Extended (Gr√°ficos)', fontweight='bold', color=EF_COLORS['azul_profundo'])\n",
        "    ax2.set_ylim(0, 1.2)\n",
        "    ax2.set_yticks([])\n",
        "\n",
        "    # 3. Neutros\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    neutros = ['gris_carbon', 'gris_grafito', 'gris_claro', 'blanco']\n",
        "    ax3.bar(range(len(neutros)), [1]*4, color=[EF_COLORS[c] for c in neutros],\n",
        "            width=0.8, edgecolor=EF_COLORS['gris_grafito'], linewidth=1.5)\n",
        "    ax3.set_xticks(range(4))\n",
        "    ax3.set_xticklabels(['Carb√≥n', 'Grafito', 'Claro', 'Blanco'], fontsize=9)\n",
        "    ax3.set_title('Neutros', fontweight='bold', color=EF_COLORS['azul_profundo'])\n",
        "    ax3.set_ylim(0, 1.2)\n",
        "    ax3.set_yticks([])\n",
        "\n",
        "    # 4. Confusion Matrix Colormap\n",
        "    ax4 = fig.add_subplot(gs[1, :])\n",
        "    gradient = np.linspace(0, 1, 256).reshape(1, -1)\n",
        "    ax4.imshow(gradient, aspect='auto', cmap=ef_confusion_cmap)\n",
        "    ax4.set_title('Colormap: Confusion Matrix (Blanco ‚Üí Azul EF)',\n",
        "                  fontweight='bold', pad=15, color=EF_COLORS['azul_profundo'])\n",
        "    ax4.set_xticks([0, 64, 128, 192, 255])\n",
        "    ax4.set_xticklabels(['Bajo', '', 'Medio', '', 'Alto'])\n",
        "    ax4.set_yticks([])\n",
        "\n",
        "    # 5. Classification Report Colormap\n",
        "    ax5 = fig.add_subplot(gs[2, :])\n",
        "    ax5.imshow(gradient, aspect='auto', cmap=ef_classification_cmap)\n",
        "    ax5.set_title('Colormap: Classification Report (Rojo ‚Üí Amarillo ‚Üí Verde)',\n",
        "                  fontweight='bold', pad=15, color=EF_COLORS['azul_profundo'])\n",
        "    ax5.set_xticks([0, 64, 128, 192, 255])\n",
        "    ax5.set_xticklabels(['Pobre', 'Regular', 'Bueno', 'Muy Bueno', 'Excelente'])\n",
        "    ax5.set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('ef_brand_palette_system.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "    plt.show()\n",
        "\n",
        "# ============================================\n",
        "print(\"‚úÖ Paleta EF Brand cargada correctamente\")\n",
        "print(\"üìä Uso: yellowbrick_ef_brand(modelo, 'Nombre')\")\n",
        "print(\"üé® Preview: preview_ef_palette()\")"
      ],
      "metadata": {
        "id": "kfCY18wnrLrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_5"
      },
      "source": [
        "# üöÄ FASE 3: MLflow Configuraci√≥n con ngrok\n",
        "\n",
        "## ¬øQu√© es MLflow?\n",
        "**MLflow** es una plataforma open source para gestionar el ciclo de vida de ML:\n",
        "- **Tracking**: Registra par√°metros, m√©tricas, modelos\n",
        "- **Projects**: C√≥digo reproducible\n",
        "- **Models**: Formato est√°ndar para deployment\n",
        "- **Registry**: Almac√©n centralizado\n",
        "\n",
        "## ¬øQu√© es ngrok?\n",
        "**ngrok** crea un t√∫nel para exponer la UI de MLflow (localhost) a trav√©s de una URL p√∫blica temporal.\n",
        "\n",
        "**Flujo:**\n",
        "```\n",
        "MLflow UI (localhost:5000) ‚Üí ngrok ‚Üí URL p√∫blica\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 3: CONFIGURACI√ìN DE MLFLOW MEJORADA\n",
        "# ==========================================\n",
        "\n",
        "print(\"üöÄ FASE 3: Configuraci√≥n de MLflow con Estructura Profesional\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë                    MLFLOW - ORGANIZACI√ìN PROFESIONAL                       ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "üéØ MEJORAS IMPLEMENTADAS:\n",
        "\n",
        "1. EXPERIMENTOS ORGANIZADOS:\n",
        "   ‚Ä¢ Agrupa todos los runs bajo un experimento √∫nico\n",
        "   ‚Ä¢ Facilita comparaci√≥n entre modelos\n",
        "   ‚Ä¢ Simula estructura de proyectos reales\n",
        "\n",
        "2. TAGS DESCRIPTIVOS:\n",
        "   ‚Ä¢ Metadata rica para filtrado\n",
        "   ‚Ä¢ Informaci√≥n de contexto (dataset, laboratorio, autor)\n",
        "   ‚Ä¢ Clasificaci√≥n por familia de modelos\n",
        "\n",
        "3. TRACKING URI PERSISTENTE:\n",
        "   ‚Ä¢ Configuraci√≥n centralizada\n",
        "   ‚Ä¢ F√°cil migraci√≥n a servidores remotos\n",
        "\n",
        "üí° VALOR: Simula MLOps profesional desde el inicio\n",
        "\"\"\")\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import threading\n",
        "import os\n",
        "import time\n",
        "import mlflow\n",
        "from getpass import getpass\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURACI√ìN DE EXPERIMENTO\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä CONFIGURANDO EXPERIMENTO\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# 1. Definir nombre del experimento\n",
        "EXPERIMENT_NAME = \"Lab_U3_Medical_Diagnosis\"\n",
        "\n",
        "# 2. Crear o recuperar experimento\n",
        "experiment = mlflow.set_experiment(EXPERIMENT_NAME)\n",
        "\n",
        "print(f\"‚úÖ Experimento configurado: '{EXPERIMENT_NAME}'\")\n",
        "print(f\"   Experiment ID: {experiment.experiment_id}\")\n",
        "print(f\"   Artifact Location: {experiment.artifact_location}\")\n",
        "\n",
        "# ==========================================\n",
        "# TAGS GLOBALES DEL PROYECTO\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüè∑Ô∏è CONFIGURANDO TAGS GLOBALES\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Tags aplicables a todo el experimento\n",
        "PROJECT_TAGS = {\n",
        "    # Contexto acad√©mico\n",
        "    \"project\": \"Medical Diagnosis Classification\",\n",
        "    \"course\": \"IEI-097 Miner√≠a de Datos\",\n",
        "    \"institution\": \"Instituto Profesional Santo Tom√°s\",\n",
        "    \"lab\": \"U3 - Laboratorio Integrador\",\n",
        "\n",
        "    # Contexto t√©cnico\n",
        "    \"dataset\": \"UCI Heart Disease\",\n",
        "    \"dataset_size\": \"303 samples\",\n",
        "    \"problem_type\": \"binary_classification\",\n",
        "\n",
        "    # Metadata del proyecto\n",
        "    \"author\": \"Eduardo Far√≠as\",\n",
        "    \"version\": \"2.3.0\",\n",
        "    \"environment\": \"Google Colab GPU T4\",\n",
        "\n",
        "    # Objetivo\n",
        "    \"target_metric\": \"f1_score\",\n",
        "    \"optimization_tool\": \"optuna\"\n",
        "}\n",
        "\n",
        "print(\"üìã Tags configurados:\")\n",
        "for key, value in PROJECT_TAGS.items():\n",
        "    print(f\"   ‚Ä¢ {key:20s}: {value}\")\n",
        "\n",
        "# ==========================================\n",
        "# FUNCI√ìN HELPER PARA RUNS\n",
        "# ==========================================\n",
        "\n",
        "def start_mlflow_run(model_name, model_family, additional_tags=None):\n",
        "    \"\"\"\n",
        "    Inicia un MLflow run con naming y tagging consistente\n",
        "\n",
        "    Args:\n",
        "        model_name: Nombre del modelo (ej: \"Logistic Regression\")\n",
        "        model_family: Familia del modelo para clasificaci√≥n\n",
        "                     (\"linear\", \"tree_based\", \"neural_network\", \"ensemble\")\n",
        "        additional_tags: Dict con tags adicionales espec√≠ficos del modelo\n",
        "\n",
        "    Returns:\n",
        "        run_name: Nombre √∫nico del run\n",
        "    \"\"\"\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Generar run_name √∫nico\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_name = f\"{model_name}_{timestamp}\"\n",
        "\n",
        "    # Iniciar run\n",
        "    mlflow.start_run(run_name=run_name)\n",
        "\n",
        "    # Tags base (del proyecto)\n",
        "    all_tags = PROJECT_TAGS.copy()\n",
        "\n",
        "    # Tags espec√≠ficos del modelo\n",
        "    all_tags.update({\n",
        "        \"model_name\": model_name,\n",
        "        \"model_family\": model_family,\n",
        "        \"run_timestamp\": timestamp\n",
        "    })\n",
        "\n",
        "    # Tags adicionales si se proveen\n",
        "    if additional_tags:\n",
        "        all_tags.update(additional_tags)\n",
        "\n",
        "    # Aplicar todos los tags\n",
        "    mlflow.set_tags(all_tags)\n",
        "\n",
        "    print(f\"‚úÖ MLflow run iniciado: {run_name}\")\n",
        "    print(f\"   Familia: {model_family}\")\n",
        "\n",
        "    return run_name\n",
        "\n",
        "print(\"\\n‚úÖ Funci√≥n helper configurada: start_mlflow_run()\")\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURACI√ìN DE TRACKING URI (ngrok)\n",
        "# ==========================================\n",
        "# Crear directorio\n",
        "# Esto crear√° una carpeta 'mlflow-lab' en tu unidad de Drive\n",
        "# y dentro guardar√° la carpeta 'mlruns' con todos los logs.\n",
        "tracking_uri = \"file:/content/drive/MyDrive/mlflow-lab\"\n",
        "mlflow.set_tracking_uri(tracking_uri)\n",
        "\n",
        "# Configurar authtoken (solo si no est√°)\n",
        "try:\n",
        "    ngrok.get_ngrok_process()\n",
        "except:\n",
        "    print(\"\\nüîê Configurando ngrok authtoken...\")\n",
        "    print(\"\\n https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "    token = getpass(\"Ingresa tu ngrok authtoken: \")\n",
        "    ngrok.set_auth_token(token)\n",
        "    print(\"Authtoken configurado\")\n",
        "\n",
        "# Iniciar MLflow UI\n",
        "def start_mlflow():\n",
        "    subprocess.run([\n",
        "        \"mlflow\", \"ui\",\n",
        "        \"--backend-store-uri\", \"/content/drive/MyDrive/mlflow-lab\",\n",
        "        \"--host\", \"127.0.0.1\",\n",
        "        \"--port\", \"5000\"\n",
        "    ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "print(\"\\nIniciando MLflow UI...\")\n",
        "thread = threading.Thread(target=start_mlflow, daemon=True)\n",
        "thread.start()\n",
        "time.sleep(8)\n",
        "\n",
        "# T√öNEL CON HOST-HEADER (LA CLAVE)\n",
        "print(\"Creando t√∫nel ngrok con --host-header...\")\n",
        "try:\n",
        "    # Forzamos Host: localhost:5000\n",
        "    public_url = ngrok.connect(\n",
        "        addr=\"5000\",\n",
        "        proto=\"http\",\n",
        "        host_header=\"localhost:5000\"  # ¬°ESTO ES EL FIX!\n",
        "    )\n",
        "    print(f\"T√∫nel creado: {public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "print(f\"\\n‚úÖ MLFLOW UI DISPONIBLE:\")\n",
        "print(f\"   üîó URL P√∫blica: {public_url}\")\n",
        "print(f\"   üìä Experimento: {EXPERIMENT_NAME}\")\n",
        "print(f\"\\nüí° Abre esta URL para ver experimentos en tiempo real\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "hgzhS-eBpy85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_7"
      },
      "source": [
        "# üìä FASE 4: Carga y Exploraci√≥n de Datos\n",
        "\n",
        "## Dataset: Heart Disease (UCI)\n",
        "- **Fuente:** UCI Machine Learning Repository\n",
        "- **Instancias:** ~303 pacientes\n",
        "- **Features:** 13 caracter√≠sticas cl√≠nicas\n",
        "- **Target:** Binario (0=sin enfermedad, 1=con enfermedad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_8"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print(\"üì• CARGANDO DATASET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'\n",
        "\n",
        "column_names = [\n",
        "    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
        "    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'\n",
        "]\n",
        "\n",
        "df = pd.read_csv(url, names=column_names, na_values='?')\n",
        "df['target'] = (df['target'] > 0).astype(int)\n",
        "\n",
        "print(f\"‚úÖ Dataset cargado: {df.shape}\")\n",
        "print(f\"\\nüéØ Distribuci√≥n target:\")\n",
        "print(df['target'].value_counts())\n",
        "print(f\"\\nBalance ratio: {df['target'].value_counts().min() / df['target'].value_counts().max():.3f}\")\n",
        "\n",
        "print(\"\\nüìã Primeras filas:\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\nüìä Informaci√≥n del dataset:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\nüìà Estad√≠sticas descriptivas:\")\n",
        "display(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_9"
      },
      "source": [
        "# üîç FASE 5: An√°lisis Exploratorio de Datos (EDA) - Profesional\n",
        "\n",
        "## ¬øQu√© es el EDA?\n",
        "\n",
        "El **An√°lisis Exploratorio de Datos (EDA)** es el proceso sistem√°tico de investigar datasets para descubrir patrones, detectar anomal√≠as, verificar hip√≥tesis y validar suposiciones **antes** de aplicar t√©cnicas de modelado.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Objetivos del EDA Profesional\n",
        "\n",
        "1. **Validar calidad**: Missing values, duplicados, tipos de datos\n",
        "2. **Entender distribuciones**: Normalidad, sesgo, outliers\n",
        "3. **Identificar relaciones**: Correlaciones, features discriminativas\n",
        "4. **Detectar problemas**: Desbalance, multicolinealidad, data leakage\n",
        "5. **Generar insights**: Patrones cl√≠nicos accionables\n",
        "6. **Automatizar an√°lisis**: Reportes profesionales para stakeholders\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Stack Tecnol√≥gico Profesional\n",
        "\n",
        "| Herramienta | Prop√≥sito | Nivel | Uso en Industria |\n",
        "|-------------|-----------|-------|------------------|\n",
        "| **Pandas** | Manipulaci√≥n de datos | B√°sico | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Universal |\n",
        "| **Matplotlib/Seaborn** | Visualizaciones est√°ticas | B√°sico | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Universal |\n",
        "| **Yellowbrick** | Visualizaciones ML-ready | Avanzado | ‚≠ê‚≠ê‚≠ê‚≠ê Data Science |\n",
        "| **SciPy** | Tests estad√≠sticos | Avanzado | ‚≠ê‚≠ê‚≠ê‚≠ê Cient√≠fico |\n",
        "| **YData-Profiling** | Reportes autom√°ticos | Profesional | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê MLOps |\n",
        "| **Sweetviz** | Comparaciones r√°pidas | Profesional | ‚≠ê‚≠ê‚≠ê‚≠ê Data Analytics |\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Estructura del An√°lisis\n",
        "\n",
        "### **An√°lisis Manual (Detallado)**\n",
        "- 5.0: Setup y configuraci√≥n\n",
        "- 5.1: Informaci√≥n general y calidad\n",
        "- 5.2: Balance de clases (Yellowbrick)\n",
        "- 5.3: Distribuciones univariadas\n",
        "- 5.4: Ranking de features (Yellowbrick)\n",
        "- 5.5: Detecci√≥n de outliers\n",
        "- 5.6: An√°lisis bivariado\n",
        "- 5.7: An√°lisis cl√≠nico espec√≠fico\n",
        "- 5.8: Correlaciones\n",
        "- 5.9: Visualizaciones multivariadas (Yellowbrick)\n",
        "\n",
        "### **Reportes Autom√°ticos (Profesional)**\n",
        "- 5.10: YData-Profiling (reporte HTML completo)\n",
        "- 5.11: Sweetviz (comparaci√≥n Disease vs No Disease)\n",
        "- 5.12: Resumen ejecutivo\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Valor Educativo\n",
        "\n",
        "**Enfoque H√≠brido:**\n",
        "- ‚úÖ An√°lisis manual ‚Üí Entender **c√≥mo** y **por qu√©**\n",
        "- ‚úÖ Reportes autom√°ticos ‚Üí Eficiencia y presentaci√≥n profesional\n",
        "\n",
        "**Graduados de este curso:**\n",
        "- Dominan an√°lisis exploratorio manual\n",
        "- Conocen herramientas de automatizaci√≥n\n",
        "- Pueden generar reportes para stakeholders no t√©cnicos\n",
        "- Est√°n listos para ambientes corporativos\n",
        "\n",
        "---\n",
        "\n",
        "‚è±Ô∏è **Tiempo total:** 15-20 minutos  \n",
        "üìä **Outputs:** ~25 visualizaciones + 2 reportes HTML interactivos  \n",
        "üéØ **Nivel:** Profesional / MLOps-ready"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# INSTALACI√ìN DE HERRAMIENTAS PROFESIONALES\n",
        "# ==========================================\n",
        "\n",
        "print(\"üì¶ INSTALANDO HERRAMIENTAS PROFESIONALES PARA EDA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "üõ†Ô∏è HERRAMIENTAS A INSTALAR:\n",
        "\n",
        "1. Yellowbrick (ya instalado)\n",
        "   ‚Üí Visualizaciones ML-ready\n",
        "\n",
        "2. YData-Profiling (antes pandas-profiling)\n",
        "   ‚Üí Reportes HTML autom√°ticos completos\n",
        "   ‚Üí An√°lisis de interacciones\n",
        "   ‚Üí Alertas de calidad de datos\n",
        "\n",
        "3. Sweetviz\n",
        "   ‚Üí Comparaciones r√°pidas entre grupos\n",
        "   ‚Üí EDA visual autom√°tico\n",
        "   ‚Üí Ligero y r√°pido\n",
        "\n",
        "‚è±Ô∏è Tiempo estimado: 30-60 segundos\n",
        "\"\"\")\n",
        "\n",
        "# Instalar herramientas\n",
        "print(\"\\nüîÑ Instalando paquetes...\")\n",
        "\n",
        "!pip install -q ydata-profiling\n",
        "!pip install -q sweetviz\n",
        "\n",
        "# ==========================================\n",
        "# INSTALACI√ìN DE UMAP PARA YELLOWBRICK\n",
        "# ==========================================\n",
        "\n",
        "print(\"üì¶ Instalando UMAP-learn para visualizaciones Manifold...\")\n",
        "\n",
        "!pip install -q umap-learn\n",
        "\n",
        "print(\"‚úÖ UMAP-learn instalado correctamente\")\n",
        "\n",
        "# Verificar instalaci√≥n\n",
        "try:\n",
        "    import umap\n",
        "    print(f\"   Versi√≥n UMAP: {umap.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è UMAP no pudo ser importado - ser√° omitido en visualizaciones\")\n",
        "\n",
        "print(\"\\n‚úÖ INSTALACI√ìN COMPLETADA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Verificar versiones\n",
        "print(\"\\nüìã VERSIONES INSTALADAS:\")\n",
        "import ydata_profiling\n",
        "import sweetviz\n",
        "import yellowbrick\n",
        "\n",
        "print(f\"   ‚Ä¢ YData-Profiling: {ydata_profiling.__version__}\")\n",
        "print(f\"   ‚Ä¢ Sweetviz:        {sweetviz.__version__}\")\n",
        "print(f\"   ‚Ä¢ Yellowbrick:     {yellowbrick.__version__}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "dCFhz2To7GnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 5.0: CONFIGURACI√ìN Y SETUP DEL EDA\n",
        "# ==========================================\n",
        "\n",
        "print(\"üîç FASE 5: An√°lisis Exploratorio de Datos (EDA) - PROFESIONAL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ==========================================\n",
        "# IMPORTS NECESARIOS\n",
        "# ==========================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# Yellowbrick\n",
        "from yellowbrick.target import ClassBalance\n",
        "from yellowbrick.features import Rank1D, Rank2D, RadViz, ParallelCoordinates\n",
        "from yellowbrick.features import PCA as PCAViz, Manifold\n",
        "\n",
        "# YData-Profiling y Sweetviz (importar solo cuando se usen)\n",
        "# from ydata_profiling import ProfileReport\n",
        "# import sweetviz as sv\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas\")\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURACI√ìN DE VISUALIZACI√ìN\n",
        "# ==========================================\n",
        "\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configuraci√≥n de fuentes\n",
        "plt.rcParams.update({\n",
        "    'font.size': 10,\n",
        "    'axes.labelsize': 11,\n",
        "    'axes.titlesize': 12,\n",
        "    'xtick.labelsize': 9,\n",
        "    'ytick.labelsize': 9,\n",
        "    'legend.fontsize': 9,\n",
        "    'figure.titlesize': 14\n",
        "})\n",
        "\n",
        "print(\"‚úÖ Configuraci√≥n de visualizaci√≥n establecida\")\n",
        "\n",
        "# ==========================================\n",
        "# IDENTIFICACI√ìN DE FEATURES\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüîç IDENTIFICANDO TIPOS DE FEATURES\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Features num√©ricas (excluyendo target)\n",
        "numeric_features = df.select_dtypes(include=[np.number]).columns.drop('target').tolist()\n",
        "\n",
        "# Features categ√≥ricas (si hay)\n",
        "categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Metadata del dataset\n",
        "total_samples = len(df)\n",
        "total_features = len(df.columns)\n",
        "n_numeric = len(numeric_features)\n",
        "n_categorical = len(categorical_features)\n",
        "\n",
        "print(f\"\\nüìä RESUMEN DEL DATASET:\")\n",
        "print(f\"   ‚Ä¢ Total de muestras:    {total_samples}\")\n",
        "print(f\"   ‚Ä¢ Total de columnas:    {total_features}\")\n",
        "print(f\"   ‚Ä¢ Features num√©ricas:   {n_numeric}\")\n",
        "print(f\"   ‚Ä¢ Features categ√≥ricas: {n_categorical}\")\n",
        "print(f\"   ‚Ä¢ Variable target:      'target' (binaria: 0=No Disease, 1=Disease)\")\n",
        "\n",
        "print(f\"\\nüìã FEATURES NUM√âRICAS ({n_numeric}):\")\n",
        "print(\"-\"*80)\n",
        "for i, feat in enumerate(numeric_features, 1):\n",
        "    dtype = df[feat].dtype\n",
        "    n_unique = df[feat].nunique()\n",
        "    min_val = df[feat].min()\n",
        "    max_val = df[feat].max()\n",
        "    print(f\"   {i:2d}. {feat:15s} | Tipo: {str(dtype):8s} | √önicos: {n_unique:3d} | Rango: [{min_val:6.1f}, {max_val:6.1f}]\")\n",
        "\n",
        "if categorical_features:\n",
        "    print(f\"\\nüìã FEATURES CATEG√ìRICAS ({n_categorical}):\")\n",
        "    print(\"-\"*80)\n",
        "    for i, feat in enumerate(categorical_features, 1):\n",
        "        n_unique = df[feat].nunique()\n",
        "        print(f\"   {i:2d}. {feat:15s} | Valores √∫nicos: {n_unique:3d}\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ No hay features categ√≥ricas en el dataset\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CONFIGURACI√ìN COMPLETADA\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nüí° Variables globales disponibles:\")\n",
        "print(f\"   ‚Ä¢ numeric_features:     {n_numeric} features\")\n",
        "print(f\"   ‚Ä¢ categorical_features: {n_categorical} features\")\n",
        "print(f\"   ‚Ä¢ total_samples:        {total_samples}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "ug5GWgmn2y_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä FASE 5.1: Informaci√≥n General y Valores Faltantes\n",
        "\n",
        "### ¬øQu√© validamos?\n",
        "\n",
        "1. **Tipos de datos**: ¬øSon correctos? (ej: edad como int, no como string)\n",
        "2. **Valores faltantes**: ¬øHay NaN? ¬øCu√°ntos? ¬øEn qu√© columnas?\n",
        "3. **Duplicados**: ¬øHay registros duplicados?\n",
        "\n",
        "### ¬øPor qu√© es cr√≠tico?\n",
        "\n",
        "- **Missing values** pueden sesgar el modelo o causar errores\n",
        "- **Duplicados** inflan m√©tricas artificialmente\n",
        "- **Tipos incorrectos** causan errores en el entrenamiento"
      ],
      "metadata": {
        "id": "WwmwrTZL72-S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_10"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==========================================\n",
        "# @title FASE 5.1: INFORMACI√ìN GENERAL Y CALIDAD DE DATOS\n",
        "# ==========================================\n",
        "\n",
        "print(\"üìä FASE 5.1: Informaci√≥n General y Calidad de Datos\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ==========================================\n",
        "# INFORMACI√ìN B√ÅSICA\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìã INFORMACI√ìN B√ÅSICA DEL DATASET\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "print(f\"Shape: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
        "print(f\"\\nColumnas: {df.columns.tolist()}\")\n",
        "\n",
        "# Info detallada\n",
        "print(\"\\nüìä Tipos de datos y valores no nulos:\")\n",
        "df.info()\n",
        "\n",
        "# ==========================================\n",
        "# ESTAD√çSTICAS DESCRIPTIVAS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä ESTAD√çSTICAS DESCRIPTIVAS:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "display(df.describe().T.style.background_gradient(cmap='YlGnBu').format(\"{:.2f}\"))\n",
        "\n",
        "# ==========================================\n",
        "# AN√ÅLISIS DE VALORES FALTANTES\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüîç AN√ÅLISIS DE VALORES FALTANTES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "missing_count = df.isnull().sum()\n",
        "missing_pct = (missing_count / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_count,\n",
        "    'Missing %': missing_pct\n",
        "}).sort_values('Missing %', ascending=False)\n",
        "\n",
        "if missing_count.sum() > 0:\n",
        "    print(\"\\n‚ö†Ô∏è VALORES FALTANTES DETECTADOS:\")\n",
        "    print(\"-\"*80)\n",
        "    display(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "    # Visualizaci√≥n\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Gr√°fico de barras\n",
        "    missing_features = missing_df[missing_df['Missing Count'] > 0]\n",
        "    axes[0].barh(range(len(missing_features)), missing_features['Missing %'],\n",
        "                 color='coral', edgecolor='black', linewidth=1.5)\n",
        "    axes[0].set_yticks(range(len(missing_features)))\n",
        "    axes[0].set_yticklabels(missing_features.index)\n",
        "    axes[0].set_xlabel('% Missing', fontweight='bold')\n",
        "    axes[0].set_title('Porcentaje de Valores Faltantes', fontweight='bold', pad=15)\n",
        "    axes[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Heatmap\n",
        "    sns.heatmap(df.isnull(), cbar=False, yticklabels=False, cmap='YlOrRd', ax=axes[1])\n",
        "    axes[1].set_title('Mapa de Valores Faltantes', fontweight='bold', pad=15)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Recomendaciones\n",
        "    print(\"\\nüí° RECOMENDACIONES:\")\n",
        "    for feat, row in missing_features.iterrows():\n",
        "        pct = row['Missing %']\n",
        "        if pct > 50:\n",
        "            print(f\"   üî¥ {feat}: {pct:.1f}% missing ‚Üí Considerar eliminar columna\")\n",
        "        elif pct > 20:\n",
        "            print(f\"   üü° {feat}: {pct:.1f}% missing ‚Üí Imputar o analizar patr√≥n\")\n",
        "        else:\n",
        "            print(f\"   üü¢ {feat}: {pct:.1f}% missing ‚Üí Imputar valores\")\n",
        "else:\n",
        "    print(\"‚úÖ No hay valores faltantes en el dataset\")\n",
        "\n",
        "# ==========================================\n",
        "# AN√ÅLISIS DE DUPLICADOS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüîç AN√ÅLISIS DE DUPLICADOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "duplicates = df.duplicated().sum()\n",
        "duplicates_pct = (duplicates / len(df)) * 100\n",
        "\n",
        "print(f\"\\nRegistros duplicados: {duplicates} ({duplicates_pct:.2f}%)\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    print(\"\\n‚ö†Ô∏è DUPLICADOS DETECTADOS:\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Mostrar primeros duplicados\n",
        "    print(\"\\nüìã Primeros 10 registros duplicados:\")\n",
        "    display(df[df.duplicated(keep=False)].head(10))\n",
        "\n",
        "    print(\"\\nüí° RECOMENDACI√ìN:\")\n",
        "    print(f\"   Eliminar {duplicates} registros duplicados antes del modelado\")\n",
        "    print(f\"   C√≥digo: df = df.drop_duplicates()\")\n",
        "else:\n",
        "    print(\"‚úÖ No hay duplicados en el dataset\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öñÔ∏è FASE 5.2: Distribuci√≥n de la Variable Target\n",
        "\n",
        "### ¬øQu√© analizamos?\n",
        "\n",
        "La distribuci√≥n de clases en el target (0 = No Disease, 1 = Disease)\n",
        "\n",
        "### ¬øPor qu√© es importante?\n",
        "\n",
        "- **Desbalance de clases** afecta el entrenamiento del modelo\n",
        "- Modelos tienden a predecir la clase mayoritaria\n",
        "- Necesitamos saber si aplicar t√©cnicas de balanceo (SMOTE, class_weight)\n",
        "\n",
        "### Herramienta: Yellowbrick ClassBalance\n",
        "\n",
        "Visualizador profesional que muestra:\n",
        "- Distribuci√≥n de clases\n",
        "- Balance ratio autom√°tico\n",
        "- Recomendaciones visuales"
      ],
      "metadata": {
        "id": "C2LEagLA8DQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 5.2: DISTRIBUCI√ìN DE LA VARIABLE TARGET\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä FASE 5.2: DISTRIBUCI√ìN DE LA VARIABLE TARGET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Contar clases\n",
        "class_counts = df['target'].value_counts().sort_index()\n",
        "total = len(df)\n",
        "\n",
        "print(\"\\nüìã DISTRIBUCI√ìN DE CLASES:\")\n",
        "print(\"-\"*80)\n",
        "print(f\"   ‚Ä¢ Clase 0 (No Disease): {class_counts[0]:3d} pacientes ({class_counts[0]/total*100:5.1f}%)\")\n",
        "print(f\"   ‚Ä¢ Clase 1 (Disease):    {class_counts[1]:3d} pacientes ({class_counts[1]/total*100:5.1f}%)\")\n",
        "\n",
        "# Balance ratio\n",
        "balance_ratio = class_counts.min() / class_counts.max()\n",
        "print(f\"\\n‚öñÔ∏è Balance Ratio: {balance_ratio:.3f}\")\n",
        "\n",
        "if balance_ratio >= 0.8:\n",
        "    print(\"‚úÖ Dataset BIEN BALANCEADO - No requiere t√©cnicas de balanceo\")\n",
        "elif balance_ratio >= 0.6:\n",
        "    print(\"‚ö†Ô∏è Dataset MODERADAMENTE DESBALANCEADO - Monitorear m√©tricas por clase\")\n",
        "else:\n",
        "    print(\"üî¥ Dataset DESBALANCEADO - Considerar SMOTE o class_weight\")\n",
        "\n",
        "# ==========================================\n",
        "# VISUALIZACI√ìN CON YELLOWBRICK\n",
        "# ==========================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# 1. Yellowbrick ClassBalance\n",
        "print(\"\\nüìä Generando visualizaci√≥n Yellowbrick...\")\n",
        "visualizer = ClassBalance(ax=axes[0], labels=['No Disease', 'Disease'])\n",
        "visualizer.fit(df['target'].values)\n",
        "visualizer.finalize()\n",
        "axes[0].set_title('Yellowbrick: Class Balance', fontweight='bold', fontsize=12, pad=15)\n",
        "\n",
        "# 2. Countplot mejorado\n",
        "colors = ['#2ecc71', '#e74c3c']\n",
        "bars = axes[1].bar(['No Disease', 'Disease'], class_counts.values,\n",
        "                   color=colors, edgecolor='black', linewidth=2, alpha=0.8)\n",
        "axes[1].set_title('Distribuci√≥n de Clases', fontweight='bold', fontsize=12, pad=15)\n",
        "axes[1].set_ylabel('N√∫mero de Pacientes', fontweight='bold')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Anotaciones\n",
        "for i, (bar, count) in enumerate(zip(bars, class_counts.values)):\n",
        "    height = bar.get_height()\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2., height + 5,\n",
        "                f'{count}\\n({count/total*100:.1f}%)',\n",
        "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "\n",
        "# 3. Balance ratio visualization\n",
        "axes[2].barh(['Balance Ratio'], [balance_ratio], color='steelblue',\n",
        "             edgecolor='black', linewidth=2, height=0.3)\n",
        "axes[2].set_xlim([0, 1])\n",
        "axes[2].axvline(0.8, color='green', linestyle='--', linewidth=2, label='Ideal (‚â•0.8)')\n",
        "axes[2].axvline(0.6, color='orange', linestyle='--', linewidth=2, label='Aceptable (‚â•0.6)')\n",
        "axes[2].set_xlabel('Ratio (min/max)', fontweight='bold')\n",
        "axes[2].set_title('Balance del Dataset', fontweight='bold', fontsize=12, pad=15)\n",
        "axes[2].legend(loc='lower right', fontsize=9)\n",
        "axes[2].grid(axis='x', alpha=0.3)\n",
        "axes[2].text(balance_ratio + 0.02, 0, f'{balance_ratio:.3f}',\n",
        "            va='center', fontweight='bold', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Visualizaci√≥n completada\")\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "BTM9JGXO1egU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä FASE 5.3: An√°lisis Univariado - Distribuciones\n",
        "\n",
        "### ¬øQu√© analizamos?\n",
        "\n",
        "La distribuci√≥n de cada feature individualmente:\n",
        "- **Forma**: ¬øNormal? ¬øSesgada? ¬øBimodal?\n",
        "- **Centralidad**: Media vs Mediana\n",
        "- **Dispersi√≥n**: Rango, varianza\n",
        "- **Normalidad**: Skewness, Kurtosis\n",
        "\n",
        "### ¬øPor qu√© es importante?\n",
        "\n",
        "- Detectar **features muy sesgadas** (requieren transformaci√≥n)\n",
        "- Identificar **distribuciones an√≥malas**\n",
        "- Decidir si aplicar **normalizaci√≥n/estandarizaci√≥n**\n",
        "- Algunos modelos (ej: Logistic Regression) asumen normalidad"
      ],
      "metadata": {
        "id": "eyP-mlg-8SvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 5.3: AN√ÅLISIS UNIVARIADO - DISTRIBUCIONES\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä FASE 5.3: An√°lisis Univariado - Distribuciones\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìã Analizando {len(numeric_features)} features num√©ricas\")\n",
        "\n",
        "# Grid de distribuciones\n",
        "n_cols = 3\n",
        "n_rows = int(np.ceil(len(numeric_features) / n_cols))\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows*4))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(numeric_features):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Histograma + KDE\n",
        "    n, bins, patches = ax.hist(df[feature], bins=30, alpha=0.6, color='steelblue',\n",
        "                                edgecolor='black', linewidth=0.5, density=True)\n",
        "\n",
        "    # KDE overlay\n",
        "    df[feature].plot(kind='kde', ax=ax, color='red', linewidth=2.5, label='KDE')\n",
        "\n",
        "    # Estad√≠sticas\n",
        "    mean_val = df[feature].mean()\n",
        "    median_val = df[feature].median()\n",
        "    std_val = df[feature].std()\n",
        "    skew = df[feature].skew()\n",
        "    kurt = df[feature].kurt()\n",
        "\n",
        "    # L√≠neas de media y mediana\n",
        "    ax.axvline(mean_val, color='green', linestyle='--', linewidth=2,\n",
        "              label=f'Mean: {mean_val:.2f}')\n",
        "    ax.axvline(median_val, color='orange', linestyle='--', linewidth=2,\n",
        "              label=f'Median: {median_val:.2f}')\n",
        "\n",
        "    # T√≠tulo con estad√≠sticas\n",
        "    ax.set_title(f'{feature}\\nSkew: {skew:.2f} | Kurt: {kurt:.2f} | Std: {std_val:.2f}',\n",
        "                fontweight='bold', fontsize=11)\n",
        "    ax.set_xlabel('')\n",
        "    ax.legend(fontsize=8, loc='upper right')\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "# Limpiar ejes vac√≠os\n",
        "for idx in range(len(numeric_features), len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Distribuciones Univariadas de Features',\n",
        "            fontweight='bold', fontsize=16, y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# RESUMEN DE NORMALIDAD\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìã RESUMEN DE NORMALIDAD (Skewness):\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "skewness_df = pd.DataFrame({\n",
        "    'Feature': numeric_features,\n",
        "    'Skewness': [df[f].skew() for f in numeric_features],\n",
        "    'Kurtosis': [df[f].kurt() for f in numeric_features],\n",
        "    'Interpretaci√≥n': ['Altamente sesgada' if abs(df[f].skew()) > 1\n",
        "                      else 'Moderadamente sesgada' if abs(df[f].skew()) > 0.5\n",
        "                      else 'Aproximadamente sim√©trica'\n",
        "                      for f in numeric_features]\n",
        "}).sort_values('Skewness', key=abs, ascending=False)\n",
        "\n",
        "display(skewness_df.style.background_gradient(subset=['Skewness', 'Kurtosis'], cmap='RdYlGn_r'))\n",
        "\n",
        "print(\"\\nüí° GU√çA DE INTERPRETACI√ìN:\")\n",
        "print(\"-\"*80)\n",
        "print(\"  SKEWNESS (Asimetr√≠a):\")\n",
        "print(\"    ‚Ä¢ |Skew| > 1.0:        Altamente sesgada ‚Üí Transformar (log, sqrt, Box-Cox)\")\n",
        "print(\"    ‚Ä¢ 0.5 < |Skew| < 1.0:  Moderadamente sesgada ‚Üí Evaluar transformaci√≥n\")\n",
        "print(\"    ‚Ä¢ |Skew| < 0.5:        Aproximadamente sim√©trica ‚Üí OK\")\n",
        "print(\"\\n  KURTOSIS (Peso de colas):\")\n",
        "print(\"    ‚Ä¢ Kurt > 3:   Colas pesadas (m√°s outliers)\")\n",
        "print(\"    ‚Ä¢ Kurt ‚âà 0:   Normal\")\n",
        "print(\"    ‚Ä¢ Kurt < 0:   Colas ligeras\")\n",
        "\n",
        "# Features que requieren transformaci√≥n\n",
        "highly_skewed = skewness_df[abs(skewness_df['Skewness']) > 1]['Feature'].tolist()\n",
        "if highly_skewed:\n",
        "    print(f\"\\n‚ö†Ô∏è FEATURES QUE REQUIEREN TRANSFORMACI√ìN ({len(highly_skewed)}):\")\n",
        "    for feat in highly_skewed:\n",
        "        print(f\"   ‚Ä¢ {feat}\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Todas las features tienen skewness aceptable\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "um4otjeb8VF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèÜ FASE 5.4: Ranking de Features (Yellowbrick)\n",
        "\n",
        "### ¬øQu√© hace Yellowbrick Rank?\n",
        "\n",
        "**Rank1D**: Eval√∫a cada feature individualmente\n",
        "- Usa Shapiro-Wilk test para normalidad\n",
        "- Features con barras altas = m√°s discriminativas\n",
        "\n",
        "**Rank2D**: Eval√∫a correlaciones entre pares de features\n",
        "- Matriz de correlaci√≥n visual\n",
        "- Detecta multicolinealidad\n",
        "- Identifica redundancia\n",
        "\n",
        "### ¬øPor qu√© es √∫til?\n",
        "\n",
        "- **Feature selection**: Identificar features m√°s importantes\n",
        "- **Reducci√≥n dimensional**: Eliminar features redundantes\n",
        "- **Entender relaciones**: Qu√© features est√°n correlacionadas"
      ],
      "metadata": {
        "id": "taK_3Tq78fdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 5.4: RANKING DE FEATURES (YELLOWBRICK)\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüèÜ FASE 5.4: Ranking de Features con Yellowbrick\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from yellowbrick.features import Rank1D, Rank2D\n",
        "\n",
        "# Preparar datos\n",
        "X = df[numeric_features]\n",
        "y = df['target']\n",
        "\n",
        "print(f\"\\nüìã Analizando {len(numeric_features)} features\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "# ==========================================\n",
        "# 1. RANK1D - Ranking de features individuales\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä Generando Rank1D (Shapiro-Wilk)...\")\n",
        "print(\"   Eval√∫a capacidad discriminativa de cada feature individual\")\n",
        "\n",
        "visualizer1 = Rank1D(\n",
        "    ax=axes[0],\n",
        "    algorithm='shapiro',  # Test de normalidad\n",
        "    features=numeric_features,\n",
        "    show_feature_names=True\n",
        ")\n",
        "visualizer1.fit(X, y)\n",
        "visualizer1.finalize()\n",
        "axes[0].set_title('Rank1D: Features por Capacidad Discriminativa\\n(Shapiro-Wilk Test)',\n",
        "                  fontweight='bold', fontsize=12, pad=15)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# ==========================================\n",
        "# 2. RANK2D - Correlaciones entre features\n",
        "# ==========================================\n",
        "\n",
        "print(\"üìä Generando Rank2D (Pearson)...\")\n",
        "print(\"   Detecta correlaciones entre pares de features\")\n",
        "\n",
        "visualizer2 = Rank2D(\n",
        "    ax=axes[1],\n",
        "    algorithm='pearson',\n",
        "    features=numeric_features,\n",
        "    show_feature_names=True,\n",
        "    colormap='RdYlGn'\n",
        ")\n",
        "visualizer2.fit(X, y)\n",
        "visualizer2.finalize()\n",
        "axes[1].set_title('Rank2D: Correlaciones entre Features\\n(Pearson Correlation)',\n",
        "                  fontweight='bold', fontsize=12, pad=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# INTERPRETACI√ìN\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüí° INTERPRETACI√ìN:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n  RANK1D (izquierda):\")\n",
        "print(\"    ‚Ä¢ Barras m√°s ALTAS = Features m√°s discriminativas\")\n",
        "print(\"    ‚Ä¢ √ötil para feature selection inicial\")\n",
        "print(\"    ‚Ä¢ Shapiro-Wilk test: mide desviaci√≥n de normalidad\")\n",
        "print(\"\\n  RANK2D (derecha):\")\n",
        "print(\"    ‚Ä¢ Verde oscuro = Correlaci√≥n POSITIVA alta\")\n",
        "print(\"    ‚Ä¢ Rojo oscuro = Correlaci√≥n NEGATIVA alta\")\n",
        "print(\"    ‚Ä¢ Amarillo/blanco = Poca/ninguna correlaci√≥n\")\n",
        "print(\"    ‚Ä¢ Detecta multicolinealidad (features redundantes)\")\n",
        "\n",
        "# Identificar features menos discriminativas\n",
        "# (valores bajos en Rank1D - pero esto requerir√≠a acceso a scores internos)\n",
        "print(\"\\nüìå RECOMENDACIONES:\")\n",
        "print(\"    1. Features con Rank1D bajo ‚Üí Candidatas para eliminaci√≥n\")\n",
        "print(\"    2. Pares con Rank2D alto (verde/rojo oscuro) ‚Üí Evaluar redundancia\")\n",
        "print(\"    3. Mantener features con alta discriminaci√≥n y baja correlaci√≥n entre s√≠\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "ThXUyZ7e1-Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ FASE 5.5: Detecci√≥n y An√°lisis de Outliers\n",
        "\n",
        "### ¬øQu√© son los Outliers?\n",
        "\n",
        "Valores que se desv√≠an significativamente del resto de los datos.\n",
        "\n",
        "### ¬øPor qu√© son cr√≠ticos en medicina?\n",
        "\n",
        "- Pueden ser **errores de medici√≥n** ‚Üí Eliminar\n",
        "- Pueden ser **casos raros pero v√°lidos** ‚Üí Mantener\n",
        "- Pueden indicar **condiciones extremas** ‚Üí Analizar\n",
        "\n",
        "### M√©todo: IQR (Interquartile Range)\n",
        "```\n",
        "Q1 = Percentil 25\n",
        "Q3 = Percentil 75\n",
        "IQR = Q3 - Q1\n",
        "Lower bound = Q1 - 1.5 √ó IQR\n",
        "Upper bound = Q3 + 1.5 √ó IQR\n",
        "Outliers = valores fuera de estos l√≠mites\n",
        "```"
      ],
      "metadata": {
        "id": "UF3hgV8Q8zvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 5.5: DETECCI√ìN Y AN√ÅLISIS DE OUTLIERS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüéØ FASE 5.5: Detecci√≥n y An√°lisis de Outliers\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "üìå M√âTODO IQR (Interquartile Range):\n",
        "   ‚Ä¢ Q1 = Percentil 25\n",
        "   ‚Ä¢ Q3 = Percentil 75\n",
        "   ‚Ä¢ IQR = Q3 - Q1\n",
        "   ‚Ä¢ L√≠mite inferior: Q1 - 1.5 √ó IQR\n",
        "   ‚Ä¢ L√≠mite superior: Q3 + 1.5 √ó IQR\n",
        "   ‚Ä¢ Valores fuera = OUTLIERS\n",
        "\"\"\")\n",
        "\n",
        "# Boxplots por clase\n",
        "n_cols = 3\n",
        "n_rows = int(np.ceil(len(numeric_features) / n_cols))\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows*4))\n",
        "axes = axes.flatten()\n",
        "\n",
        "outlier_summary = {}\n",
        "\n",
        "for idx, feature in enumerate(numeric_features):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Crear DataFrame para boxplot\n",
        "    data_to_plot = [\n",
        "        df[df['target'] == 0][feature].dropna(),\n",
        "        df[df['target'] == 1][feature].dropna()\n",
        "    ]\n",
        "\n",
        "    # Boxplot\n",
        "    bp = ax.boxplot(data_to_plot, labels=['No Disease', 'Disease'],\n",
        "                    patch_artist=True, widths=0.6,\n",
        "                    boxprops=dict(facecolor='lightblue', edgecolor='black', linewidth=1.5),\n",
        "                    whiskerprops=dict(color='black', linewidth=1.5),\n",
        "                    capprops=dict(color='black', linewidth=1.5),\n",
        "                    medianprops=dict(color='red', linewidth=2),\n",
        "                    flierprops=dict(marker='o', markerfacecolor='red', markersize=6,\n",
        "                                   markeredgecolor='black', alpha=0.6))\n",
        "\n",
        "    # Colorear boxes\n",
        "    colors = ['#2ecc71', '#e74c3c']\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.6)\n",
        "\n",
        "    # Calcular outliers con IQR\n",
        "    Q1 = df[feature].quantile(0.25)\n",
        "    Q3 = df[feature].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
        "    outlier_summary[feature] = {\n",
        "        'count': len(outliers),\n",
        "        'percentage': len(outliers)/len(df)*100,\n",
        "        'lower_bound': lower_bound,\n",
        "        'upper_bound': upper_bound\n",
        "    }\n",
        "\n",
        "    # T√≠tulo con informaci√≥n\n",
        "    ax.set_title(f'{feature}\\nOutliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)',\n",
        "                fontweight='bold', fontsize=11)\n",
        "    ax.set_ylabel('Valor', fontweight='bold')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # L√≠neas de l√≠mites (opcional - puede saturar)\n",
        "    # ax.axhline(lower_bound, color='blue', linestyle='--', linewidth=1, alpha=0.3)\n",
        "    # ax.axhline(upper_bound, color='blue', linestyle='--', linewidth=1, alpha=0.3)\n",
        "\n",
        "# Limpiar ejes vac√≠os\n",
        "for idx in range(len(numeric_features), len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Detecci√≥n de Outliers por Feature (Disease vs No Disease)',\n",
        "            fontweight='bold', fontsize=16, y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# RESUMEN DE OUTLIERS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä RESUMEN DE OUTLIERS:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "outlier_table = pd.DataFrame.from_dict(outlier_summary, orient='index')\n",
        "outlier_table = outlier_table.sort_values('count', ascending=False)\n",
        "outlier_table['percentage'] = outlier_table['percentage'].round(2)\n",
        "\n",
        "display(outlier_table.style.background_gradient(subset=['count', 'percentage'], cmap='Reds'))\n",
        "\n",
        "print(\"\\nüí° INTERPRETACI√ìN:\")\n",
        "print(\"-\"*80)\n",
        "print(\"  ‚Ä¢ Features con >10% outliers: Revisar si son errores o casos v√°lidos\")\n",
        "print(\"  ‚Ä¢ En medicina, valores extremos pueden ser cl√≠nicamente relevantes\")\n",
        "print(\"  ‚Ä¢ NO eliminar outliers sin validaci√≥n m√©dica/de dominio\")\n",
        "\n",
        "# Features con m√°s outliers\n",
        "top_outliers = outlier_table.nlargest(3, 'count')\n",
        "print(f\"\\nüîù TOP 3 features con m√°s outliers:\")\n",
        "for feat, row in top_outliers.iterrows():\n",
        "    print(f\"   ‚Ä¢ {feat:15s}: {int(row['count']):3d} outliers ({row['percentage']:5.1f}%)\")\n",
        "\n",
        "# Recomendaci√≥n\n",
        "total_outliers = outlier_table['count'].sum()\n",
        "print(f\"\\nüìä Total de outliers detectados: {int(total_outliers)}\")\n",
        "print(f\"   Porcentaje del dataset: {total_outliers/len(df)/len(numeric_features)*100:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "581roAqP81C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä FASE 5.6: An√°lisis Bivariado (Features vs Target)\n",
        "\n",
        "### ¬øQu√© analizamos?\n",
        "\n",
        "C√≥mo cada feature se relaciona con el target (enfermedad):\n",
        "- ¬øLas distribuciones son **diferentes** entre clases?\n",
        "- ¬øHay **separaci√≥n clara**?\n",
        "- ¬øQu√© features son m√°s **discriminativas**?\n",
        "\n",
        "### ¬øPor qu√© es cr√≠tico?\n",
        "\n",
        "- Features con **distribuciones separadas** ‚Üí √∫tiles para predicci√≥n\n",
        "- Features con **distribuciones superpuestas** ‚Üí menos discriminativas\n",
        "- Ayuda a entender **qu√© factores influyen** en la enfermedad\n",
        "\n",
        "### Interpretaci√≥n Visual\n",
        "\n",
        "- **Verde** = No Disease\n",
        "- **Rojo** = Disease\n",
        "- Mayor separaci√≥n = Mayor poder predictivo"
      ],
      "metadata": {
        "id": "WwTvT1ct88EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 5.6: AN√ÅLISIS BIVARIADO (Features vs Target)\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä FASE 5.6: An√°lisis Bivariado - Features vs Target\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "üéØ OBJETIVO: Identificar features discriminativas\n",
        "\n",
        "üìä INTERPRETACI√ìN:\n",
        "   ‚Ä¢ Distribuciones SEPARADAS ‚Üí Feature √∫til para predicci√≥n\n",
        "   ‚Ä¢ Distribuciones SUPERPUESTAS ‚Üí Feature menos discriminativa\n",
        "   ‚Ä¢ Verde = No Disease | Rojo = Disease\n",
        "\"\"\")\n",
        "\n",
        "# Grid de distribuciones por clase\n",
        "n_cols = 3\n",
        "n_rows = int(np.ceil(len(numeric_features) / n_cols))\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows*4))\n",
        "axes = axes.flatten()\n",
        "\n",
        "comparison_data = []\n",
        "\n",
        "for idx, feature in enumerate(numeric_features):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Datos por clase\n",
        "    no_disease = df[df['target'] == 0][feature]\n",
        "    disease = df[df['target'] == 1][feature]\n",
        "\n",
        "    # Histogramas\n",
        "    ax.hist(no_disease, bins=20, alpha=0.5, label='No Disease',\n",
        "           color='#2ecc71', edgecolor='black', linewidth=0.5, density=True)\n",
        "    ax.hist(disease, bins=20, alpha=0.5, label='Disease',\n",
        "           color='#e74c3c', edgecolor='black', linewidth=0.5, density=True)\n",
        "\n",
        "    # KDE\n",
        "    no_disease.plot(kind='kde', ax=ax, color='darkgreen', linewidth=2.5)\n",
        "    disease.plot(kind='kde', ax=ax, color='darkred', linewidth=2.5)\n",
        "\n",
        "    # Medias\n",
        "    mean_no_disease = no_disease.mean()\n",
        "    mean_disease = disease.mean()\n",
        "\n",
        "    ax.axvline(mean_no_disease, color='darkgreen', linestyle='--', linewidth=2, alpha=0.7)\n",
        "    ax.axvline(mean_disease, color='darkred', linestyle='--', linewidth=2, alpha=0.7)\n",
        "\n",
        "    # Calcular diferencia\n",
        "    diff = mean_disease - mean_no_disease\n",
        "    pct_change = (diff / mean_no_disease) * 100 if mean_no_disease != 0 else 0\n",
        "\n",
        "    comparison_data.append({\n",
        "        'Feature': feature,\n",
        "        'Mean_No_Disease': mean_no_disease,\n",
        "        'Mean_Disease': mean_disease,\n",
        "        'Difference': diff,\n",
        "        '% Change': pct_change\n",
        "    })\n",
        "\n",
        "    # T√≠tulo con diferencia de medias\n",
        "    ax.set_title(f'{feature}\\nŒî Media: {diff:+.2f} ({pct_change:+.1f}%)',\n",
        "                fontweight='bold', fontsize=11)\n",
        "    ax.set_xlabel('')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "# Limpiar ejes vac√≠os\n",
        "for idx in range(len(numeric_features), len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Distribuciones de Features por Clase (Disease vs No Disease)',\n",
        "            fontweight='bold', fontsize=16, y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# TABLA COMPARATIVA\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìã COMPARACI√ìN DE ESTAD√çSTICAS POR CLASE:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# ‚úÖ SOLUCI√ìN: Crear columna auxiliar con valor absoluto\n",
        "comparison_df['Abs_Change'] = comparison_df['% Change'].abs()\n",
        "\n",
        "# Ordenar por valor absoluto\n",
        "comparison_df = comparison_df.sort_values('Abs_Change', ascending=False)\n",
        "\n",
        "# Eliminar columna auxiliar antes de mostrar\n",
        "comparison_df_display = comparison_df.drop('Abs_Change', axis=1)\n",
        "\n",
        "display(comparison_df_display.style.background_gradient(subset=['% Change'], cmap='RdYlGn_r').format({\n",
        "    'Mean_No_Disease': '{:.2f}',\n",
        "    'Mean_Disease': '{:.2f}',\n",
        "    'Difference': '{:+.2f}',\n",
        "    '% Change': '{:+.1f}%'\n",
        "}))\n",
        "\n",
        "print(\"\\nüí° INTERPRETACI√ìN:\")\n",
        "print(\"-\"*80)\n",
        "print(\"  ‚Ä¢ % Change ALTO (>20%):     Feature muy discriminativa\")\n",
        "print(\"  ‚Ä¢ % Change POSITIVO:        Valores m√°s altos en Disease\")\n",
        "print(\"  ‚Ä¢ % Change NEGATIVO:        Valores m√°s altos en No Disease\")\n",
        "print(\"  ‚Ä¢ % Change BAJO (<10%):     Feature poco discriminativa\")\n",
        "\n",
        "# ==========================================\n",
        "# TOP FEATURES DISCRIMINATIVAS\n",
        "# ==========================================\n",
        "\n",
        "print(f\"\\nüèÜ TOP 5 FEATURES M√ÅS DISCRIMINATIVAS:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# ‚úÖ SOLUCI√ìN: Usar la columna 'Abs_Change' que ya existe en comparison_df\n",
        "top_features = comparison_df.nlargest(5, 'Abs_Change')\n",
        "\n",
        "for i, row in enumerate(top_features.itertuples(), 1):\n",
        "    feature = row.Feature\n",
        "    diff = row.Difference\n",
        "    pct_change = getattr(row, '_5')  # '% Change' es el √≠ndice 5\n",
        "\n",
        "    direction = \"‚Üë mayor\" if diff > 0 else \"‚Üì menor\"\n",
        "    print(f\"   {i}. {feature:15s}: {direction} en Disease ({pct_change:+6.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "unCw08an89Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öïÔ∏è FASE 5.7: An√°lisis Cl√≠nico Espec√≠fico\n",
        "\n",
        "### ¬øPor qu√© es √∫nico en medicina?\n",
        "\n",
        "En diagn√≥stico m√©dico, existen **rangos cl√≠nicos establecidos** por gu√≠as m√©dicas:\n",
        "- Presi√≥n arterial: Normal, Elevada, Alta\n",
        "- Colesterol: Normal, Borderline, Alto\n",
        "- Edad: Grupos de riesgo\n",
        "\n",
        "### Objetivo\n",
        "\n",
        "Correlacionar rangos cl√≠nicos con presencia de enfermedad para:\n",
        "- Validar hip√≥tesis m√©dicas\n",
        "- Identificar grupos de riesgo\n",
        "- Generar insights accionables\n",
        "\n",
        "### Valor Educativo\n",
        "\n",
        "Ense√±a a estudiantes a combinar:\n",
        "- **Conocimiento del dominio** (medicina)\n",
        "- **An√°lisis de datos** (data science)"
      ],
      "metadata": {
        "id": "yaSOlgE99Sxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 5.7: AN√ÅLISIS CL√çNICO ESPEC√çFICO\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n‚öïÔ∏è FASE 5.7: An√°lisis Cl√≠nico Espec√≠fico\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "üìå RANGOS CL√çNICOS ESTABLECIDOS (basados en gu√≠as m√©dicas):\n",
        "\n",
        "   PRESI√ìN ARTERIAL (trestbps):\n",
        "   ‚Ä¢ Normal:      90-120 mmHg\n",
        "   ‚Ä¢ Elevada:     120-140 mmHg\n",
        "   ‚Ä¢ Alta:        >140 mmHg\n",
        "\n",
        "   COLESTEROL (chol):\n",
        "   ‚Ä¢ Normal:      <200 mg/dL\n",
        "   ‚Ä¢ Borderline:  200-240 mg/dL\n",
        "   ‚Ä¢ Alto:        >240 mg/dL\n",
        "\n",
        "   FRECUENCIA CARD√çACA M√ÅXIMA (thalach):\n",
        "   ‚Ä¢ Baja:        <100 bpm\n",
        "   ‚Ä¢ Normal:      100-170 bpm\n",
        "   ‚Ä¢ Alta:        >170 bpm\n",
        "\n",
        "   EDAD (age):\n",
        "   ‚Ä¢ Joven:       <40 a√±os\n",
        "   ‚Ä¢ Adulto:      40-60 a√±os\n",
        "   ‚Ä¢ Mayor:       >60 a√±os\n",
        "\n",
        "üéØ OBJETIVO: Correlacionar rangos cl√≠nicos con enfermedad cardiovascular\n",
        "\"\"\")\n",
        "\n",
        "# Definir rangos cl√≠nicos\n",
        "clinical_ranges = {\n",
        "    'trestbps': {\n",
        "        'Normal (90-120)': (90, 120),\n",
        "        'Elevada (120-140)': (120, 140),\n",
        "        'Alta (>140)': (140, 250)\n",
        "    },\n",
        "    'chol': {\n",
        "        'Normal (<200)': (0, 200),\n",
        "        'Borderline (200-240)': (200, 240),\n",
        "        'Alto (>240)': (240, 600)\n",
        "    },\n",
        "    'thalach': {\n",
        "        'Baja (<100)': (0, 100),\n",
        "        'Normal (100-170)': (100, 170),\n",
        "        'Alta (>170)': (170, 250)\n",
        "    },\n",
        "    'age': {\n",
        "        'Joven (<40)': (0, 40),\n",
        "        'Adulto (40-60)': (40, 60),\n",
        "        'Mayor (>60)': (60, 100)\n",
        "    }\n",
        "}\n",
        "\n",
        "# An√°lisis por rangos\n",
        "results_clinical = []\n",
        "\n",
        "print(\"\\nüìä AN√ÅLISIS POR RANGOS CL√çNICOS:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for feature, ranges in clinical_ranges.items():\n",
        "    if feature in df.columns:\n",
        "        print(f\"\\nüìã {feature.upper()}:\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "        for range_name, (low, high) in ranges.items():\n",
        "            mask = (df[feature] >= low) & (df[feature] < high)\n",
        "            count = mask.sum()\n",
        "\n",
        "            if count > 0:\n",
        "                disease_rate = df[mask]['target'].mean() * 100\n",
        "                no_disease_count = (mask & (df['target'] == 0)).sum()\n",
        "                disease_count = (mask & (df['target'] == 1)).sum()\n",
        "\n",
        "                print(f\"  {range_name:25s}: {count:3d} pac. ({count/len(df)*100:5.1f}%) \"\n",
        "                      f\"‚Üí Disease: {disease_count:3d} ({disease_rate:5.1f}%)\")\n",
        "\n",
        "                results_clinical.append({\n",
        "                    'Feature': feature,\n",
        "                    'Rango': range_name,\n",
        "                    'Total': count,\n",
        "                    'Disease Rate (%)': disease_rate\n",
        "                })\n",
        "\n",
        "# ==========================================\n",
        "# VISUALIZACI√ìN POR FEATURE\n",
        "# ==========================================\n",
        "\n",
        "results_df = pd.DataFrame(results_clinical)\n",
        "features_to_plot = list(clinical_ranges.keys())\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(features_to_plot):\n",
        "    if feature in df.columns:\n",
        "        ax = axes[idx]\n",
        "\n",
        "        # Filtrar datos del feature actual\n",
        "        feature_data = results_df[results_df['Feature'] == feature]\n",
        "\n",
        "        # Gr√°fico de barras\n",
        "        bars = ax.bar(range(len(feature_data)), feature_data['Disease Rate (%)'],\n",
        "                      color=['#2ecc71', '#f39c12', '#e74c3c'][:len(feature_data)],\n",
        "                      edgecolor='black', linewidth=1.5, alpha=0.8)\n",
        "\n",
        "        ax.set_xticks(range(len(feature_data)))\n",
        "        ax.set_xticklabels(feature_data['Rango'], rotation=15, ha='right', fontsize=10)\n",
        "        ax.set_ylabel('% con Enfermedad Cardiovascular', fontweight='bold', fontsize=11)\n",
        "        ax.set_title(f'{feature.upper()} - Tasa de Enfermedad por Rango',\n",
        "                    fontweight='bold', fontsize=12, pad=15)\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # Anotaciones\n",
        "        for i, (bar, row) in enumerate(zip(bars, feature_data.itertuples())):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                   f'{height:.1f}%\\n(n={row.Total})',\n",
        "                   ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
        "\n",
        "        # L√≠nea de referencia (promedio general)\n",
        "        avg_disease_rate = df['target'].mean() * 100\n",
        "        ax.axhline(avg_disease_rate, color='blue', linestyle='--', linewidth=2,\n",
        "                  alpha=0.7, label=f'Promedio: {avg_disease_rate:.1f}%')\n",
        "        ax.legend(fontsize=9)\n",
        "\n",
        "plt.suptitle('Tasa de Enfermedad Cardiovascular por Rangos Cl√≠nicos',\n",
        "            fontweight='bold', fontsize=14, y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "rLEfAIoE9VbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üë• FASE 5.7.1: An√°lisis Detallado por Grupos de Edad\n",
        "\n",
        "### ¬øPor qu√© es importante?\n",
        "\n",
        "La **edad es un factor de riesgo cr√≠tico** en enfermedad cardiovascular:\n",
        "- Riesgo aumenta con la edad\n",
        "- Diferentes estrategias de prevenci√≥n por grupo\n",
        "- Pol√≠ticas de salud p√∫blica por grupo etario"
      ],
      "metadata": {
        "id": "7hw41Bd19buY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 5.7.1: AN√ÅLISIS POR GRUPOS DE EDAD\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüë• AN√ÅLISIS DETALLADO POR GRUPOS DE EDAD\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear grupos de edad\n",
        "df_temp = df.copy()\n",
        "df_temp['age_group'] = pd.cut(df_temp['age'],\n",
        "                               bins=[0, 40, 50, 60, 70, 100],\n",
        "                               labels=['<40', '40-49', '50-59', '60-69', '70+'])\n",
        "\n",
        "# Estad√≠sticas por grupo\n",
        "age_stats = df_temp.groupby('age_group').agg({\n",
        "    'target': ['count', 'sum', 'mean']\n",
        "})\n",
        "\n",
        "age_stats.columns = ['Total Pacientes', 'Con Enfermedad', 'Tasa Enfermedad']\n",
        "age_stats['% del Total'] = (age_stats['Total Pacientes'] / len(df_temp)) * 100\n",
        "age_stats['Tasa Enfermedad'] = age_stats['Tasa Enfermedad'] * 100\n",
        "\n",
        "print(\"\\nüìä ESTAD√çSTICAS POR GRUPO DE EDAD:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "display(age_stats.style.background_gradient(subset=['Tasa Enfermedad'], cmap='Reds').format({\n",
        "    'Total Pacientes': '{:.0f}',\n",
        "    'Con Enfermedad': '{:.0f}',\n",
        "    'Tasa Enfermedad': '{:.1f}%',\n",
        "    '% del Total': '{:.1f}%'\n",
        "}))\n",
        "\n",
        "# ==========================================\n",
        "# VISUALIZACI√ìN COMPLETA POR EDAD\n",
        "# ==========================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# 1. Distribuci√≥n de pacientes por edad\n",
        "age_counts = df_temp['age_group'].value_counts().sort_index()\n",
        "axes[0].bar(range(len(age_counts)), age_counts.values,\n",
        "           color='steelblue', edgecolor='black', linewidth=1.5, alpha=0.8)\n",
        "axes[0].set_xticks(range(len(age_counts)))\n",
        "axes[0].set_xticklabels(age_counts.index, fontsize=11, fontweight='bold')\n",
        "axes[0].set_ylabel('N√∫mero de Pacientes', fontweight='bold', fontsize=11)\n",
        "axes[0].set_title('Distribuci√≥n de Pacientes por Edad', fontweight='bold', fontsize=12, pad=15)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "for i, v in enumerate(age_counts.values):\n",
        "    axes[0].text(i, v + 1, str(v), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 2. Tasa de enfermedad por edad\n",
        "disease_rate = age_stats['Tasa Enfermedad']\n",
        "colors_age = ['#2ecc71' if x < 50 else '#f39c12' if x < 70 else '#e74c3c'\n",
        "              for x in disease_rate.values]\n",
        "\n",
        "bars = axes[1].bar(range(len(disease_rate)), disease_rate.values,\n",
        "                   color=colors_age, edgecolor='black', linewidth=1.5, alpha=0.8)\n",
        "axes[1].set_xticks(range(len(disease_rate)))\n",
        "axes[1].set_xticklabels(disease_rate.index, fontsize=11, fontweight='bold')\n",
        "axes[1].set_ylabel('% con Enfermedad', fontweight='bold', fontsize=11)\n",
        "axes[1].set_title('Tasa de Enfermedad por Grupo de Edad', fontweight='bold', fontsize=12, pad=15)\n",
        "axes[1].axhline(df_temp['target'].mean() * 100, color='blue', linestyle='--',\n",
        "               linewidth=2, label=f'Promedio: {df_temp[\"target\"].mean()*100:.1f}%')\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "for i, v in enumerate(disease_rate.values):\n",
        "    axes[1].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 3. Stacked bar (Disease vs No Disease)\n",
        "no_disease_counts = []\n",
        "disease_counts = []\n",
        "\n",
        "for age_grp in age_counts.index:\n",
        "    mask = df_temp['age_group'] == age_grp\n",
        "    no_disease_counts.append((mask & (df_temp['target'] == 0)).sum())\n",
        "    disease_counts.append((mask & (df_temp['target'] == 1)).sum())\n",
        "\n",
        "x = range(len(age_counts))\n",
        "axes[2].bar(x, no_disease_counts, label='No Disease', color='#2ecc71',\n",
        "           edgecolor='black', linewidth=1.5, alpha=0.8)\n",
        "axes[2].bar(x, disease_counts, bottom=no_disease_counts, label='Disease',\n",
        "           color='#e74c3c', edgecolor='black', linewidth=1.5, alpha=0.8)\n",
        "\n",
        "axes[2].set_xticks(x)\n",
        "axes[2].set_xticklabels(age_counts.index, fontsize=11, fontweight='bold')\n",
        "axes[2].set_ylabel('N√∫mero de Pacientes', fontweight='bold', fontsize=11)\n",
        "axes[2].set_title('Distribuci√≥n Disease/No Disease por Edad', fontweight='bold', fontsize=12, pad=15)\n",
        "axes[2].legend(fontsize=10)\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# INSIGHTS CLAVE\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüí° INSIGHTS CL√çNICOS CLAVE:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "highest_risk_group = age_stats['Tasa Enfermedad'].idxmax()\n",
        "highest_risk_rate = age_stats.loc[highest_risk_group, 'Tasa Enfermedad']\n",
        "\n",
        "print(f\"  üî¥ Grupo de mayor riesgo: {highest_risk_group} a√±os ({highest_risk_rate:.1f}% con enfermedad)\")\n",
        "\n",
        "# Tendencia\n",
        "age_trend = df_temp.groupby('age_group')['target'].mean()\n",
        "trend_increasing = age_trend.is_monotonic_increasing\n",
        "\n",
        "if trend_increasing:\n",
        "    print(f\"  üìà TENDENCIA: La tasa de enfermedad AUMENTA con la edad (tendencia creciente)\")\n",
        "else:\n",
        "    print(f\"  üìä TENDENCIA: La tasa de enfermedad NO es estrictamente creciente\")\n",
        "\n",
        "# Diferencia entre grupos extremos\n",
        "youngest_rate = age_stats.iloc[0]['Tasa Enfermedad']\n",
        "oldest_rate = age_stats.iloc[-1]['Tasa Enfermedad']\n",
        "diff = oldest_rate - youngest_rate\n",
        "\n",
        "print(f\"  ‚öñÔ∏è Diferencia entre grupos extremos:\")\n",
        "print(f\"     ‚Ä¢ {age_stats.index[0]}: {youngest_rate:.1f}%\")\n",
        "print(f\"     ‚Ä¢ {age_stats.index[-1]}: {oldest_rate:.1f}%\")\n",
        "print(f\"     ‚Ä¢ Diferencia: {diff:+.1f} puntos porcentuales\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "AdQSrMYO9dqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîó FASE 5.8: An√°lisis de Correlaciones\n",
        "\n",
        "### ¬øQu√© mide la correlaci√≥n?\n",
        "\n",
        "La **correlaci√≥n de Pearson** mide relaci√≥n lineal entre variables:\n",
        "- **+1**: Correlaci√≥n positiva perfecta\n",
        "- **0**: Sin correlaci√≥n\n",
        "- **-1**: Correlaci√≥n negativa perfecta\n",
        "\n",
        "### ¬øPor qu√© es importante?\n",
        "\n",
        "1. **Correlaci√≥n con target**: Features m√°s predictivas\n",
        "2. **Multicolinealidad**: Features redundantes (correlaci√≥n alta entre s√≠)\n",
        "3. **Feature selection**: Eliminar features redundantes\n",
        "\n",
        "### Regla pr√°ctica\n",
        "\n",
        "- |r| > 0.7 entre features ‚Üí Considerar eliminar una\n",
        "- |r| > 0.3 con target ‚Üí Feature √∫til para predicci√≥n"
      ],
      "metadata": {
        "id": "kpZo8rne9gMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 5.8: AN√ÅLISIS DE CORRELACIONES\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüîó FASE 5.8: An√°lisis de Correlaciones\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calcular correlaciones\n",
        "features_for_corr = numeric_features + ['target']\n",
        "corr_matrix = df[features_for_corr].corr()\n",
        "\n",
        "# Visualizaci√≥n mejorada\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "# ==========================================\n",
        "# 1. HEATMAP COMPLETA\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä Generando matriz de correlaci√≥n...\")\n",
        "\n",
        "# M√°scara triangular superior\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
        "\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdYlGn', center=0,\n",
        "            square=True, linewidths=1.5, cbar_kws={\"shrink\": 0.8}, ax=axes[0],\n",
        "            vmin=-1, vmax=1, annot_kws={'fontsize': 9})\n",
        "axes[0].set_title('Matriz de Correlaci√≥n (Pearson)\\nTri√°ngulo inferior',\n",
        "                 fontweight='bold', fontsize=14, pad=20)\n",
        "\n",
        "# ==========================================\n",
        "# 2. CORRELACI√ìN CON TARGET\n",
        "# ==========================================\n",
        "\n",
        "target_corr = corr_matrix['target'].drop('target').sort_values(ascending=False)\n",
        "\n",
        "colors = ['#e74c3c' if x > 0 else '#3498db' for x in target_corr.values]\n",
        "bars = axes[1].barh(range(len(target_corr)), target_corr.values, color=colors,\n",
        "                    edgecolor='black', linewidth=1.5, alpha=0.8)\n",
        "\n",
        "axes[1].set_yticks(range(len(target_corr)))\n",
        "axes[1].set_yticklabels(target_corr.index, fontsize=11, fontweight='bold')\n",
        "axes[1].set_xlabel('Correlaci√≥n de Pearson', fontweight='bold', fontsize=12)\n",
        "axes[1].set_title('Correlaci√≥n de Features con Target', fontweight='bold', fontsize=14, pad=20)\n",
        "axes[1].axvline(0, color='black', linewidth=1.5)\n",
        "axes[1].grid(axis='x', alpha=0.3)\n",
        "axes[1].set_xlim([-0.6, 0.6])\n",
        "\n",
        "# Anotaciones\n",
        "for i, v in enumerate(target_corr.values):\n",
        "    x_pos = v + 0.02 if v > 0 else v - 0.02\n",
        "    axes[1].text(x_pos, i, f'{v:.3f}',\n",
        "                va='center', ha='left' if v > 0 else 'right',\n",
        "                fontweight='bold', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# AN√ÅLISIS DE CORRELACIONES\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüèÜ TOP 5 FEATURES M√ÅS CORRELACIONADAS CON TARGET:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, (feat, corr_val) in enumerate(target_corr.head().items(), 1):\n",
        "    direction = \"positiva\" if corr_val > 0 else \"negativa\"\n",
        "    strength = \"fuerte\" if abs(corr_val) > 0.5 else \"moderada\" if abs(corr_val) > 0.3 else \"d√©bil\"\n",
        "    print(f\"   {i}. {feat:15s}: {corr_val:+.3f} (correlaci√≥n {direction} {strength})\")\n",
        "\n",
        "print(\"\\nüí° INTERPRETACI√ìN:\")\n",
        "print(\"-\"*80)\n",
        "print(\"  ‚Ä¢ r > 0:    Correlaci√≥n POSITIVA  ‚Üí ‚Üë feature, ‚Üë probabilidad disease\")\n",
        "print(\"  ‚Ä¢ r < 0:    Correlaci√≥n NEGATIVA  ‚Üí ‚Üë feature, ‚Üì probabilidad disease\")\n",
        "print(\"  ‚Ä¢ |r| > 0.5: Correlaci√≥n FUERTE\")\n",
        "print(\"  ‚Ä¢ |r| > 0.3: Correlaci√≥n MODERADA\")\n",
        "print(\"  ‚Ä¢ |r| < 0.3: Correlaci√≥n D√âBIL\")\n",
        "\n",
        "# ==========================================\n",
        "# MULTICOLINEALIDAD\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è AN√ÅLISIS DE MULTICOLINEALIDAD:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "high_corr_pairs = []\n",
        "for i in range(len(corr_matrix.columns)-1):  # Excluir target\n",
        "    for j in range(i+1, len(corr_matrix.columns)-1):\n",
        "        corr_val = corr_matrix.iloc[i, j]\n",
        "        if abs(corr_val) > 0.7:\n",
        "            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
        "\n",
        "if high_corr_pairs:\n",
        "    print(\"Features con alta correlaci√≥n entre s√≠ (|r| > 0.7):\")\n",
        "    for feat1, feat2, corr_val in high_corr_pairs:\n",
        "        print(f\"   ‚Ä¢ {feat1:15s} ‚Üî {feat2:15s}: {corr_val:+.3f}\")\n",
        "    print(\"\\nüí° Considerar eliminar una de las features altamente correlacionadas\")\n",
        "    print(\"   (conservar la que tenga mayor correlaci√≥n con target)\")\n",
        "else:\n",
        "    print(\"‚úÖ No se detect√≥ multicolinealidad severa\")\n",
        "    print(\"   Todas las correlaciones entre features son |r| < 0.7\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "vjuV7kEB9oGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üé® FASE 5.9: Visualizaciones Multivariadas (Yellowbrick)\n",
        "\n",
        "### ¬øQu√© son visualizaciones multivariadas?\n",
        "\n",
        "T√©cnicas para visualizar **todas las features simult√°neamente** y ver c√≥mo se relacionan con el target.\n",
        "\n",
        "### Herramientas Yellowbrick\n",
        "\n",
        "1. **Parallel Coordinates**: Cada l√≠nea = un paciente, color = clase\n",
        "2. **RadViz**: Proyecci√≥n circular multivariada\n",
        "3. **PCA**: Reducci√≥n dimensional a 2D/3D\n",
        "4. **Manifold (t-SNE/UMAP)**: Proyecciones no lineales\n",
        "\n",
        "### ¬øPara qu√© sirven?\n",
        "\n",
        "- Ver **separabilidad de clases** visualmente\n",
        "- Detectar **clustering natural**\n",
        "- Evaluar **reducci√≥n dimensional**"
      ],
      "metadata": {
        "id": "RB0ag6xT9uKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 5.9: VISUALIZACIONES MULTIVARIADAS (YELLOWBRICK)\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüé® FASE 5.9: Visualizaciones Multivariadas con Yellowbrick\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# ==========================================\n",
        "# PREPARAR DATOS Y MANEJAR NaN\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä Preparando datos para visualizaciones multivariadas...\")\n",
        "\n",
        "# Verificar NaN\n",
        "nan_count = df[numeric_features].isnull().sum().sum()\n",
        "\n",
        "if nan_count > 0:\n",
        "    print(f\"‚ö†Ô∏è Detectados {nan_count} valores NaN en el dataset\")\n",
        "    print(\"   Aplicando imputaci√≥n con la mediana...\")\n",
        "\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    X_clean = imputer.fit_transform(df[numeric_features])\n",
        "    X_df = pd.DataFrame(X_clean, columns=numeric_features)\n",
        "\n",
        "    print(\"‚úÖ Valores NaN imputados con la mediana\")\n",
        "else:\n",
        "    print(\"‚úÖ No hay valores NaN en el dataset\")\n",
        "    X_df = df[numeric_features].copy()\n",
        "\n",
        "# Preparar X e y\n",
        "X = X_df.values\n",
        "y = df['target'].values\n",
        "\n",
        "# Escalar datos\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(f\"‚úÖ Datos preparados: {X.shape[0]} muestras √ó {X.shape[1]} features\")\n",
        "\n",
        "# ==========================================\n",
        "# 1. PARALLEL COORDINATES\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä 1. Generando Parallel Coordinates...\")\n",
        "print(\"   ‚Ä¢ Cada l√≠nea = un paciente\")\n",
        "print(\"   ‚Ä¢ Verde = No Disease | Rojo = Disease\")\n",
        "print(\"   ‚Ä¢ Separaci√≥n entre colores = discriminaci√≥n\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(18, 6))\n",
        "\n",
        "try:\n",
        "    visualizer = ParallelCoordinates(\n",
        "        ax=ax,\n",
        "        features=numeric_features,\n",
        "        classes=['No Disease', 'Disease'],\n",
        "        normalize='standard',\n",
        "        sample=0.5,\n",
        "        shuffle=True,\n",
        "        colors=['#2ecc71', '#e74c3c'],\n",
        "        alpha=0.3\n",
        "    )\n",
        "\n",
        "    visualizer.fit_transform(X, y)\n",
        "    visualizer.finalize()\n",
        "    ax.set_title('Parallel Coordinates: Vista Multivariada de Todas las Features',\n",
        "                fontweight='bold', fontsize=14, pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"‚úÖ Parallel Coordinates completado\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error en Parallel Coordinates: {type(e).__name__}\")\n",
        "    plt.close()\n",
        "\n",
        "# ==========================================\n",
        "# 2. RADVIZ\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä 2. Generando RadViz...\")\n",
        "print(\"   ‚Ä¢ Cada punto = un paciente\")\n",
        "print(\"   ‚Ä¢ Posici√≥n = combinaci√≥n de features\")\n",
        "print(\"   ‚Ä¢ Clustering de colores = separaci√≥n de clases\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "try:\n",
        "    visualizer = RadViz(\n",
        "        ax=ax,\n",
        "        features=numeric_features,\n",
        "        classes=['No Disease', 'Disease'],\n",
        "        colors=['#2ecc71', '#e74c3c'],\n",
        "        alpha=0.6\n",
        "    )\n",
        "\n",
        "    visualizer.fit_transform(X_scaled, y)\n",
        "    visualizer.finalize()\n",
        "    ax.set_title('RadViz: Visualizaci√≥n Radial Multivariada',\n",
        "                fontweight='bold', fontsize=14, pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"‚úÖ RadViz completado\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error en RadViz: {type(e).__name__}\")\n",
        "    plt.close()\n",
        "\n",
        "# ==========================================\n",
        "# 3. PCA DECOMPOSITION (MANUAL - M√ÅS ROBUSTO)\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä 3. Generando PCA Decomposition...\")\n",
        "print(\"   ‚Ä¢ Reducci√≥n dimensional a 2D\")\n",
        "print(\"   ‚Ä¢ Preserva m√°xima varianza\")\n",
        "\n",
        "try:\n",
        "    # Realizar PCA manualmente (m√°s robusto que Yellowbrick)\n",
        "    pca = PCA(n_components=3, random_state=42)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Varianza explicada\n",
        "    var_ratios = pca.explained_variance_ratio_\n",
        "    var_pc1 = var_ratios[0] * 100\n",
        "    var_pc2 = var_ratios[1] * 100\n",
        "    var_pc3 = var_ratios[2] * 100\n",
        "    var_total = sum(var_ratios[:2]) * 100\n",
        "\n",
        "    # Crear visualizaci√≥n\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # PCA 2D\n",
        "    for target_val, color, label in [(0, '#2ecc71', 'No Disease'), (1, '#e74c3c', 'Disease')]:\n",
        "        mask = y == target_val\n",
        "        axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1],\n",
        "                       c=color, label=label, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "    axes[0].set_xlabel(f'PC1 ({var_pc1:.1f}% varianza)', fontweight='bold', fontsize=11)\n",
        "    axes[0].set_ylabel(f'PC2 ({var_pc2:.1f}% varianza)', fontweight='bold', fontsize=11)\n",
        "    axes[0].set_title(f'PCA: Proyecci√≥n 2D\\nVarianza total: {var_total:.1f}%',\n",
        "                     fontweight='bold', fontsize=12, pad=15)\n",
        "    axes[0].legend(fontsize=10)\n",
        "    axes[0].grid(alpha=0.3)\n",
        "\n",
        "    # PCA 3D usando subplot est√°ndar (no 3D para evitar errores)\n",
        "    # Mostramos PC1 vs PC3 en su lugar\n",
        "    for target_val, color, label in [(0, '#2ecc71', 'No Disease'), (1, '#e74c3c', 'Disease')]:\n",
        "        mask = y == target_val\n",
        "        axes[1].scatter(X_pca[mask, 0], X_pca[mask, 2],\n",
        "                       c=color, label=label, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "    axes[1].set_xlabel(f'PC1 ({var_pc1:.1f}% varianza)', fontweight='bold', fontsize=11)\n",
        "    axes[1].set_ylabel(f'PC3 ({var_pc3:.1f}% varianza)', fontweight='bold', fontsize=11)\n",
        "    axes[1].set_title(f'PCA: Proyecci√≥n PC1 vs PC3\\nPC3: {var_pc3:.1f}% varianza',\n",
        "                     fontweight='bold', fontsize=12, pad=15)\n",
        "    axes[1].legend(fontsize=10)\n",
        "    axes[1].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"‚úÖ PCA completado\")\n",
        "    print(f\"   ‚Ä¢ Varianza PC1: {var_pc1:.1f}%\")\n",
        "    print(f\"   ‚Ä¢ Varianza PC2: {var_pc2:.1f}%\")\n",
        "    print(f\"   ‚Ä¢ Varianza PC3: {var_pc3:.1f}%\")\n",
        "    print(f\"   ‚Ä¢ Varianza total (PC1+PC2): {var_total:.1f}%\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error en PCA: {type(e).__name__}\")\n",
        "    print(f\"   Detalles: {str(e)[:200]}\")\n",
        "    var_total = 0\n",
        "    plt.close()\n",
        "\n",
        "# ==========================================\n",
        "# 4. t-SNE (MANUAL - M√ÅS ROBUSTO)\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä 4. Generando t-SNE...\")\n",
        "print(\"   ‚è±Ô∏è Esto puede tomar 30-60 segundos...\")\n",
        "\n",
        "try:\n",
        "    # Realizar t-SNE manualmente\n",
        "    print(\"   üìä Calculando t-SNE (preserva estructura local)...\")\n",
        "\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "    X_tsne = tsne.fit_transform(X_scaled)\n",
        "\n",
        "    # Crear visualizaci√≥n\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    for target_val, color, label in [(0, '#2ecc71', 'No Disease'), (1, '#e74c3c', 'Disease')]:\n",
        "        mask = y == target_val\n",
        "        ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1],\n",
        "                  c=color, label=label, alpha=0.6, s=60, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "    ax.set_xlabel('t-SNE Component 1', fontweight='bold', fontsize=11)\n",
        "    ax.set_ylabel('t-SNE Component 2', fontweight='bold', fontsize=11)\n",
        "    ax.set_title('t-SNE: Proyecci√≥n No Lineal\\n(Preserva estructura local - √ötil para detectar clusters)',\n",
        "                fontweight='bold', fontsize=13, pad=20)\n",
        "    ax.legend(fontsize=11, loc='best')\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"   ‚úÖ t-SNE completado exitosamente\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error en t-SNE: {type(e).__name__}\")\n",
        "    print(f\"      Detalles: {str(e)[:200]}\")\n",
        "    plt.close()\n",
        "\n",
        "# ==========================================\n",
        "# 5. UMAP (MANUAL - SI EST√Å DISPONIBLE)\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä 5. Generando UMAP (opcional)...\")\n",
        "\n",
        "# Verificar si UMAP est√° disponible\n",
        "UMAP_AVAILABLE = False\n",
        "try:\n",
        "    import umap.umap_ as umap\n",
        "    UMAP_AVAILABLE = True\n",
        "    print(\"   ‚úÖ UMAP detectado y disponible\")\n",
        "except ImportError:\n",
        "    print(\"   ‚è≠Ô∏è UMAP no disponible - omitiendo\")\n",
        "    print(\"   üí° Para instalar: !pip install umap-learn\")\n",
        "\n",
        "if UMAP_AVAILABLE:\n",
        "    try:\n",
        "        print(\"   üìä Calculando UMAP (preserva estructura global)...\")\n",
        "        print(\"   ‚è±Ô∏è Esto puede tomar 20-40 segundos...\")\n",
        "\n",
        "        # Realizar UMAP manualmente\n",
        "        reducer = umap.UMAP(n_neighbors=30, min_dist=0.3, metric='euclidean', random_state=42)\n",
        "        X_umap = reducer.fit_transform(X_scaled)\n",
        "\n",
        "        # Crear visualizaci√≥n\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "        for target_val, color, label in [(0, '#2ecc71', 'No Disease'), (1, '#e74c3c', 'Disease')]:\n",
        "            mask = y == target_val\n",
        "            ax.scatter(X_umap[mask, 0], X_umap[mask, 1],\n",
        "                      c=color, label=label, alpha=0.6, s=60, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "        ax.set_xlabel('UMAP Component 1', fontweight='bold', fontsize=11)\n",
        "        ax.set_ylabel('UMAP Component 2', fontweight='bold', fontsize=11)\n",
        "        ax.set_title('UMAP: Proyecci√≥n No Lineal\\n(Preserva estructura global - M√°s r√°pido que t-SNE)',\n",
        "                    fontweight='bold', fontsize=13, pad=20)\n",
        "        ax.legend(fontsize=11, loc='best')\n",
        "        ax.grid(alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"   ‚úÖ UMAP completado exitosamente\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error en UMAP: {type(e).__name__}\")\n",
        "        print(f\"      Detalles: {str(e)[:200]}\")\n",
        "        plt.close()\n",
        "\n",
        "# ==========================================\n",
        "# INTERPRETACI√ìN GENERAL\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üí° INTERPRETACI√ìN DE VISUALIZACIONES MULTIVARIADAS:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "  üìä PARALLEL COORDINATES:\n",
        "    ‚Ä¢ L√≠neas separadas por color ‚Üí Features discriminativas\n",
        "    ‚Ä¢ L√≠neas mezcladas ‚Üí Features poco √∫tiles\n",
        "    ‚Ä¢ Cada eje = una feature\n",
        "    ‚Ä¢ √ötil para detectar patrones en todas las features simult√°neamente\n",
        "\n",
        "  üéØ RADVIZ:\n",
        "    ‚Ä¢ Clusters de verde vs rojo ‚Üí Clases bien separadas\n",
        "    ‚Ä¢ Mezcla de colores ‚Üí Overlap entre clases\n",
        "    ‚Ä¢ Cada punto = un paciente proyectado en c√≠rculo\n",
        "    ‚Ä¢ √ötil para visualizar separabilidad multidimensional\n",
        "\n",
        "  üìê PCA (Principal Component Analysis):\n",
        "    ‚Ä¢ Varianza alta en PC1+PC2 ‚Üí Buena representaci√≥n 2D\n",
        "    ‚Ä¢ Clusters separados ‚Üí Clases distinguibles linealmente\n",
        "    ‚Ä¢ T√©cnica LINEAL - Preserva varianza m√°xima\n",
        "    ‚Ä¢ PC1: Direcci√≥n de m√°xima variabilidad en los datos\n",
        "\n",
        "  üîÆ t-SNE (t-Distributed Stochastic Neighbor Embedding):\n",
        "    ‚Ä¢ Clusters bien definidos ‚Üí Clases muy separables\n",
        "    ‚Ä¢ Overlap ‚Üí Dificultad para clasificar\n",
        "    ‚Ä¢ T√©cnica NO LINEAL - Preserva vecindarios LOCALES\n",
        "    ‚Ä¢ Excelente para detectar clusters y agrupaciones\n",
        "\n",
        "  üåê UMAP (Uniform Manifold Approximation and Projection):\n",
        "    ‚Ä¢ Similar a t-SNE pero m√°s r√°pido\n",
        "    ‚Ä¢ Preserva estructura GLOBAL mejor que t-SNE\n",
        "    ‚Ä¢ √ötil para grandes datasets\n",
        "    ‚Ä¢ Clusters + relaciones entre clusters\n",
        "\"\"\")\n",
        "\n",
        "# Resumen de separabilidad\n",
        "print(\"\\nüìä RESUMEN DE SEPARABILIDAD DE CLASES:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "separability_score = 0\n",
        "total_techniques = 0\n",
        "\n",
        "if var_total > 0:\n",
        "    total_techniques += 1\n",
        "    if var_total > 70:\n",
        "        print(\"  ‚úÖ PCA: Buena separabilidad lineal\")\n",
        "        separability_score += 1\n",
        "    elif var_total > 50:\n",
        "        print(\"  ‚ö†Ô∏è PCA: Separabilidad moderada\")\n",
        "    else:\n",
        "        print(\"  ‚ùå PCA: Baja separabilidad lineal\")\n",
        "\n",
        "print(\"  ‚úÖ t-SNE: Visualizaci√≥n completada (revisar gr√°fico para separabilidad)\")\n",
        "total_techniques += 1\n",
        "\n",
        "if UMAP_AVAILABLE:\n",
        "    print(\"  ‚úÖ UMAP: Visualizaci√≥n completada (revisar gr√°fico para separabilidad)\")\n",
        "    total_techniques += 1\n",
        "\n",
        "print(f\"\\nüí° Se completaron {total_techniques} t√©cnicas de reducci√≥n dimensional\")\n",
        "print(\"   Revisa los gr√°ficos para evaluar qu√© tan separables son las clases\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "xVtufx-Q9xdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ FASE 5.10: Resumen Ejecutivo del EDA\n",
        "\n",
        "S√≠ntesis de todos los hallazgos del An√°lisis Exploratorio."
      ],
      "metadata": {
        "id": "_9UMD_A792bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 5.10: REPORTE AUTOM√ÅTICO CON SWEETVIZ\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüç≠ FASE 5.10: Reporte Autom√°tico Profesional con Sweetviz\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë                    SWEETVIZ - REPORTE EDA PROFESIONAL                      ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "üìä GENERA REPORTE HTML VISUAL COMPLETO CON:\n",
        "\n",
        "   ‚úÖ An√°lisis univariado de todas las features\n",
        "   ‚úÖ Comparaci√≥n Disease vs No Disease\n",
        "   ‚úÖ Visualizaciones de distribuciones\n",
        "   ‚úÖ Correlaciones y asociaciones\n",
        "   ‚úÖ Missing values y tipos de datos\n",
        "   ‚úÖ Estad√≠sticas descriptivas completas\n",
        "\n",
        "‚ö° VENTAJAS DE SWEETVIZ:\n",
        "   ‚Ä¢ R√ÅPIDO: 30-60 segundos\n",
        "   ‚Ä¢ Excelente para COMPARACIONES entre grupos\n",
        "   ‚Ä¢ Interfaz visual profesional\n",
        "   ‚Ä¢ Ideal para stakeholders no t√©cnicos\n",
        "\"\"\")\n",
        "\n",
        "# ==========================================\n",
        "# WORKAROUND PARA NUMPY 2.0\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüîß Aplicando workaround para compatibilidad NumPy 2.0...\")\n",
        "\n",
        "\n",
        "# Patchear NumPy para compatibilidad con Sweetviz\n",
        "if not hasattr(np, 'VisibleDeprecationWarning'):\n",
        "    # Crear el atributo faltante\n",
        "    np.VisibleDeprecationWarning = DeprecationWarning\n",
        "    print(\"   ‚úÖ Workaround NumPy aplicado: VisibleDeprecationWarning agregado\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Configuraci√≥n de compatibilidad completada\")\n",
        "\n",
        "# ==========================================\n",
        "# GENERAR REPORTE SWEETVIZ\n",
        "# ==========================================\n",
        "\n",
        "GENERAR_SWEETVIZ = True  # Cambiar a False para omitir\n",
        "\n",
        "if GENERAR_SWEETVIZ:\n",
        "    print(\"\\n‚è≥ Generando reporte Sweetviz (30-60 seg)...\")\n",
        "\n",
        "    try:\n",
        "        import sweetviz as sv\n",
        "\n",
        "        print(\"   üìä Comparando: Disease vs No Disease...\")\n",
        "\n",
        "        # Generar reporte\n",
        "        report = sv.compare_intra(\n",
        "            df,\n",
        "            df['target'] == 1,  # Condici√≥n: target = 1 (Disease)\n",
        "            ['Disease', 'No Disease'],  # Labels para los grupos\n",
        "            target_feat='target'\n",
        "        )\n",
        "\n",
        "        # Guardar reporte\n",
        "        output_file = \"heart_disease_sweetviz_report.html\"\n",
        "        report.show_html(output_file, open_browser=False, layout='vertical')\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"‚úÖ REPORTE SWEETVIZ GENERADO EXITOSAMENTE!\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nüìÑ Archivo: {output_file}\")\n",
        "        print(f\"üìä Tipo: Comparaci√≥n Disease vs No Disease\")\n",
        "\n",
        "        print(\"\\nüí° C√ìMO USAR EL REPORTE:\")\n",
        "        print(\"-\"*80)\n",
        "        print(\"   1. Descarga el archivo .html de Google Colab:\")\n",
        "        print(\"      ‚Ä¢ Click en üìÅ (Files) en sidebar izquierdo\")\n",
        "        print(\"      ‚Ä¢ Busca 'heart_disease_sweetviz_report.html'\")\n",
        "        print(\"      ‚Ä¢ Click derecho ‚Üí Download\")\n",
        "        print(\"\\n   2. Abre en tu navegador:\")\n",
        "        print(\"      ‚Ä¢ Doble click en el archivo descargado\")\n",
        "        print(\"      ‚Ä¢ El reporte es interactivo y autocontenido\")\n",
        "\n",
        "        print(\"\\nüéØ CONTENIDO DEL REPORTE:\")\n",
        "        print(\"-\"*80)\n",
        "        print(\"   ‚úÖ Comparaciones visuales Disease vs No Disease\")\n",
        "        print(\"   ‚úÖ Estad√≠sticas por grupo\")\n",
        "        print(\"   ‚úÖ Correlaciones y asociaciones\")\n",
        "        print(\"   ‚úÖ Distribuciones interactivas\")\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"\\n‚ùå Error importando Sweetviz: {e}\")\n",
        "        print(\"   üí° Soluci√≥n: !pip install sweetviz\")\n",
        "\n",
        "    except AttributeError as e:\n",
        "        print(f\"\\n‚ùå Error de compatibilidad NumPy: {e}\")\n",
        "        print(\"   ‚ö†Ô∏è Sweetviz no es completamente compatible con NumPy 2.0\")\n",
        "        print(\"   üí° El an√°lisis manual (Fases 5.1-5.9) ya es completo y profesional\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error generando reporte: {type(e).__name__}\")\n",
        "        print(f\"   Detalles: {str(e)[:300]}\")\n",
        "        print(\"\\nüí° ALTERNATIVAS:\")\n",
        "        print(\"   1. El an√°lisis manual con Yellowbrick es exhaustivo\")\n",
        "        print(\"   2. Usa SOLUCI√ìN 2 (an√°lisis manual con reportes personalizados)\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚è≠Ô∏è GENERACI√ìN DE SWEETVIZ OMITIDA\")\n",
        "    print(\"   Para generar: GENERAR_SWEETVIZ = True\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "-CTfVKtK95XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÑ REPORTES AUTOM√ÅTICOS PROFESIONALES\n",
        "\n",
        "## Transici√≥n: Manual ‚Üí Autom√°tico\n",
        "\n",
        "Hasta ahora hemos realizado **an√°lisis manual detallado** para entender **c√≥mo** y **por qu√©**.\n",
        "\n",
        "Ahora usaremos herramientas de **automatizaci√≥n profesional** para:\n",
        "1. Generar reportes completos en segundos\n",
        "2. Compartir con stakeholders no t√©cnicos\n",
        "3. Documentar an√°lisis de forma reproducible\n",
        "4. Simular workflow empresarial\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Herramientas de Automatizaci√≥n\n",
        "\n",
        "### 1. YData-Profiling (antes Pandas-Profiling)\n",
        "\n",
        "**¬øQu√© hace?**\n",
        "- Genera reporte HTML interactivo completo\n",
        "- Incluye: distribuciones, correlaciones, missing values, duplicados\n",
        "- An√°lisis de interacciones entre variables\n",
        "- Alertas autom√°ticas de calidad de datos\n",
        "\n",
        "**Ventajas:**\n",
        "- ‚úÖ An√°lisis exhaustivo autom√°tico\n",
        "- ‚úÖ Interactivo (explorable en navegador)\n",
        "- ‚úÖ Profesional para presentaciones\n",
        "\n",
        "**Desventajas:**\n",
        "- ‚ùå Lento (2-5 minutos en datasets grandes)\n",
        "- ‚ùå Menos control sobre detalles\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Sweetviz\n",
        "\n",
        "**¬øQu√© hace?**\n",
        "- Genera reporte visual r√°pido\n",
        "- **Especializado en comparaciones** (Train vs Test, Disease vs No Disease)\n",
        "- EDA visual autom√°tico\n",
        "\n",
        "**Ventajas:**\n",
        "- ‚úÖ Muy r√°pido (30-60 segundos)\n",
        "- ‚úÖ Excelente para comparaciones entre grupos\n",
        "- ‚úÖ Interfaz visual atractiva\n",
        "\n",
        "**Desventajas:**\n",
        "- ‚ùå Menos detallado que YData-Profiling\n",
        "- ‚ùå No interactivo (est√°tico)\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Cu√°ndo Usar Cada Uno\n",
        "\n",
        "| Escenario | Herramienta | Raz√≥n |\n",
        "|-----------|-------------|-------|\n",
        "| **Exploraci√≥n inicial r√°pida** | Sweetviz | Velocidad |\n",
        "| **An√°lisis exhaustivo** | YData-Profiling | Profundidad |\n",
        "| **Comparar grupos** | Sweetviz | Especializaci√≥n |\n",
        "| **Presentar a stakeholders** | YData-Profiling | Profesionalismo |\n",
        "| **Documentaci√≥n proyecto** | Ambos | Complementarios |\n",
        "| **Entender a fondo** | Manual (Fases 5.1-5.9) | Control total |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Nota Importante\n",
        "\n",
        "Los reportes autom√°ticos **complementan** el an√°lisis manual, no lo reemplazan:\n",
        "- ‚úÖ An√°lisis manual ‚Üí Entiendes los datos\n",
        "- ‚úÖ Reportes autom√°ticos ‚Üí Comunicas eficientemente\n",
        "\n",
        "**Data Scientists profesionales usan ambos.**"
      ],
      "metadata": {
        "id": "cMjHEV_A-HFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 5.10: YDATA-PROFILING - REPORTE COMPLETO\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìÑ FASE 5.10: Reporte Autom√°tico con YData-Profiling\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë                    YDATA-PROFILING (PANDAS-PROFILING)                      ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "üìä GENERA REPORTE HTML INTERACTIVO COMPLETO CON:\n",
        "\n",
        "   ‚úÖ Overview: Estad√≠sticas generales del dataset\n",
        "   ‚úÖ Variables: An√°lisis detallado de cada feature\n",
        "   ‚úÖ Interactions: Scatterplots entre pares de variables\n",
        "   ‚úÖ Correlations: Matrices de Pearson, Spearman, Kendall, Phi_k\n",
        "   ‚úÖ Missing Values: An√°lisis de datos faltantes\n",
        "   ‚úÖ Sample: Primeras y √∫ltimas filas\n",
        "   ‚úÖ Duplicate Rows: Detecci√≥n de duplicados\n",
        "   ‚úÖ Alerts: Advertencias autom√°ticas de calidad\n",
        "\n",
        "‚ö†Ô∏è IMPORTANTE:\n",
        "   ‚Ä¢ Generaci√≥n puede tomar 2-5 minutos\n",
        "   ‚Ä¢ El archivo HTML es interactivo (abrir en navegador)\n",
        "   ‚Ä¢ Peso aprox: 5-10 MB\n",
        "\n",
        "üéØ USO PROFESIONAL:\n",
        "   ‚Ä¢ Documentaci√≥n de proyectos\n",
        "   ‚Ä¢ Reportes para stakeholders\n",
        "   ‚Ä¢ Auditor√≠as de calidad de datos\n",
        "\"\"\")\n",
        "\n",
        "# Opci√≥n para generar o no (configurable)\n",
        "GENERAR_YDATA_PROFILING = True  # Cambiar a False para omitir\n",
        "\n",
        "if GENERAR_YDATA_PROFILING:\n",
        "    print(\"\\n‚è≥ Generando reporte YData-Profiling (esto tomar√° 2-5 min)...\")\n",
        "    print(\"   Por favor espera, es normal que tome tiempo...\")\n",
        "\n",
        "    try:\n",
        "        from ydata_profiling import ProfileReport\n",
        "\n",
        "        # Crear reporte con configuraci√≥n √≥ptima\n",
        "        profile = ProfileReport(\n",
        "            df,\n",
        "            title=\"Heart Disease - EDA Report (YData-Profiling)\",\n",
        "            dataset={\n",
        "                \"description\": \"UCI Heart Disease Dataset - Medical Diagnosis\",\n",
        "                \"creator\": \"Eduardo Far√≠as - IEI-097\",\n",
        "                \"url\": \"https://archive.ics.uci.edu/ml/datasets/heart+disease\"\n",
        "            },\n",
        "            variables={\n",
        "                \"descriptions\": {\n",
        "                    \"target\": \"0 = No Disease, 1 = Disease\"\n",
        "                }\n",
        "            },\n",
        "            # Configuraci√≥n para optimizar velocidad\n",
        "            explorative=True,\n",
        "            minimal=False,\n",
        "            correlations={\n",
        "                \"pearson\": {\"calculate\": True},\n",
        "                \"spearman\": {\"calculate\": True},\n",
        "                \"kendall\": {\"calculate\": False},  # Muy lento, desactivar\n",
        "                \"phi_k\": {\"calculate\": True},\n",
        "                \"cramers\": {\"calculate\": False}\n",
        "            },\n",
        "            missing_diagrams={\n",
        "                \"heatmap\": True,\n",
        "                \"dendrogram\": True,\n",
        "                \"matrix\": True,\n",
        "                \"bar\": True\n",
        "            },\n",
        "            interactions={\n",
        "                \"continuous\": True,\n",
        "                \"targets\": [\"target\"]  # Analizar interacciones con target\n",
        "            },\n",
        "            samples={\n",
        "                \"head\": 10,\n",
        "                \"tail\": 10,\n",
        "                \"random\": 10\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Guardar HTML\n",
        "        output_file = \"heart_disease_ydata_profiling.html\"\n",
        "        profile.to_file(output_file)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"‚úÖ REPORTE YDATA-PROFILING GENERADO EXITOSAMENTE!\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nüìÑ Archivo: {output_file}\")\n",
        "        print(f\"üìä Tama√±o: ~{profile.to_html().__sizeof__() / 1024 / 1024:.1f} MB (estimado)\")\n",
        "\n",
        "        print(\"\\nüí° C√ìMO USAR EL REPORTE:\")\n",
        "        print(\"-\"*80)\n",
        "        print(\"   1. Descarga el archivo .html de Google Colab:\")\n",
        "        print(\"      ‚Ä¢ Click en üìÅ (Files) en sidebar izquierdo\")\n",
        "        print(\"      ‚Ä¢ Click derecho en 'heart_disease_ydata_profiling.html'\")\n",
        "        print(\"      ‚Ä¢ Download\")\n",
        "        print(\"\\n   2. Abre el archivo en tu navegador:\")\n",
        "        print(\"      ‚Ä¢ Doble click en el archivo descargado\")\n",
        "        print(\"      ‚Ä¢ Explora las pesta√±as: Overview, Variables, Interactions, etc.\")\n",
        "        print(\"\\n   3. Comparte con stakeholders:\")\n",
        "        print(\"      ‚Ä¢ El reporte es standalone (no requiere Python)\")\n",
        "        print(\"      ‚Ä¢ Ideal para presentaciones y documentaci√≥n\")\n",
        "\n",
        "        print(\"\\nüìä SECCIONES DEL REPORTE:\")\n",
        "        print(\"-\"*80)\n",
        "        print(\"   ‚Ä¢ Overview:       Resumen general del dataset\")\n",
        "        print(\"   ‚Ä¢ Variables:      An√°lisis detallado por feature\")\n",
        "        print(\"   ‚Ä¢ Interactions:   Scatterplots target vs features\")\n",
        "        print(\"   ‚Ä¢ Correlations:   Matrices de correlaci√≥n m√∫ltiples\")\n",
        "        print(\"   ‚Ä¢ Missing values: An√°lisis de datos faltantes\")\n",
        "        print(\"   ‚Ä¢ Sample:         Datos de ejemplo\")\n",
        "        print(\"   ‚Ä¢ Alerts:         ‚ö†Ô∏è Problemas detectados autom√°ticamente\")\n",
        "\n",
        "        # Mostrar en Colab (opcional - puede ser pesado)\n",
        "        # profile.to_notebook_iframe()\n",
        "\n",
        "        print(\"\\nüí° INSIGHTS CLAVE DEL REPORTE:\")\n",
        "        print(\"-\"*80)\n",
        "        print(\"   ‚úÖ Revisa la secci√≥n 'Alerts' primero (problemas cr√≠ticos)\")\n",
        "        print(\"   ‚úÖ Explora 'Interactions' para ver features vs target\")\n",
        "        print(\"   ‚úÖ Compara correlaciones Pearson vs Spearman vs Phi_k\")\n",
        "        print(\"   ‚úÖ Analiza 'Variables' para features con advertencias\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"‚ö†Ô∏è ERROR AL GENERAR REPORTE YDATA-PROFILING\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"   Tipo: {type(e).__name__}\")\n",
        "        print(f\"   Detalles: {str(e)[:200]}\")\n",
        "        print(\"\\nüí° POSIBLES SOLUCIONES:\")\n",
        "        print(\"   1. Verificar instalaci√≥n: !pip install -q ydata-profiling\")\n",
        "        print(\"   2. Reiniciar runtime si falla\")\n",
        "        print(\"   3. El an√°lisis manual (Fases 5.1-5.9) ya est√° completo\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚è≠Ô∏è GENERACI√ìN DE YDATA-PROFILING OMITIDA\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"   Configuraci√≥n: GENERAR_YDATA_PROFILING = False\")\n",
        "    print(\"\\nüí° Para generar el reporte:\")\n",
        "    print(\"   1. Cambia GENERAR_YDATA_PROFILING = True\")\n",
        "    print(\"   2. Vuelve a ejecutar esta celda\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "z6DgDKZ2-KOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 5.11: SWEETVIZ - COMPARACI√ìN R√ÅPIDA\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüç≠ FASE 5.11: Reporte R√°pido con Sweetviz\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë                           SWEETVIZ - EDA R√ÅPIDO                            ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "üìä GENERA REPORTE HTML VISUAL COMPARATIVO CON:\n",
        "\n",
        "   ‚úÖ An√°lisis univariado de todas las features\n",
        "   ‚úÖ Comparaci√≥n entre grupos (Disease vs No Disease)\n",
        "   ‚úÖ Visualizaciones de distribuciones\n",
        "   ‚úÖ Correlaciones y asociaciones\n",
        "   ‚úÖ Missing values y tipos de datos\n",
        "\n",
        "‚ö° VENTAJAS VS YDATA-PROFILING:\n",
        "   ‚Ä¢ Mucho m√°s R√ÅPIDO (30-60 seg vs 2-5 min)\n",
        "   ‚Ä¢ Excelente para COMPARACIONES entre grupos\n",
        "   ‚Ä¢ Interfaz visual m√°s atractiva\n",
        "   ‚Ä¢ Ideal para EDA exploratorio inicial\n",
        "\n",
        "‚ö†Ô∏è LIMITACIONES:\n",
        "   ‚Ä¢ Menos detallado que YData-Profiling\n",
        "   ‚Ä¢ No incluye an√°lisis de interacciones profundo\n",
        "   ‚Ä¢ Reporte est√°tico (menos interactivo)\n",
        "\n",
        "üéØ USO PROFESIONAL:\n",
        "   ‚Ä¢ Comparar Train vs Test sets\n",
        "   ‚Ä¢ Comparar Disease vs No Disease\n",
        "   ‚Ä¢ EDA r√°pido antes de modelado\n",
        "   ‚Ä¢ Reportes para revisi√≥n r√°pida\n",
        "\"\"\")\n",
        "\n",
        "# Opci√≥n para generar o no\n",
        "GENERAR_SWEETVIZ = True  # Cambiar a False para omitir\n",
        "\n",
        "if GENERAR_SWEETVIZ:\n",
        "    print(\"\\n‚è≥ Generando reporte Sweetviz (30-60 seg)...\")\n",
        "\n",
        "    try:\n",
        "        import sweetviz as sv\n",
        "\n",
        "        # ==========================================\n",
        "        # OPCI√ìN 1: AN√ÅLISIS SIMPLE (TODO EL DATASET)\n",
        "        # ==========================================\n",
        "\n",
        "        # report = sv.analyze(df, target_feat='target')\n",
        "\n",
        "        # ==========================================\n",
        "        # OPCI√ìN 2: COMPARACI√ìN (DISEASE VS NO DISEASE) ‚≠ê RECOMENDADO\n",
        "        # ==========================================\n",
        "\n",
        "        print(\"   üìä Comparando: Disease vs No Disease...\")\n",
        "        print(\"   Esto genera insights m√°s valiosos que an√°lisis simple\")\n",
        "\n",
        "        report = sv.compare_intra(\n",
        "            df,\n",
        "            df['target'] == 1,  # Condici√≥n: target = 1 (Disease)\n",
        "            ['Disease', 'No Disease'],  # Labels para los grupos\n",
        "            target_feat='target'\n",
        "        )\n",
        "\n",
        "        # Guardar reporte\n",
        "        output_file = \"heart_disease_sweetviz.html\"\n",
        "        report.show_html(output_file, open_browser=False, layout='vertical')\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"‚úÖ REPORTE SWEETVIZ GENERADO EXITOSAMENTE!\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nüìÑ Archivo: {output_file}\")\n",
        "\n",
        "        print(\"\\nüí° C√ìMO USAR EL REPORTE:\")\n",
        "        print(\"-\"*80)\n",
        "        print(\"   1. Descarga el archivo .html de Google Colab:\")\n",
        "        print(\"      ‚Ä¢ Click en üìÅ (Files) en sidebar\")\n",
        "        print(\"      ‚Ä¢ Click derecho en 'heart_disease_sweetviz.html'\")\n",
        "        print(\"      ‚Ä¢ Download\")\n",
        "        print(\"\\n   2. Abre en tu navegador:\")\n",
        "        print(\"      ‚Ä¢ Doble click en el archivo descargado\")\n",
        "        print(\"      ‚Ä¢ Navegaci√≥n vertical por features\")\n",
        "        print(\"\\n   3. Explora comparaciones:\")\n",
        "        print(\"      ‚Ä¢ Cada feature muestra Disease vs No Disease\")\n",
        "        print(\"      ‚Ä¢ Barras azules y naranjas para comparar distribuciones\")\n",
        "        print(\"      ‚Ä¢ Estad√≠sticas side-by-side\")\n",
        "\n",
        "        print(\"\\nüìä QU√â VER EN EL REPORTE:\")\n",
        "        print(\"-\"*80)\n",
        "        print(\"   üîç COMPARACIONES CLAVE:\")\n",
        "        print(\"      ‚Ä¢ Features con mayor diferencia entre grupos\")\n",
        "        print(\"      ‚Ä¢ Distribuciones separadas vs superpuestas\")\n",
        "        print(\"      ‚Ä¢ Valores extremos en cada grupo\")\n",
        "        print(\"\\n   üìà M√âTRICAS AUTOM√ÅTICAS:\")\n",
        "        print(\"      ‚Ä¢ Associations (correlaciones)\")\n",
        "        print(\"      ‚Ä¢ Type (num√©rico/categ√≥rico)\")\n",
        "        print(\"      ‚Ä¢ Missing values\")\n",
        "        print(\"      ‚Ä¢ Distinct values\")\n",
        "\n",
        "        print(\"\\nüéØ INSIGHTS ESPERADOS:\")\n",
        "        print(\"-\"*80)\n",
        "        print(\"   ‚úÖ Features donde Disease y No Disease difieren mucho ‚Üí √ötiles para predicci√≥n\")\n",
        "        print(\"   ‚úÖ Features con distribuciones similares ‚Üí Menos discriminativas\")\n",
        "        print(\"   ‚úÖ Patrones cl√≠nicos visuales (ej: mayor edad ‚Üí mayor Disease %)\")\n",
        "\n",
        "        print(\"\\nüèÜ COMPARACI√ìN: SWEETVIZ vs YDATA-PROFILING:\")\n",
        "        print(\"-\"*80)\n",
        "        print(\"   SWEETVIZ:          R√°pido, visual, comparaciones\")\n",
        "        print(\"   YDATA-PROFILING:   Completo, interactivo, profundo\")\n",
        "        print(\"   RECOMENDACI√ìN:     Usar AMBOS (complementarios)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"‚ö†Ô∏è ERROR AL GENERAR REPORTE SWEETVIZ\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"   Tipo: {type(e).__name__}\")\n",
        "        print(f\"   Detalles: {str(e)[:200]}\")\n",
        "        print(\"\\nüí° POSIBLES SOLUCIONES:\")\n",
        "        print(\"   1. Verificar instalaci√≥n: !pip install -q sweetviz\")\n",
        "        print(\"   2. Reiniciar runtime si falla\")\n",
        "        print(\"   3. El an√°lisis manual ya est√° completo\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚è≠Ô∏è GENERACI√ìN DE SWEETVIZ OMITIDA\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"   Configuraci√≥n: GENERAR_SWEETVIZ = False\")\n",
        "    print(\"\\nüí° Para generar el reporte:\")\n",
        "    print(\"   1. Cambia GENERAR_SWEETVIZ = True\")\n",
        "    print(\"   2. Vuelve a ejecutar esta celda\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "URpZnPfU-QLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä COMPARACI√ìN: Manual vs Autom√°tico\n",
        "\n",
        "### An√°lisis Realizado en este EDA\n",
        "\n",
        "| Aspecto | Manual (5.1-5.9) | YData-Profiling | Sweetviz |\n",
        "|---------|------------------|-----------------|----------|\n",
        "| **Tiempo** | 10-15 min | 2-5 min | 30-60 seg |\n",
        "| **Control** | ‚úÖ Total | ‚ùå Limitado | ‚ùå Limitado |\n",
        "| **Profundidad** | ‚úÖ M√°xima | ‚úÖ Alta | ‚ö†Ô∏è Media |\n",
        "| **Personalizaci√≥n** | ‚úÖ Total | ‚ùå Baja | ‚ùå Baja |\n",
        "| **Comparaciones** | ‚ö†Ô∏è Manual | ‚ùå No | ‚úÖ S√≠ |\n",
        "| **Interactividad** | ‚ùå No | ‚úÖ Alta | ‚ö†Ô∏è Media |\n",
        "| **Presentabilidad** | ‚ö†Ô∏è Jupyter | ‚úÖ‚úÖ Muy alta | ‚úÖ Alta |\n",
        "| **Aprendizaje** | ‚úÖ‚úÖ M√°ximo | ‚ùå Bajo | ‚ùå Bajo |\n",
        "\n",
        "---\n",
        "\n",
        "### Estrategia Profesional Recomendada\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ              WORKFLOW EDA PROFESIONAL                       ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                             ‚îÇ\n",
        "‚îÇ  1. EXPLORACI√ìN INICIAL (5 min)                            ‚îÇ\n",
        "‚îÇ     ‚îî‚îÄ‚îÄ Sweetviz: Vista r√°pida del dataset                 ‚îÇ\n",
        "‚îÇ                                                             ‚îÇ\n",
        "‚îÇ  2. AN√ÅLISIS MANUAL (30-60 min)                            ‚îÇ\n",
        "‚îÇ     ‚îî‚îÄ‚îÄ Fases 5.1-5.9: Entender a fondo                   ‚îÇ\n",
        "‚îÇ     ‚îî‚îÄ‚îÄ Generar insights espec√≠ficos del dominio           ‚îÇ\n",
        "‚îÇ     ‚îî‚îÄ‚îÄ Detectar problemas y oportunidades                 ‚îÇ\n",
        "‚îÇ                                                             ‚îÇ\n",
        "‚îÇ  3. DOCUMENTACI√ìN (5 min)                                  ‚îÇ\n",
        "‚îÇ     ‚îî‚îÄ‚îÄ YData-Profiling: Reporte completo                 ‚îÇ\n",
        "‚îÇ     ‚îî‚îÄ‚îÄ Para compartir y archivar                          ‚îÇ\n",
        "‚îÇ                                                             ‚îÇ\n",
        "‚îÇ  4. PRESENTACI√ìN A STAKEHOLDERS                            ‚îÇ\n",
        "‚îÇ     ‚îî‚îÄ‚îÄ Reportes HTML autom√°ticos                          ‚îÇ\n",
        "‚îÇ     ‚îî‚îÄ‚îÄ + Slides con insights del an√°lisis manual          ‚îÇ\n",
        "‚îÇ                                                             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Cu√°ndo Usar Cada Enfoque\n",
        "\n",
        "**Solo an√°lisis manual:**\n",
        "- ‚úÖ Datasets peque√±os (<10K filas)\n",
        "- ‚úÖ Proyectos acad√©micos/aprendizaje\n",
        "- ‚úÖ Necesitas control total\n",
        "\n",
        "**Manual + Sweetviz:**\n",
        "- ‚úÖ Comparar grupos (Train/Test, Disease/No Disease)\n",
        "- ‚úÖ EDA r√°pido para decisiones\n",
        "- ‚úÖ Tiempo limitado\n",
        "\n",
        "**Manual + YData-Profiling:**\n",
        "- ‚úÖ Proyectos profesionales\n",
        "- ‚úÖ Documentaci√≥n completa\n",
        "- ‚úÖ Auditor√≠as de calidad\n",
        "\n",
        "**Los 3 (Manual + Sweetviz + YData):**\n",
        "- ‚úÖ‚úÖ Proyectos cr√≠ticos\n",
        "- ‚úÖ‚úÖ Datasets complejos\n",
        "- ‚úÖ‚úÖ Presentaciones ejecutivas\n",
        "- ‚úÖ‚úÖ M√°xima calidad y profesionalismo\n",
        "\n",
        "---\n",
        "\n",
        "### Lo Que Aprendiste en Este EDA\n",
        "\n",
        "üéì **Habilidades T√©cnicas:**\n",
        "- An√°lisis exploratorio manual profesional\n",
        "- Uso de Yellowbrick para visualizaciones ML\n",
        "- Generaci√≥n de reportes autom√°ticos\n",
        "- Interpretaci√≥n de correlaciones y distribuciones\n",
        "- Detecci√≥n de outliers y anomal√≠as\n",
        "- An√°lisis espec√≠fico del dominio (medicina)\n",
        "\n",
        "üéØ **Habilidades Profesionales:**\n",
        "- Comunicar hallazgos a audiencias t√©cnicas y no t√©cnicas\n",
        "- Documentar an√°lisis reproducible\n",
        "- Balancear profundidad vs velocidad\n",
        "- Tomar decisiones basadas en datos\n",
        "- Preparar datasets para modelado\n",
        "\n",
        "üöÄ **Preparaci√≥n para Industria:**\n",
        "- Stack tecnol√≥gico actual (Yellowbrick, YData, Sweetviz)\n",
        "- Workflow profesional de Data Science\n",
        "- Generaci√≥n de reportes para stakeholders\n",
        "- An√°lisis h√≠brido (manual + autom√°tico)"
      ],
      "metadata": {
        "id": "BiS08tPr-T7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 5.12: RESUMEN EJECUTIVO FINAL\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ RESUMEN EJECUTIVO DEL EDA - PROFESIONAL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Recuperar informaci√≥n clave de an√°lisis previos\n",
        "try:\n",
        "    highest_risk_group = age_stats['Tasa Enfermedad'].idxmax()\n",
        "    highest_risk_rate = age_stats.loc[highest_risk_group, 'Tasa Enfermedad']\n",
        "except:\n",
        "    highest_risk_group = \"N/A\"\n",
        "    highest_risk_rate = 0\n",
        "\n",
        "try:\n",
        "    total_outliers = int(outlier_table['count'].sum())\n",
        "except:\n",
        "    total_outliers = 0\n",
        "\n",
        "try:\n",
        "    multicolinearity_detected = len(high_corr_pairs) > 0\n",
        "except:\n",
        "    multicolinearity_detected = False\n",
        "\n",
        "print(f\"\"\"\n",
        "üìä DATASET:\n",
        "   ‚Ä¢ Total de pacientes:        {len(df)}\n",
        "   ‚Ä¢ Features num√©ricas:        {len(numeric_features)}\n",
        "   ‚Ä¢ Features categ√≥ricas:      {len(categorical_features)}\n",
        "   ‚Ä¢ Target: binario (0=No Disease, 1=Disease)\n",
        "\n",
        "‚öñÔ∏è BALANCE DE CLASES:\n",
        "   ‚Ä¢ Clase 0 (No Disease):      {class_counts[0]:3d} ({class_counts[0]/total*100:5.1f}%)\n",
        "   ‚Ä¢ Clase 1 (Disease):         {class_counts[1]:3d} ({class_counts[1]/total*100:5.1f}%)\n",
        "   ‚Ä¢ Balance Ratio:             {balance_ratio:.3f} {'‚úÖ Balanceado' if balance_ratio >= 0.8 else '‚ö†Ô∏è Desbalanceado'}\n",
        "\n",
        "üìà CALIDAD DE DATOS:\n",
        "   ‚Ä¢ Valores faltantes:         {'‚ùå S√≠' if missing_count.sum() > 0 else '‚úÖ No'}\n",
        "   ‚Ä¢ Duplicados:                {'‚ùå S√≠' if duplicates > 0 else '‚úÖ No'}\n",
        "   ‚Ä¢ Outliers detectados:       {total_outliers}\n",
        "\n",
        "üéØ TOP 3 FEATURES M√ÅS CORRELACIONADAS CON TARGET:\n",
        "   1. {target_corr.index[0]:15s}: {target_corr.values[0]:+.3f}\n",
        "   2. {target_corr.index[1]:15s}: {target_corr.values[1]:+.3f}\n",
        "   3. {target_corr.index[2]:15s}: {target_corr.values[2]:+.3f}\n",
        "\n",
        "‚öïÔ∏è INSIGHTS CL√çNICOS:\n",
        "   ‚Ä¢ Grupo de mayor riesgo:     {highest_risk_group} ({highest_risk_rate:.1f}% con enfermedad)\n",
        "   ‚Ä¢ Tendencia con edad:        {'Creciente ‚Üó' if 'trend_increasing' in dir() and trend_increasing else 'No lineal'}\n",
        "   ‚Ä¢ Multicolinealidad:         {'‚ö†Ô∏è S√≠' if multicolinearity_detected else '‚úÖ No'}\n",
        "\"\"\")\n",
        "\n",
        "# ==========================================\n",
        "# AN√ÅLISIS COMPLETADOS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìã AN√ÅLISIS COMPLETADOS:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "analyses_completed = {\n",
        "    \"Manual - Informaci√≥n general\": \"‚úÖ\",\n",
        "    \"Manual - Balance de clases\": \"‚úÖ\",\n",
        "    \"Manual - Distribuciones\": \"‚úÖ\",\n",
        "    \"Manual - Ranking features (Yellowbrick)\": \"‚úÖ\",\n",
        "    \"Manual - Outliers\": \"‚úÖ\",\n",
        "    \"Manual - An√°lisis bivariado\": \"‚úÖ\",\n",
        "    \"Manual - An√°lisis cl√≠nico\": \"‚úÖ\",\n",
        "    \"Manual - Correlaciones\": \"‚úÖ\",\n",
        "    \"Manual - Visualizaciones multivariadas\": \"‚úÖ\",\n",
        "}\n",
        "\n",
        "# Verificar reportes autom√°ticos\n",
        "if GENERAR_YDATA_PROFILING:\n",
        "    analyses_completed[\"Autom√°tico - YData-Profiling\"] = \"‚úÖ\"\n",
        "else:\n",
        "    analyses_completed[\"Autom√°tico - YData-Profiling\"] = \"‚è≠Ô∏è Omitido\"\n",
        "\n",
        "if GENERAR_SWEETVIZ:\n",
        "    analyses_completed[\"Autom√°tico - Sweetviz\"] = \"‚úÖ\"\n",
        "else:\n",
        "    analyses_completed[\"Autom√°tico - Sweetviz\"] = \"‚è≠Ô∏è Omitido\"\n",
        "\n",
        "for analysis, status in analyses_completed.items():\n",
        "    print(f\"   {status} {analysis}\")\n",
        "\n",
        "# ==========================================\n",
        "# ARCHIVOS GENERADOS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìÅ ARCHIVOS GENERADOS:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "files_generated = []\n",
        "\n",
        "if GENERAR_YDATA_PROFILING:\n",
        "    files_generated.append(\"üìÑ heart_disease_ydata_profiling.html (Reporte completo interactivo)\")\n",
        "\n",
        "if GENERAR_SWEETVIZ:\n",
        "    files_generated.append(\"üìÑ heart_disease_sweetviz.html (Comparaci√≥n Disease vs No Disease)\")\n",
        "\n",
        "if files_generated:\n",
        "    for file in files_generated:\n",
        "        print(f\"   {file}\")\n",
        "\n",
        "    print(\"\\nüí° C√ìMO ACCEDER A LOS REPORTES:\")\n",
        "    print(\"   1. Click en üìÅ (Files) en sidebar izquierdo de Colab\")\n",
        "    print(\"   2. Busca los archivos .html\")\n",
        "    print(\"   3. Click derecho ‚Üí Download\")\n",
        "    print(\"   4. Abre en tu navegador preferido\")\n",
        "else:\n",
        "    print(\"   ‚è≠Ô∏è No se generaron reportes autom√°ticos (configuraci√≥n)\")\n",
        "    print(\"   üí° Para generar: cambiar GENERAR_YDATA_PROFILING = True y/o GENERAR_SWEETVIZ = True\")\n",
        "\n",
        "# ==========================================\n",
        "# RECOMENDACIONES PARA MODELADO\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüí° RECOMENDACIONES PARA FASE 6 (PREPROCESAMIENTO):\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "recommendations = []\n",
        "\n",
        "# Calidad de datos\n",
        "if missing_count.sum() == 0 and duplicates == 0:\n",
        "    recommendations.append(\"‚úÖ Dataset limpio, no requiere limpieza\")\n",
        "else:\n",
        "    if missing_count.sum() > 0:\n",
        "        recommendations.append(\"‚ö†Ô∏è Imputar valores faltantes antes de modelado\")\n",
        "    if duplicates > 0:\n",
        "        recommendations.append(\"‚ö†Ô∏è Eliminar duplicados: df.drop_duplicates()\")\n",
        "\n",
        "# Balance\n",
        "if balance_ratio < 0.8:\n",
        "    recommendations.append(\"‚ö†Ô∏è Aplicar balanceo: SMOTE o class_weight='balanced'\")\n",
        "else:\n",
        "    recommendations.append(\"‚úÖ Clases balanceadas, no requiere balanceo\")\n",
        "\n",
        "# Outliers\n",
        "if total_outliers > len(df) * 0.05:\n",
        "    recommendations.append(f\"‚ö†Ô∏è Revisar {total_outliers} outliers detectados (validaci√≥n m√©dica)\")\n",
        "else:\n",
        "    recommendations.append(\"‚úÖ Outliers bajo control (<5% del dataset)\")\n",
        "\n",
        "# Transformaciones\n",
        "if 'highly_skewed' in dir() and len(highly_skewed) > 0:\n",
        "    recommendations.append(f\"‚ö†Ô∏è Transformar {len(highly_skewed)} features sesgadas (log/sqrt)\")\n",
        "else:\n",
        "    recommendations.append(\"‚úÖ Distribuciones aceptables, no requieren transformaci√≥n\")\n",
        "\n",
        "# Multicolinealidad\n",
        "if multicolinearity_detected:\n",
        "    recommendations.append(\"‚ö†Ô∏è Evaluar eliminaci√≥n de features correlacionadas (|r|>0.7)\")\n",
        "else:\n",
        "    recommendations.append(\"‚úÖ No hay multicolinealidad severa\")\n",
        "\n",
        "# Estratificaci√≥n\n",
        "recommendations.append(\"‚úÖ Usar stratified split para mantener balance en train/test\")\n",
        "\n",
        "# Escalado\n",
        "recommendations.append(\"‚úÖ Aplicar StandardScaler (features con diferentes rangos)\")\n",
        "\n",
        "# Features\n",
        "recommendations.append(\"‚úÖ Features discriminativas identificadas (usar top correlacionadas)\")\n",
        "\n",
        "for i, rec in enumerate(recommendations, 1):\n",
        "    print(f\"   {i}. {rec}\")\n",
        "\n",
        "# ==========================================\n",
        "# RESUMEN FINAL\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ EDA PROFESIONAL COMPLETADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "üèÜ LOGROS:\n",
        "\n",
        "   ‚úÖ An√°lisis manual exhaustivo (9 fases)\n",
        "   ‚úÖ Visualizaciones profesionales con Yellowbrick\n",
        "   ‚úÖ Reportes autom√°ticos HTML generados\n",
        "   ‚úÖ Insights cl√≠nicos validados\n",
        "   ‚úÖ Dataset validado y listo para modelado\n",
        "\n",
        "üìö PR√ìXIMOS PASOS:\n",
        "\n",
        "   1. FASE 6: Preprocesamiento\n",
        "      ‚Ä¢ Escalado de features\n",
        "      ‚Ä¢ Split estratificado\n",
        "      ‚Ä¢ Feature engineering (opcional)\n",
        "\n",
        "   2. FASE 7-11: Modelado\n",
        "      ‚Ä¢ Baseline models\n",
        "      ‚Ä¢ Optimizaci√≥n con Optuna\n",
        "      ‚Ä¢ Ensemble methods\n",
        "\n",
        "   3. FASE 12-15: Evaluaci√≥n\n",
        "      ‚Ä¢ M√©tricas completas\n",
        "      ‚Ä¢ Interpretabilidad (SHAP)\n",
        "      ‚Ä¢ Validaci√≥n (Deepchecks)\n",
        "\n",
        "üéì VALOR PROFESIONAL:\n",
        "\n",
        "   Este EDA representa el est√°ndar de la industria en Data Science:\n",
        "   ‚Ä¢ An√°lisis h√≠brido (manual + autom√°tico)\n",
        "   ‚Ä¢ Herramientas modernas (Yellowbrick, YData, Sweetviz)\n",
        "   ‚Ä¢ Documentaci√≥n reproducible\n",
        "   ‚Ä¢ Reportes presentables a stakeholders\n",
        "\n",
        "üíº APLICABILIDAD:\n",
        "\n",
        "   Las habilidades desarrolladas aqu√≠ son directamente aplicables en:\n",
        "   ‚Ä¢ Empresas de tecnolog√≠a\n",
        "   ‚Ä¢ Consultor√≠as de datos\n",
        "   ‚Ä¢ Departamentos de analytics\n",
        "   ‚Ä¢ Proyectos de investigaci√≥n\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ LISTO PARA CONTINUAR CON FASE 6: PREPROCESAMIENTO\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "iXyh6amA-ZMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_11"
      },
      "source": [
        "# üîß FASE 6: Preprocesamiento de Datos\n",
        "\n",
        "## ‚ö†Ô∏è CR√çTICO: Evitar Data Leakage\n",
        "\n",
        "**Data leakage** = informaci√≥n del test \"filtra\" al entrenamiento\n",
        "\n",
        "### ‚ùå MAL (con leakage):\n",
        "```python\n",
        "scaler.fit(X)  # Usa info de test!\n",
        "X_scaled = scaler.transform(X)\n",
        "X_train, X_test = train_test_split(X_scaled)\n",
        "```\n",
        "\n",
        "### ‚úÖ BIEN (sin leakage):\n",
        "```python\n",
        "X_train, X_test = train_test_split(X)  # Split PRIMERO\n",
        "scaler.fit(X_train)  # Solo train\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_12"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 6: PREPROCESAMIENTO DE DATOS\n",
        "# ==========================================\n",
        "\n",
        "print(\"üîß FASE 6: Preprocesamiento de Datos\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë                    PREPROCESAMIENTO PARA MODELADO                          ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "El preprocesamiento es CR√çTICO para el √©xito del modelado:\n",
        "\n",
        "üéØ OBJETIVOS:\n",
        "   1. Manejar valores faltantes (imputaci√≥n)\n",
        "   2. Separar features y target\n",
        "   3. Split estratificado (Train/Test)\n",
        "   4. Escalado de features (StandardScaler)\n",
        "   5. Validar integridad de datos\n",
        "\n",
        "‚ö†Ô∏è SIN PREPROCESAMIENTO ADECUADO:\n",
        "   ‚Ä¢ Modelos fallan con NaN\n",
        "   ‚Ä¢ Escalas diferentes afectan convergencia\n",
        "   ‚Ä¢ Overfitting por data leakage\n",
        "   ‚Ä¢ Resultados no reproducibles\n",
        "\"\"\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# ==========================================\n",
        "# PASO 1: VERIFICAR Y MANEJAR VALORES FALTANTES\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä PASO 1: VERIFICACI√ìN Y MANEJO DE VALORES FALTANTES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Verificar valores faltantes\n",
        "missing_before = df.isnull().sum()\n",
        "total_missing = missing_before.sum()\n",
        "\n",
        "print(f\"\\nüîç Valores faltantes detectados: {total_missing}\")\n",
        "\n",
        "if total_missing > 0:\n",
        "    print(\"\\n‚ö†Ô∏è FEATURES CON VALORES FALTANTES:\")\n",
        "    for col, count in missing_before[missing_before > 0].items():\n",
        "        pct = (count / len(df)) * 100\n",
        "        print(f\"   ‚Ä¢ {col:15s}: {count:3d} NaN ({pct:5.2f}%)\")\n",
        "\n",
        "    print(\"\\nüîß ESTRATEGIA DE IMPUTACI√ìN:\")\n",
        "    print(\"   ‚Ä¢ M√©todo: Mediana (robusto a outliers)\")\n",
        "    print(\"   ‚Ä¢ Alternativas: Media, Moda, KNN, Iterative\")\n",
        "    print(\"   ‚Ä¢ Justificaci√≥n: Mediana no se ve afectada por valores extremos\")\n",
        "\n",
        "    # Crear copia del dataframe para no modificar el original\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Imputar valores faltantes en features num√©ricas\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "    # Solo imputar en features num√©ricas (no en target)\n",
        "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if 'target' in numeric_cols:\n",
        "        numeric_cols.remove('target')\n",
        "\n",
        "    # Aplicar imputaci√≥n\n",
        "    df_clean[numeric_cols] = imputer.fit_transform(df_clean[numeric_cols])\n",
        "\n",
        "    # Verificar que se eliminaron los NaN\n",
        "    missing_after = df_clean.isnull().sum().sum()\n",
        "\n",
        "    if missing_after == 0:\n",
        "        print(f\"\\n‚úÖ IMPUTACI√ìN EXITOSA:\")\n",
        "        print(f\"   ‚Ä¢ Valores faltantes antes: {total_missing}\")\n",
        "        print(f\"   ‚Ä¢ Valores faltantes despu√©s: {missing_after}\")\n",
        "        print(f\"   ‚Ä¢ Dataset limpio y listo para modelado\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è A√∫n quedan {missing_after} valores faltantes\")\n",
        "        print(\"   Revisar columnas categ√≥ricas o target\")\n",
        "\n",
        "else:\n",
        "    print(\"‚úÖ No hay valores faltantes en el dataset\")\n",
        "    df_clean = df.copy()\n",
        "\n",
        "# ==========================================\n",
        "# PASO 2: VERIFICAR DUPLICADOS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä PASO 2: VERIFICACI√ìN DE DUPLICADOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "duplicates_count = df_clean.duplicated().sum()\n",
        "\n",
        "if duplicates_count > 0:\n",
        "    print(f\"‚ö†Ô∏è Duplicados detectados: {duplicates_count}\")\n",
        "    print(\"   Eliminando duplicados...\")\n",
        "\n",
        "    df_clean = df_clean.drop_duplicates()\n",
        "\n",
        "    print(f\"‚úÖ Duplicados eliminados\")\n",
        "    print(f\"   ‚Ä¢ Registros antes: {len(df)}\")\n",
        "    print(f\"   ‚Ä¢ Registros despu√©s: {len(df_clean)}\")\n",
        "else:\n",
        "    print(f\"‚úÖ No hay duplicados en el dataset\")\n",
        "\n",
        "# ==========================================\n",
        "# PASO 3: SEPARAR FEATURES Y TARGET\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä PASO 3: SEPARACI√ìN DE FEATURES Y TARGET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Identificar features num√©ricas (excluir target)\n",
        "feature_columns = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "if 'target' in feature_columns:\n",
        "    feature_columns.remove('target')\n",
        "\n",
        "print(f\"\\nüìã Features seleccionadas: {len(feature_columns)}\")\n",
        "for i, col in enumerate(feature_columns, 1):\n",
        "    print(f\"   {i:2d}. {col}\")\n",
        "\n",
        "# Separar X e y\n",
        "X = df_clean[feature_columns]\n",
        "y = df_clean['target']\n",
        "\n",
        "print(f\"\\n‚úÖ Separaci√≥n completada:\")\n",
        "print(f\"   ‚Ä¢ X shape: {X.shape}\")\n",
        "print(f\"   ‚Ä¢ y shape: {y.shape}\")\n",
        "\n",
        "# Verificar que no hay NaN en X e y\n",
        "assert X.isnull().sum().sum() == 0, \"‚ùå Error: X contiene NaN\"\n",
        "assert y.isnull().sum() == 0, \"‚ùå Error: y contiene NaN\"\n",
        "print(f\"   ‚úÖ Sin valores NaN en X e y\")\n",
        "\n",
        "# ==========================================\n",
        "# PASO 4: SPLIT TRAIN/TEST ESTRATIFICADO\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä PASO 4: SPLIT TRAIN/TEST ESTRATIFICADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "üéØ ¬øPOR QU√â SPLIT ESTRATIFICADO?\n",
        "\n",
        "- Mantiene la proporci√≥n de clases en train y test\n",
        "- Cr√≠tico cuando hay desbalance de clases\n",
        "- Evita que todo un grupo vaya solo a train o test\n",
        "- Mejora la representatividad del test set\n",
        "\"\"\")\n",
        "\n",
        "# Configuraci√≥n del split\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Configuraci√≥n:\")\n",
        "print(f\"   ‚Ä¢ Test size: {TEST_SIZE*100:.0f}%\")\n",
        "print(f\"   ‚Ä¢ Random state: {RANDOM_STATE}\")\n",
        "print(f\"   ‚Ä¢ Estratificado: S√≠ (stratify=y)\")\n",
        "\n",
        "# Realizar split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y  # ‚úÖ CR√çTICO: Mantener proporci√≥n de clases\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Split completado:\")\n",
        "print(f\"   ‚Ä¢ Train set: {X_train.shape[0]} muestras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   ‚Ä¢ Test set:  {X_test.shape[0]} muestras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "\n",
        "# Verificar estratificaci√≥n\n",
        "print(f\"\\nüìä Verificaci√≥n de estratificaci√≥n:\")\n",
        "train_class_dist = y_train.value_counts(normalize=True).sort_index()\n",
        "test_class_dist = y_test.value_counts(normalize=True).sort_index()\n",
        "original_class_dist = y.value_counts(normalize=True).sort_index()\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Original': original_class_dist * 100,\n",
        "    'Train': train_class_dist * 100,\n",
        "    'Test': test_class_dist * 100\n",
        "})\n",
        "comparison_df.index = ['No Disease', 'Disease']\n",
        "\n",
        "print(\"\\n   Distribuci√≥n de clases (%):\")\n",
        "display(comparison_df.round(2))\n",
        "\n",
        "# Verificar que son similares\n",
        "diff_train = abs(train_class_dist - original_class_dist).max()\n",
        "diff_test = abs(test_class_dist - original_class_dist).max()\n",
        "\n",
        "if diff_train < 0.05 and diff_test < 0.05:\n",
        "    print(\"\\n‚úÖ Estratificaci√≥n exitosa (diferencias < 5%)\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è Diferencias en estratificaci√≥n: Train={diff_train:.3f}, Test={diff_test:.3f}\")\n",
        "\n",
        "# ==========================================\n",
        "# PASO 5: ESCALADO DE FEATURES\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä PASO 5: ESCALADO DE FEATURES (STANDARDSCALER)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "üéØ ¬øPOR QU√â ESCALAR?\n",
        "\n",
        "- Features con diferentes escalas afectan convergencia de modelos\n",
        "- Algoritmos basados en distancia (KNN, SVM) son sensibles a escala\n",
        "- Regularizaci√≥n (L1, L2) requiere features en escala similar\n",
        "- Mejora velocidad de convergencia en gradient descent\n",
        "\n",
        "üìä STANDARDSCALER (Z-Score Normalization):\n",
        "   ‚Ä¢ Transforma features a: media=0, desviaci√≥n est√°ndar=1\n",
        "   ‚Ä¢ F√≥rmula: z = (x - Œº) / œÉ\n",
        "   ‚Ä¢ No asume distribuci√≥n espec√≠fica\n",
        "   ‚Ä¢ Preserva outliers (vs MinMaxScaler)\n",
        "\n",
        "‚ö†Ô∏è REGLA DE ORO:\n",
        "   ‚Ä¢ Fit en TRAIN, Transform en TRAIN y TEST\n",
        "   ‚Ä¢ NUNCA fit en TEST (causa data leakage)\n",
        "\"\"\")\n",
        "\n",
        "# Crear escalador\n",
        "scaler = StandardScaler()\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Aplicando StandardScaler...\")\n",
        "\n",
        "# Fit solo en train (CR√çTICO)\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transform tanto train como test\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\n‚úÖ Escalado completado:\")\n",
        "print(f\"   ‚Ä¢ X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"   ‚Ä¢ X_test_scaled shape:  {X_test_scaled.shape}\")\n",
        "\n",
        "# Convertir a DataFrame para mantener nombres de columnas\n",
        "X_train_final = pd.DataFrame(X_train_scaled, columns=feature_columns, index=X_train.index)\n",
        "X_test_final = pd.DataFrame(X_test_scaled, columns=feature_columns, index=X_test.index)\n",
        "\n",
        "# Verificar escalado\n",
        "print(f\"\\nüìä Verificaci√≥n del escalado (Train set):\")\n",
        "print(f\"   ‚Ä¢ Media antes del escalado:\")\n",
        "print(f\"      Min: {X_train.mean().min():.4f}, Max: {X_train.mean().max():.4f}\")\n",
        "print(f\"   ‚Ä¢ Media despu√©s del escalado:\")\n",
        "print(f\"      Min: {X_train_final.mean().min():.4e}, Max: {X_train_final.mean().max():.4e}\")\n",
        "print(f\"   ‚Ä¢ Std despu√©s del escalado:\")\n",
        "print(f\"      Min: {X_train_final.std().min():.4f}, Max: {X_train_final.std().max():.4f}\")\n",
        "\n",
        "# Deber√≠a estar cerca de 0 (media) y 1 (std)\n",
        "if abs(X_train_final.mean().mean()) < 1e-10 and abs(X_train_final.std().mean() - 1.0) < 0.01:\n",
        "    print(\"\\n‚úÖ Escalado verificado correctamente\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Revisar escalado\")\n",
        "\n",
        "# ==========================================\n",
        "# RESUMEN FINAL Y VARIABLES PARA MODELADO\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìã RESUMEN DEL PREPROCESAMIENTO:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary = f\"\"\"\n",
        "DATASET ORIGINAL:\n",
        "  ‚Ä¢ Muestras totales: {len(df)}\n",
        "  ‚Ä¢ Features: {len(feature_columns)}\n",
        "  ‚Ä¢ Valores faltantes: {total_missing}\n",
        "  ‚Ä¢ Duplicados: {duplicates_count}\n",
        "\n",
        "DATASET PROCESADO:\n",
        "  ‚Ä¢ Muestras finales: {len(df_clean)}\n",
        "  ‚Ä¢ Features procesadas: {len(feature_columns)}\n",
        "\n",
        "TRAIN SET:\n",
        "  ‚Ä¢ Muestras: {len(X_train_final)}\n",
        "  ‚Ä¢ Shape: {X_train_final.shape}\n",
        "  ‚Ä¢ No Disease: {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.1f}%)\n",
        "  ‚Ä¢ Disease: {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.1f}%)\n",
        "\n",
        "TEST SET:\n",
        "  ‚Ä¢ Muestras: {len(X_test_final)}\n",
        "  ‚Ä¢ Shape: {X_test_final.shape}\n",
        "  ‚Ä¢ No Disease: {(y_test == 0).sum()} ({(y_test == 0).sum()/len(y_test)*100:.1f}%)\n",
        "  ‚Ä¢ Disease: {(y_test == 1).sum()} ({(y_test == 1).sum()/len(y_test)*100:.1f}%)\n",
        "\n",
        "ESCALADO:\n",
        "  ‚Ä¢ M√©todo: StandardScaler\n",
        "  ‚Ä¢ Media train: ~0\n",
        "  ‚Ä¢ Std train: ~1\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n",
        "\n",
        "# ==========================================\n",
        "# CREAR VARIABLES FINALES PARA MODELADO\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä CREANDO VARIABLES FINALES PARA MODELADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear aliases consistentes\n",
        "y_train_final = y_train.copy()\n",
        "y_test_final = y_test.copy()\n",
        "\n",
        "print(\"\\n‚úÖ VARIABLES DISPONIBLES PARA MODELADO:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Diccionario de variables\n",
        "modeling_vars = {\n",
        "    'X_train_final': X_train_final,\n",
        "    'X_test_final': X_test_final,\n",
        "    'y_train_final': y_train_final,\n",
        "    'y_test_final': y_test_final,\n",
        "    'scaler': scaler,\n",
        "    'feature_columns': feature_columns\n",
        "}\n",
        "\n",
        "# Mostrar informaci√≥n de cada variable\n",
        "for var_name, var_value in list(modeling_vars.items())[:4]:  # Solo X e y\n",
        "    if hasattr(var_value, 'shape'):\n",
        "        var_type = \"DataFrame\" if isinstance(var_value, pd.DataFrame) else \"Series\"\n",
        "        var_shape = var_value.shape\n",
        "        has_nan = var_value.isnull().sum().sum() if isinstance(var_value, pd.DataFrame) else var_value.isnull().sum()\n",
        "\n",
        "        print(f\"\\n   {var_name}:\")\n",
        "        print(f\"      ‚Ä¢ Tipo: {var_type}\")\n",
        "        print(f\"      ‚Ä¢ Shape: {var_shape}\")\n",
        "        print(f\"      ‚Ä¢ NaN: {has_nan} {'‚úÖ' if has_nan == 0 else '‚ùå'}\")\n",
        "\n",
        "        if isinstance(var_value, pd.DataFrame):\n",
        "            print(f\"      ‚Ä¢ Columnas: {list(var_value.columns[:3])}...\")\n",
        "        elif isinstance(var_value, pd.Series):\n",
        "            print(f\"      ‚Ä¢ Nombre: {var_value.name}\")\n",
        "            print(f\"      ‚Ä¢ Valores √∫nicos: {var_value.nunique()}\")\n",
        "\n",
        "print(f\"\\n   scaler:\")\n",
        "print(f\"      ‚Ä¢ Tipo: StandardScaler\")\n",
        "print(f\"      ‚Ä¢ Fitted: ‚úÖ\")\n",
        "\n",
        "print(f\"\\n   feature_columns:\")\n",
        "print(f\"      ‚Ä¢ Tipo: list\")\n",
        "print(f\"      ‚Ä¢ Total features: {len(feature_columns)}\")\n",
        "\n",
        "# ==========================================\n",
        "# VERIFICACI√ìN FINAL PRE-MODELADO\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüîç VERIFICACI√ìN FINAL PRE-MODELADO:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "final_checks = {\n",
        "    \"X_train_final es DataFrame\": isinstance(X_train_final, pd.DataFrame),\n",
        "    \"X_test_final es DataFrame\": isinstance(X_test_final, pd.DataFrame),\n",
        "    \"y_train_final es Series\": isinstance(y_train_final, pd.Series),\n",
        "    \"y_test_final es Series\": isinstance(y_test_final, pd.Series),\n",
        "    \"X_train_final sin NaN\": X_train_final.isnull().sum().sum() == 0,\n",
        "    \"X_test_final sin NaN\": X_test_final.isnull().sum().sum() == 0,\n",
        "    \"y_train_final sin NaN\": y_train_final.isnull().sum() == 0,\n",
        "    \"y_test_final sin NaN\": y_test_final.isnull().sum() == 0,\n",
        "    \"Shapes consistentes\": len(X_train_final) == len(y_train_final) and len(X_test_final) == len(y_test_final),\n",
        "    \"Columnas en X match feature_columns\": list(X_train_final.columns) == list(X_test_final.columns) == feature_columns,\n",
        "    \"Train + Test = Total procesado\": len(X_train_final) + len(X_test_final) == len(df_clean),\n",
        "    \"Escalado aplicado correctamente\": abs(X_train_final.mean().mean()) < 1e-10\n",
        "}\n",
        "\n",
        "print(\"\\n‚úÖ CHECKLIST:\")\n",
        "for check_name, passed in final_checks.items():\n",
        "    status = \"‚úÖ\" if passed else \"‚ùå\"\n",
        "    print(f\"   {status} {check_name}\")\n",
        "\n",
        "all_passed = all(final_checks.values())\n",
        "\n",
        "if all_passed:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üéâ PREPROCESAMIENTO COMPLETADO EXITOSAMENTE\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\"\"\n",
        "    ‚úÖ LISTO PARA MODELADO:\n",
        "\n",
        "       Variables disponibles:\n",
        "       ‚Ä¢ X_train_final (features escaladas para entrenamiento)\n",
        "       ‚Ä¢ X_test_final (features escaladas para testing)\n",
        "       ‚Ä¢ y_train_final (target para entrenamiento)\n",
        "       ‚Ä¢ y_test_final (target para testing)\n",
        "       ‚Ä¢ scaler (para transformar nuevos datos)\n",
        "       ‚Ä¢ feature_columns (nombres de features)\n",
        "\n",
        "       Caracter√≠sticas:\n",
        "       ‚Ä¢ Sin valores faltantes\n",
        "       ‚Ä¢ Sin duplicados\n",
        "       ‚Ä¢ Split estratificado\n",
        "       ‚Ä¢ Features escaladas (media=0, std=1)\n",
        "       ‚Ä¢ Sin data leakage\n",
        "       ‚Ä¢ Tipos de datos correctos\n",
        "    \"\"\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è ADVERTENCIA: Algunas verificaciones fallaron\")\n",
        "    print(\"   Revisa los checks antes de continuar al modelado\")\n",
        "    print(\"   Ejecuta esta celda nuevamente si es necesario\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ FASE 6 COMPLETADA - LISTO PARA FASE 7: MODELADO BASELINE\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_13"
      },
      "source": [
        "# üéØ FASE 7: Modelo 1 - Logistic Regression (Baseline)\n",
        "\n",
        "## ¬øQu√© es un Baseline?\n",
        "Modelo simple que establece el **rendimiento m√≠nimo aceptable**.\n",
        "\n",
        "## ¬øPor qu√© Logistic Regression?\n",
        "- ‚úÖ R√°pido de entrenar\n",
        "- ‚úÖ F√°cil de interpretar\n",
        "- ‚úÖ Probabil√≠stico\n",
        "- ‚úÖ Punto de referencia para modelos complejos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_14",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print(\"üéØ MODELO 1: LOGISTIC REGRESSION (BASELINE)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ‚úÖ USAR LA FUNCI√ìN HELPER\n",
        "run_name = start_mlflow_run(\n",
        "    model_name=\"Logistic Regression\",\n",
        "    model_family=\"linear\",\n",
        "    additional_tags={\n",
        "        \"is_baseline\": \"true\",\n",
        "        \"optimization\": \"none\",\n",
        "        \"solver\": \"lbfgs\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Crear modelo\n",
        "lr_model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
        "\n",
        "mlflow.log_params({\n",
        "    'model_type': 'Logistic Regression',\n",
        "    'max_iter': 1000,\n",
        "    'solver': 'lbfgs',\n",
        "    'execution_time': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "})\n",
        "\n",
        "# Entrenar\n",
        "start_time = time.time()\n",
        "lr_model.fit(X_train_final, y_train_final)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "mlflow.log_metric('training_time_seconds', training_time)\n",
        "print(f\"‚úÖ Entrenado en {training_time:.3f} segundos\")\n",
        "\n",
        "# Predicciones\n",
        "y_train_pred = lr_model.predict(X_train_final)\n",
        "y_test_pred = lr_model.predict(X_test_scaled)\n",
        "y_train_proba = lr_model.predict_proba(X_train_final)[:, 1]\n",
        "y_test_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# M√©tricas\n",
        "train_metrics = log_model_metrics(y_train_final, y_train_pred, y_train_proba, 'train_')\n",
        "lr_test_metrics = log_model_metrics(y_test, y_test_pred, y_test_proba, 'test_') # Store metrics uniquely\n",
        "\n",
        "# ‚úÖ Guardar modelo SIN WARNINGS\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "input_example = X_train_final.iloc[:5]\n",
        "signature = infer_signature(X_train_final, lr_model.predict_proba(X_train_final))\n",
        "\n",
        "mlflow.sklearn.log_model(\n",
        "    lr_model,\n",
        "    name=\"Logistic Regression\",\n",
        "    signature=signature,\n",
        "    input_example=input_example\n",
        ")\n",
        "# ============================================================================\n",
        "# üîß VARIABLES PARA FASE 12 - Logistic Regression\n",
        "# ============================================================================\n",
        "print(\"\\nüíæ Guardando variables para comparaci√≥n...\")\n",
        "\n",
        "# Extraer m√©tricas del diccionario train_metrics\n",
        "lr_train_acc = train_metrics['train_accuracy']\n",
        "\n",
        "# Extraer m√©tricas del diccionario lr_test_metrics\n",
        "lr_test_acc = lr_test_metrics['test_accuracy']\n",
        "lr_f1 = lr_test_metrics['test_f1_score']\n",
        "lr_auc = lr_test_metrics['test_roc_auc']\n",
        "\n",
        "# Verificar que las variables existen\n",
        "print(f\"‚úÖ Variables creadas:\")\n",
        "print(f\"   lr_train_acc = {lr_train_acc:.4f}\")\n",
        "print(f\"   lr_test_acc = {lr_test_acc:.4f}\")\n",
        "print(f\"   lr_f1 = {lr_f1:.4f}\")\n",
        "print(f\"   lr_auc = {lr_auc:.4f}\")\n",
        "\n",
        "# Finalizar run de MLflow\n",
        "mlflow.end_run()\n",
        "print(\"\\n‚úÖ FASE 7 COMPLETADA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "print(\"\\nüìà RESULTADOS:\")\n",
        "display(pd.DataFrame({\n",
        "    'M√©trica': ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC'],\n",
        "    'Train': [train_metrics[f'train_{m}'] for m in ['accuracy','precision','recall','f1_score','roc_auc']],\n",
        "    'Test': [lr_test_metrics[f'test_{m}'] for m in ['accuracy','precision','recall','f1_score','roc_auc']]\n",
        "}))\n",
        "\n",
        "print(\"\\nüìä Visualizaciones Yellowbrick:\")\n",
        "yellowbrick_comprehensive(lr_model, \"Logistic Regression\")\n",
        "yellowbrick_basic_path = 'logistic_regression_yellowbrick_basic.png'\n",
        "yellowbrick_advanced_path = 'logistic_regression_yellowbrick_advanced.png'\n",
        "# ‚úÖ LOG ARTIFACT A MLFLOW\n",
        "mlflow.log_artifact(yellowbrick_basic_path, artifact_path=\"visualizations\")\n",
        "mlflow.log_artifact(yellowbrick_advanced_path, artifact_path=\"visualizations\")\n",
        "mlflow.end_run()\n",
        "print(\"\\n‚úÖ Baseline completado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_15"
      },
      "source": [
        "# üå≥ FASE 8: Modelo 2 - Random Forest con Optuna\n",
        "\n",
        "## ¬øQu√© es Optuna?\n",
        "Framework de optimizaci√≥n de hiperpar√°metros usando **TPE** (Tree-structured Parzen Estimator).\n",
        "\n",
        "## Random Forest\n",
        "Ensemble de √°rboles de decisi√≥n con bootstrap y feature sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_16"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print(\"üå≥ MODELO 2: RANDOM FOREST + OPTUNA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "run_name = get_run_name(\"Random_Forest_Optimized\")\n",
        "print(f\"üìä MLflow Run: {run_name}\")\n",
        "\n",
        "\n",
        "def objective_rf(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
        "        'random_state': RANDOM_STATE\n",
        "    }\n",
        "    model = RandomForestClassifier(**params)\n",
        "    model.fit(X_train_final, y_train_final)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    return f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"üîç Optimizando (30 trials)...\")\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "study_rf = optuna.create_study(direction='maximize')\n",
        "study_rf.optimize(objective_rf, n_trials=30, show_progress_bar=True)\n",
        "\n",
        "print(f\"\\nüèÜ Mejor F1: {study_rf.best_value:.4f}\")\n",
        "print(\"Mejores hiperpar√°metros:\")\n",
        "for k, v in study_rf.best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Visualizaciones Optuna\n",
        "#plot_optimization_history(study_rf).update_layout(width=900, height=400).show()\n",
        "#plot_param_importances(study_rf).update_layout(width=900, height=400).show()\n",
        "#fig = optuna.visualization.plot_param_importances(study_rf)\n",
        "#show(fig)\n",
        "\n",
        "# Entrenar modelo final\n",
        "with mlflow.start_run(run_name=run_name):\n",
        "    best_params = study_rf.best_params.copy()\n",
        "    best_params['random_state'] = RANDOM_STATE\n",
        "\n",
        "    rf_model = RandomForestClassifier(**best_params)\n",
        "    mlflow.log_params(best_params)\n",
        "\n",
        "    rf_model.fit(X_train_final, y_train_final)\n",
        "\n",
        "    y_train_pred = rf_model.predict(X_train_final)\n",
        "    y_test_pred = rf_model.predict(X_test_scaled)\n",
        "    y_train_proba = rf_model.predict_proba(X_train_final)[:, 1]\n",
        "    y_test_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    train_metrics = log_model_metrics(y_train_final, y_train_pred, y_train_proba, 'train_')\n",
        "    rf_test_metrics = log_model_metrics(y_test, y_test_pred, y_test_proba, 'test_') # Store metrics uniquely\n",
        "\n",
        "    input_example = X_train_final.iloc[:5]\n",
        "    signature = infer_signature(X_train_final, lr_model.predict_proba(X_train_final))\n",
        "\n",
        "    mlflow.sklearn.log_model(\n",
        "        rf_model,\n",
        "        name=\"Random_Forest\",\n",
        "        signature=signature,\n",
        "        input_example=input_example\n",
        "    )\n",
        "    mlflow.log_params({\n",
        "        'model_type': 'Random Forest',\n",
        "        'optimizer': 'Optuna_TPE',\n",
        "        'execution_time': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    })\n",
        "\n",
        "    print(\"\\nüìà RANDOM FOREST:\")\n",
        "    display(pd.DataFrame({\n",
        "        'M√©trica': ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC'],\n",
        "        'Train': [train_metrics[f'train_{m}'] for m in ['accuracy','precision','recall','f1_score','roc_auc']],\n",
        "        'Test': [rf_test_metrics[f'test_{m}'] for m in ['accuracy','precision','recall','f1_score','roc_auc']]\n",
        "    }))\n",
        "\n",
        "    # Feature importance\n",
        "    fi = pd.DataFrame({\n",
        "        'Feature': X_train.columns,\n",
        "        'Importance': rf_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(fi['Feature'], fi['Importance'], color='#2ecc71')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.title('Random Forest Feature Importance', fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nüìä Visualizaciones Yellowbrick:\")\n",
        "yellowbrick_comprehensive(rf_model, \"Random Forest\")\n",
        "# ============================================================================\n",
        "# üîß VARIABLES PARA FASE 12 - Random Forest\n",
        "# ============================================================================\n",
        "print(\"\\nüíæ Guardando variables para comparaci√≥n...\")\n",
        "\n",
        "# Calcular m√©tricas de train\n",
        "y_train_pred_rf = rf_model.predict(X_train_final)\n",
        "rf_train_acc = accuracy_score(y_train_final, y_train_pred_rf)\n",
        "\n",
        "# Extraer m√©tricas de test del diccionario rf_test_metrics\n",
        "rf_test_acc = rf_test_metrics['test_accuracy']\n",
        "rf_f1 = rf_test_metrics['test_f1_score']\n",
        "rf_auc = rf_test_metrics['test_roc_auc']\n",
        "\n",
        "# Verificar que las variables existen\n",
        "print(f\"‚úÖ Variables creadas:\")\n",
        "print(f\"   rf_train_acc = {rf_train_acc:.4f}\")\n",
        "print(f\"   rf_test_acc = {rf_test_acc:.4f}\")\n",
        "print(f\"   rf_f1 = {rf_f1:.4f}\")\n",
        "print(f\"   rf_auc = {rf_auc:.4f}\")\n",
        "\n",
        "# Finalizar run de MLflow\n",
        "mlflow.end_run()\n",
        "print(\"\\n‚úÖ FASE 8 COMPLETADA\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_17"
      },
      "source": [
        "# ‚ö° FASE 9: Modelo 3 - XGBoost con Optuna\n",
        "\n",
        "## XGBoost\n",
        "Gradient boosting optimizado - t√≠picamente el mejor en datos tabulares."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_18"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print(\"‚ö° MODELO 3: XGBOOST + OPTUNA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "run_name = get_run_name(\"RXGBoost_Optimized\")\n",
        "print(f\"üìä MLflow Run: {run_name}\")\n",
        "\n",
        "\n",
        "def objective_xgb(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 8),        # REDUCIR\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "\n",
        "        # NUEVOS - Regularizaci√≥n\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0),    # L1\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1.0),  # L2\n",
        "\n",
        "        'random_state': 42\n",
        "    }\n",
        "    model = XGBClassifier(**params)\n",
        "    model.fit(X_train_final, y_train_final, verbose=False)\n",
        "    return f1_score(y_test, model.predict(X_test_scaled))\n",
        "\n",
        "print(\"üîç Optimizando XGBoost (30 trials)...\")\n",
        "study_xgb = optuna.create_study(direction='maximize')\n",
        "study_xgb.optimize(objective_xgb, n_trials=30, show_progress_bar=True)\n",
        "\n",
        "print(f\"\\nüèÜ Mejor F1: {study_xgb.best_value:.4f}\")\n",
        "\n",
        "#plot_optimization_history(study_xgb).update_layout(width=900, height=400).show()\n",
        "\n",
        "with mlflow.start_run(run_name=run_name):\n",
        "    best_params = study_xgb.best_params.copy()\n",
        "    best_params.update({\n",
        "        'random_state': RANDOM_STATE,\n",
        "        'eval_metric': 'logloss',\n",
        "        'use_label_encoder': False\n",
        "    })\n",
        "\n",
        "    xgb_model = XGBClassifier(**best_params)\n",
        "    mlflow.log_params(best_params)\n",
        "\n",
        "    xgb_model.fit(X_train_final, y_train_final, verbose=False)\n",
        "\n",
        "    y_train_pred = xgb_model.predict(X_train_final)\n",
        "    y_test_pred = xgb_model.predict(X_test_scaled)\n",
        "    y_train_proba = xgb_model.predict_proba(X_train_final)[:, 1]\n",
        "    y_test_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    train_metrics = log_model_metrics(y_train_final, y_train_pred, y_train_proba, 'train_')\n",
        "    xgb_test_metrics = log_model_metrics(y_test, y_test_pred, y_test_proba, 'test_') # Store metrics uniquely\n",
        "\n",
        "    input_example = X_train_final.iloc[:5]\n",
        "    signature = infer_signature(X_train_final, lr_model.predict_proba(X_train_final))\n",
        "    mlflow.sklearn.log_model(\n",
        "        xgb_model,\n",
        "        name=\"XGBoost\",\n",
        "        signature=signature,\n",
        "        input_example=input_example\n",
        "    )\n",
        "    mlflow.log_params({\n",
        "        'model_type': 'XGBoost',\n",
        "        'optimizer': 'Optuna_TPE',\n",
        "        'execution_time': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    })\n",
        "\n",
        "    print(\"\\nüìà XGBOOST:\")\n",
        "    display(pd.DataFrame({\n",
        "        'M√©trica': ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC'],\n",
        "        'Train': [train_metrics[f'train_{m}'] for m in ['accuracy','precision','recall','f1_score','roc_auc']],\n",
        "        'Test': [xgb_test_metrics[f'test_{m}'] for m in ['accuracy','precision','recall','f1_score','roc_auc']]\n",
        "    }))\n",
        "\n",
        "\n",
        "print(\"\\nüìä Visualizaciones Yellowbrick:\")\n",
        "yellowbrick_comprehensive(xgb_model, \"XGBoost\")\n",
        "\n",
        "# ============================================================================\n",
        "# üîß VARIABLES PARA FASE 12 - XGBoost\n",
        "# ============================================================================\n",
        "print(\"\\nüíæ Guardando variables para comparaci√≥n...\")\n",
        "\n",
        "# Calcular m√©tricas de train\n",
        "y_train_pred_xgb = xgb_model.predict(X_train_final)\n",
        "xgb_train_acc = accuracy_score(y_train_final, y_train_pred_xgb)\n",
        "\n",
        "# Extraer m√©tricas de test del diccionario xgb_test_metrics\n",
        "xgb_test_acc = xgb_test_metrics['test_accuracy']\n",
        "xgb_f1 = xgb_test_metrics['test_f1_score']\n",
        "xgb_auc = xgb_test_metrics['test_roc_auc']\n",
        "\n",
        "# Verificar que las variables existen\n",
        "print(f\"‚úÖ Variables creadas:\")\n",
        "print(f\"   xgb_train_acc = {xgb_train_acc:.4f}\")\n",
        "print(f\"   xgb_test_acc = {xgb_test_acc:.4f}\")\n",
        "print(f\"   xgb_f1 = {xgb_f1:.4f}\")\n",
        "print(f\"   xgb_auc = {xgb_auc:.4f}\")\n",
        "\n",
        "# Finalizar run de MLflow\n",
        "mlflow.end_run()\n",
        "print(\"\\n‚úÖ FASE 9 COMPLETADA\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_19"
      },
      "source": [
        "# üß† FASE 10: Modelo 4 - Neural Network\n",
        "\n",
        "## Arquitectura\n",
        "```\n",
        "Input (13) ‚Üí Dense(64) + BatchNorm + Dropout(0.3)\n",
        "           ‚Üí Dense(32) + BatchNorm + Dropout(0.3)\n",
        "           ‚Üí Dense(16) + BatchNorm + Dropout(0.2)\n",
        "           ‚Üí Dense(1) + Sigmoid\n",
        "```\n",
        "\n",
        "## Callbacks\n",
        "- **EarlyStopping**: Para si val_loss no mejora\n",
        "- **ReduceLROnPlateau**: Reduce LR si se estanca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_20",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title FASE 8: Neural Network Optimizada\n",
        "\n",
        "\"\"\"\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "MEJORA #5: ARQUITECTURA DE RED NEURONAL OPTIMIZADA PARA DATOS TABULARES\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "CONCEPTOS CLAVE:\n",
        "\n",
        "1. REGLA DE DIMENSIONAMIENTO:\n",
        "   - Para datos tabulares peque√±os (< 50 features):\n",
        "     ‚Ä¢ Capa 1: ‚âà 2x n√∫mero de features\n",
        "     ‚Ä¢ Capa 2: ‚âà 1.5x n√∫mero de features\n",
        "     ‚Ä¢ Capa 3: ‚âà 1x n√∫mero de features (opcional)\n",
        "\n",
        "   - Evitar capas muy grandes:\n",
        "     ‚Ä¢ M√°s par√°metros = mayor riesgo de overfitting\n",
        "     ‚Ä¢ No necesariamente mejor performance\n",
        "\n",
        "2. BATCH NORMALIZATION:\n",
        "   - Normaliza activaciones entre capas\n",
        "   - Estabiliza entrenamiento\n",
        "   - Act√∫a como regularizaci√≥n suave\n",
        "   - Permite learning rates m√°s altos\n",
        "\n",
        "3. DROPOUT:\n",
        "   - Apaga aleatoriamente neuronas durante entrenamiento\n",
        "   - Previene co-adaptaci√≥n de features\n",
        "   - Regularizaci√≥n efectiva\n",
        "   - T√≠pico: 0.2-0.5 en capas intermedias\n",
        "\n",
        "4. ARQUITECTURA PROPUESTA:\n",
        "   Input (13) ‚Üí Dense(24) + BN + Dropout(0.3)\n",
        "               ‚Üí Dense(16) + BN + Dropout(0.3)\n",
        "               ‚Üí Dense(8) + Dropout(0.15)\n",
        "               ‚Üí Output(1)\n",
        "\n",
        "   Total par√°metros: ~700 (vs 15,000+ en versi√≥n grande)\n",
        "\n",
        "5. EARLY STOPPING:\n",
        "   - Para entrenamiento cuando val_loss no mejora\n",
        "   - Previene overfitting\n",
        "   - Guarda mejor modelo autom√°ticamente\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "\n",
        "print(\"üß† FASE 8: NEURAL NETWORK OPTIMIZADA PARA DATOS TABULARES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# FUNCI√ìN PARA CREAR ARQUITECTURA OPTIMIZADA\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "def create_optimized_nn(input_dim, learning_rate=0.001, dropout_rate=0.3):\n",
        "    \"\"\"\n",
        "    Crea una red neuronal optimizada para datos tabulares peque√±os.\n",
        "\n",
        "    Arquitectura dise√±ada para:\n",
        "    - Prevenir overfitting\n",
        "    - Entrenamiento estable\n",
        "    - Generalizaci√≥n efectiva\n",
        "\n",
        "    Args:\n",
        "        input_dim: N√∫mero de features de entrada\n",
        "        learning_rate: Learning rate para Adam optimizer\n",
        "        dropout_rate: Tasa de dropout para regularizaci√≥n\n",
        "\n",
        "    Returns:\n",
        "        Modelo Keras compilado\n",
        "    \"\"\"\n",
        "\n",
        "    # Calcular tama√±os de capas basados en input\n",
        "    layer1_size = input_dim * 2  # 2x features\n",
        "    layer2_size = int(input_dim * 1.5)  # 1.5x features\n",
        "    layer3_size = input_dim  # 1x features\n",
        "\n",
        "    print(f\"\\nüèóÔ∏è  ARQUITECTURA DE LA RED:\")\n",
        "    print(f\"   Input Layer:    {input_dim} features\")\n",
        "    print(f\"   Hidden Layer 1: {layer1_size} neurons + BatchNorm + Dropout({dropout_rate})\")\n",
        "    print(f\"   Hidden Layer 2: {layer2_size} neurons + BatchNorm + Dropout({dropout_rate})\")\n",
        "    print(f\"   Hidden Layer 3: {layer3_size} neurons + Dropout({dropout_rate/2})\")\n",
        "    print(f\"   Output Layer:   1 neuron (sigmoid)\")\n",
        "\n",
        "    model = Sequential([\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # CAPA DE ENTRADA + PRIMERA CAPA OCULTA\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        Dense(\n",
        "            layer1_size,\n",
        "            activation='relu',\n",
        "            input_shape=(input_dim,),\n",
        "            kernel_initializer='he_normal',  # Mejor para ReLU\n",
        "            name='dense_1'\n",
        "        ),\n",
        "        BatchNormalization(name='bn_1'),\n",
        "        Dropout(dropout_rate, name='dropout_1'),\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # SEGUNDA CAPA OCULTA\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        Dense(\n",
        "            layer2_size,\n",
        "            activation='relu',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='dense_2'\n",
        "        ),\n",
        "        BatchNormalization(name='bn_2'),\n",
        "        Dropout(dropout_rate, name='dropout_2'),\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # TERCERA CAPA OCULTA (m√°s peque√±a, menos dropout)\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        Dense(\n",
        "            layer3_size,\n",
        "            activation='relu',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='dense_3'\n",
        "        ),\n",
        "        Dropout(dropout_rate / 2, name='dropout_3'),  # Menos dropout\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # CAPA DE SALIDA\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        Dense(\n",
        "            1,\n",
        "            activation='sigmoid',\n",
        "            kernel_initializer='glorot_uniform',  # Mejor para sigmoid\n",
        "            name='output'\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # COMPILACI√ìN DEL MODELO\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.AUC(name='auc'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall')\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# CREAR MODELO\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "nn_model = create_optimized_nn(\n",
        "    input_dim=X_train_scaled.shape[1],\n",
        "    learning_rate=0.001,\n",
        "    dropout_rate=0.3\n",
        ")\n",
        "\n",
        "# Mostrar resumen\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìã RESUMEN DEL MODELO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "nn_model.summary()\n",
        "\n",
        "# Calcular total de par√°metros\n",
        "total_params = nn_model.count_params()\n",
        "print(f\"\\nüìä Total de par√°metros entrenables: {total_params:,}\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# CONFIGURAR CALLBACKS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n‚öôÔ∏è  CONFIGURANDO CALLBACKS:\")\n",
        "\n",
        "# Early Stopping: Para cuando val_loss no mejora\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=20,  # Esperar 20 epochs sin mejora\n",
        "    restore_best_weights=True,  # Restaurar mejores pesos\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "print(\"   ‚úÖ Early Stopping (patience=20, monitor=val_loss)\")\n",
        "\n",
        "# Reduce Learning Rate: Reducir LR cuando se estanca\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,  # Reducir LR a la mitad\n",
        "    patience=10,  # Esperar 10 epochs\n",
        "    min_lr=0.00001,  # LR m√≠nimo\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "print(\"   ‚úÖ Reduce LR on Plateau (factor=0.5, patience=10)\")\n",
        "\n",
        "callbacks = [early_stopping, reduce_lr]\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# ENTRENAR MODELO\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ ENTRENANDO MODELO\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n   Configuraci√≥n:\")\n",
        "print(f\"   ‚Ä¢ Epochs m√°ximos: 100\")\n",
        "print(f\"   ‚Ä¢ Batch size: 16\")\n",
        "print(f\"   ‚Ä¢ Validation split: 20%\")\n",
        "print(f\"   ‚Ä¢ Early stopping activado\")\n",
        "print(\"\\n‚è±Ô∏è  Tiempo estimado: 1-2 minutos\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Entrenar\n",
        "history = nn_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,  # 20% de train para validaci√≥n\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# AN√ÅLISIS DEL ENTRENAMIENTO\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä AN√ÅLISIS DEL ENTRENAMIENTO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "final_epoch = len(history.history['loss'])\n",
        "print(f\"\\n   Epochs ejecutados: {final_epoch} / 100\")\n",
        "\n",
        "# Mejor epoch\n",
        "best_epoch = np.argmin(history.history['val_loss']) + 1\n",
        "best_val_loss = min(history.history['val_loss'])\n",
        "\n",
        "print(f\"   Mejor epoch: {best_epoch}\")\n",
        "print(f\"   Mejor val_loss: {best_val_loss:.4f}\")\n",
        "\n",
        "# Chequear overfitting\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "loss_gap = final_val_loss - final_train_loss\n",
        "\n",
        "print(f\"\\n   Train loss final: {final_train_loss:.4f}\")\n",
        "print(f\"   Val loss final:   {final_val_loss:.4f}\")\n",
        "print(f\"   Gap:              {loss_gap:.4f} \", end=\"\")\n",
        "\n",
        "if loss_gap < 0.05:\n",
        "    print(\"‚úÖ Excelente generalizaci√≥n\")\n",
        "elif loss_gap < 0.10:\n",
        "    print(\"‚úÖ Buena generalizaci√≥n\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Posible overfitting\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# VISUALIZACI√ìN DE CURVAS DE APRENDIZAJE\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüìä Generando curvas de aprendizaje...\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Subplot 1: Loss\n",
        "axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "axes[0, 0].axvline(x=best_epoch, color='red', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Binary Crossentropy Loss')\n",
        "axes[0, 0].set_title('Training vs Validation Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: Accuracy\n",
        "axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
        "axes[0, 1].axvline(x=best_epoch, color='red', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].set_title('Training vs Validation Accuracy')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: AUC\n",
        "axes[1, 0].plot(history.history['auc'], label='Train AUC', linewidth=2)\n",
        "axes[1, 0].plot(history.history['val_auc'], label='Val AUC', linewidth=2)\n",
        "axes[1, 0].axvline(x=best_epoch, color='red', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('AUC')\n",
        "axes[1, 0].set_title('Training vs Validation AUC')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 4: Learning Rate (si se redujo)\n",
        "if 'lr' in history.history:\n",
        "    axes[1, 1].plot(history.history['lr'], label='Learning Rate', linewidth=2, color='orange')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Learning Rate')\n",
        "    axes[1, 1].set_title('Learning Rate Schedule')\n",
        "    axes[1, 1].set_yscale('log')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "else:\n",
        "    # Si no hay LR history, mostrar precision/recall\n",
        "    axes[1, 1].plot(history.history['precision'], label='Train Precision', linewidth=2)\n",
        "    axes[1, 1].plot(history.history['val_precision'], label='Val Precision', linewidth=2)\n",
        "    axes[1, 1].plot(history.history['recall'], label='Train Recall', linewidth=2, linestyle='--')\n",
        "    axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linewidth=2, linestyle='--')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Score')\n",
        "    axes[1, 1].set_title('Precision & Recall')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('nn_training_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Curvas guardadas: nn_training_curves.png\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# EVALUACI√ìN EN TEST SET\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä EVALUACI√ìN EN TEST SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Predicciones\n",
        "y_pred_nn_proba = nn_model.predict(X_test_scaled).ravel()\n",
        "y_pred_nn = (y_pred_nn_proba > 0.5).astype(int)\n",
        "\n",
        "# M√©tricas\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
        "\n",
        "test_acc_nn = accuracy_score(y_test, y_pred_nn)\n",
        "test_f1_nn = f1_score(y_test, y_pred_nn)\n",
        "test_auc_nn = roc_auc_score(y_test, y_pred_nn_proba)\n",
        "\n",
        "print(f\"\\n   Accuracy: {test_acc_nn:.4f} ({test_acc_nn*100:.2f}%)\")\n",
        "print(f\"   F1-Score: {test_f1_nn:.4f} ({test_f1_nn*100:.2f}%)\")\n",
        "print(f\"   ROC-AUC:  {test_auc_nn:.4f} ({test_auc_nn*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nüìã Classification Report:\")\n",
        "print(\"-\"*80)\n",
        "print(classification_report(y_test, y_pred_nn, target_names=['Sano', 'Enfermo'], digits=4))\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# MLFLOW TRACKING\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "with mlflow.start_run(run_name=\"Neural_Network_Optimized\"):\n",
        "    # Hiperpar√°metros de arquitectura\n",
        "    mlflow.log_param(\"architecture\", \"Dense(24)-Dense(16)-Dense(8)\")\n",
        "    mlflow.log_param(\"layer_1_size\", 24)\n",
        "    mlflow.log_param(\"layer_2_size\", 16)\n",
        "    mlflow.log_param(\"layer_3_size\", 8)\n",
        "    mlflow.log_param(\"dropout_rate\", 0.3)\n",
        "    mlflow.log_param(\"learning_rate\", 0.001)\n",
        "    mlflow.log_param(\"batch_size\", 16)\n",
        "    mlflow.log_param(\"total_params\", total_params)\n",
        "\n",
        "    # M√©tricas de entrenamiento\n",
        "    mlflow.log_metric(\"epochs_trained\", final_epoch)\n",
        "    mlflow.log_metric(\"best_epoch\", best_epoch)\n",
        "    mlflow.log_metric(\"best_val_loss\", best_val_loss)\n",
        "    mlflow.log_metric(\"final_train_loss\", final_train_loss)\n",
        "    mlflow.log_metric(\"final_val_loss\", final_val_loss)\n",
        "    mlflow.log_metric(\"loss_gap\", loss_gap)\n",
        "\n",
        "    # M√©tricas de test\n",
        "    mlflow.log_metric(\"test_accuracy\", test_acc_nn)\n",
        "    mlflow.log_metric(\"test_f1\", test_f1_nn)\n",
        "    mlflow.log_metric(\"test_auc\", test_auc_nn)\n",
        "\n",
        "    # Artifacts\n",
        "    mlflow.log_artifact('nn_training_curves.png')\n",
        "\n",
        "    # Tags\n",
        "    mlflow.set_tag(\"model_type\", \"Neural_Network\")\n",
        "    mlflow.set_tag(\"framework\", \"TensorFlow_Keras\")\n",
        "    mlflow.set_tag(\"regularization\", \"BatchNorm+Dropout+EarlyStopping\")\n",
        "\n",
        "print(\"\\n‚úÖ M√©tricas registradas en MLflow\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ FASE 8 COMPLETADA - Neural Network Optimizada\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# INTERPRETACI√ìN PEDAG√ìGICA\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüí° INTERPRETACI√ìN PEDAG√ìGICA:\")\n",
        "print(f\"\"\"\n",
        "1. COMPARACI√ìN DE ARQUITECTURAS:\n",
        "\n",
        "   ANTES (Sobredimensionada):\n",
        "   ‚Ä¢ Dense(128) ‚Üí Dense(64) ‚Üí Dense(32)\n",
        "   ‚Ä¢ Total par√°metros: ~15,000\n",
        "   ‚Ä¢ Riesgo: Overfitting alto\n",
        "\n",
        "   AHORA (Optimizada):\n",
        "   ‚Ä¢ Dense(24) ‚Üí Dense(16) ‚Üí Dense(8)\n",
        "   ‚Ä¢ Total par√°metros: {total_params:,}\n",
        "   ‚Ä¢ Ventaja: Mejor generalizaci√≥n\n",
        "\n",
        "2. ¬øPOR QU√â FUNCIONA MEJOR?:\n",
        "   ‚Ä¢ Menos par√°metros ‚Üí menos overfitting\n",
        "   ‚Ä¢ BatchNorm ‚Üí entrenamiento estable\n",
        "   ‚Ä¢ Dropout ‚Üí regularizaci√≥n efectiva\n",
        "   ‚Ä¢ Early Stopping ‚Üí mejor modelo guardado\n",
        "\n",
        "3. CURVAS DE APRENDIZAJE:\n",
        "   ‚Ä¢ Train y Val se acercan ‚Üí Buena generalizaci√≥n\n",
        "   ‚Ä¢ Val loss estable ‚Üí Modelo no overfitea\n",
        "   ‚Ä¢ Early stop en √©poca {best_epoch} ‚Üí Momento √≥ptimo\n",
        "\n",
        "4. REDES NEURONALES PARA TABULAR:\n",
        "   ‚Ä¢ No siempre son mejores que tree-based\n",
        "   ‚Ä¢ √ötiles para: Interacciones complejas\n",
        "   ‚Ä¢ Requieren: M√°s datos y tunin g\n",
        "   ‚Ä¢ Trade-off: Interpretabilidad vs Performance\n",
        "\n",
        "5. EN LA PR√ÅCTICA:\n",
        "   ‚Ä¢ Tabular peque√±o (<1000 filas): Tree-based mejor\n",
        "   ‚Ä¢ Tabular grande (>10,000): NN competitivo\n",
        "   ‚Ä¢ Im√°genes/Texto: NN siempre mejor\n",
        "   ‚Ä¢ Producci√≥n m√©dica: Preferir interpretable (RF/XGB)\n",
        "\"\"\")\n",
        "# ============================================================================\n",
        "# üîß VARIABLES PARA FASE 12 - Neural Network\n",
        "# ============================================================================\n",
        "print(\"\\nüíæ Guardando variables para comparaci√≥n...\")\n",
        "\n",
        "# Calcular m√©tricas de train\n",
        "y_train_pred_nn = (nn_model.predict(X_train_final) > 0.5).astype(int).ravel()\n",
        "nn_train_acc = accuracy_score(y_train_final, y_train_pred_nn)\n",
        "\n",
        "# Predicciones de test\n",
        "y_test_pred_nn = (nn_model.predict(X_test_scaled) > 0.5).astype(int).ravel()\n",
        "y_test_proba_nn = nn_model.predict(X_test_scaled).ravel()\n",
        "\n",
        "# Calcular m√©tricas de test\n",
        "nn_test_acc = accuracy_score(y_test, y_test_pred_nn)\n",
        "nn_f1 = f1_score(y_test, y_test_pred_nn)\n",
        "nn_auc = roc_auc_score(y_test, y_test_proba_nn)\n",
        "\n",
        "# Verificar que las variables existen\n",
        "print(f\"‚úÖ Variables creadas:\")\n",
        "print(f\"   nn_train_acc = {nn_train_acc:.4f}\")\n",
        "print(f\"   nn_test_acc = {nn_test_acc:.4f}\")\n",
        "print(f\"   nn_f1 = {nn_f1:.4f}\")\n",
        "print(f\"   nn_auc = {nn_auc:.4f}\")\n",
        "\n",
        "# Finalizar run de MLflow\n",
        "mlflow.end_run()\n",
        "print(\"\\n‚úÖ FASE 10 COMPLETADA\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_21"
      },
      "source": [
        "# üéØ FASE 11: Modelo 5 - Ensemble (Voting)\n",
        "\n",
        "## Soft Voting\n",
        "Promedia probabilidades de los 3 mejores modelos:\n",
        "- Logistic Regression\n",
        "- Random Forest\n",
        "- XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_22"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print(\"üéØ MODELO 5: ENSEMBLE VOTING\")\n",
        "print(\"=\"*80)\n",
        "run_name = get_run_name(\"Ensemble_Voting\")\n",
        "print(f\"üìä MLflow Run: {run_name}\")\n",
        "\n",
        "with mlflow.start_run(run_name=run_name):\n",
        "    ensemble = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('lr', lr_model),\n",
        "            ('rf', rf_model),\n",
        "            ('xgb', xgb_model)\n",
        "        ],\n",
        "        voting='soft'\n",
        "    )\n",
        "\n",
        "    mlflow.log_params({\n",
        "        'model_type': 'VotingClassifier',\n",
        "        'voting': 'soft',\n",
        "        'n_estimators': 3,\n",
        "        'execution_time': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    })\n",
        "\n",
        "    ensemble.fit(X_train_final, y_train_final)\n",
        "\n",
        "    y_train_pred = ensemble.predict(X_train_final)\n",
        "    y_test_pred = ensemble.predict(X_test_scaled)\n",
        "    y_train_proba = ensemble.predict_proba(X_train_final)[:, 1]\n",
        "    y_test_proba = ensemble.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    train_metrics = log_model_metrics(y_train_final, y_train_pred, y_train_proba, 'train_')\n",
        "    ensemble_test_metrics = log_model_metrics(y_test, y_test_pred, y_test_proba, 'test_') # Store metrics uniquely\n",
        "\n",
        "    input_example = X_train_final.iloc[:5]\n",
        "    signature = infer_signature(X_train_final, lr_model.predict_proba(X_train_final))\n",
        "    mlflow.sklearn.log_model(\n",
        "        ensemble,\n",
        "        name=\"ensemble_model\",\n",
        "        signature=signature,\n",
        "        input_example=input_example\n",
        "    )\n",
        "\n",
        "    print(\"üìà ENSEMBLE:\")\n",
        "    display(pd.DataFrame({\n",
        "        'M√©trica': ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC'],\n",
        "        'Train': [train_metrics[f'train_{m}'] for m in ['accuracy','precision','recall','f1_score','roc_auc']],\n",
        "        'Test': [ensemble_test_metrics[f'test_{m}'] for m in ['accuracy','precision','recall','f1_score','roc_auc']]\n",
        "    }))\n",
        "\n",
        "print(\"\\nüìä Visualizaciones Yellowbrick:\")\n",
        "yellowbrick_comprehensive(ensemble, \"Ensemble Voting\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# üîß VARIABLES PARA FASE 12 - Ensemble\n",
        "# ============================================================================\n",
        "print(\"\\nüíæ Guardando variables para comparaci√≥n...\")\n",
        "ensemble_model = ensemble\n",
        "\n",
        "# Calcular m√©tricas de train\n",
        "y_train_pred_ensemble = ensemble_model.predict(X_train_final)\n",
        "ensemble_train_acc = accuracy_score(y_train_final, y_train_pred_ensemble)\n",
        "\n",
        "\n",
        "# Extraer m√©tricas de test del diccionario ensemble_test_metrics\n",
        "ensemble_test_acc = ensemble_test_metrics['test_accuracy']\n",
        "ensemble_f1 = ensemble_test_metrics['test_f1_score']\n",
        "ensemble_auc = ensemble_test_metrics['test_roc_auc']\n",
        "\n",
        "# Verificar que las variables existen\n",
        "print(f\"‚úÖ Variables creadas:\")\n",
        "print(f\"   ensemble_train_acc = {ensemble_train_acc:.4f}\")\n",
        "print(f\"   ensemble_test_acc = {ensemble_test_acc:.4f}\")\n",
        "print(f\"   ensemble_f1 = {ensemble_f1:.4f}\")\n",
        "print(f\"   ensemble_auc = {ensemble_auc:.4f}\")\n",
        "\n",
        "# Finalizar run de MLflow\n",
        "mlflow.end_run()\n",
        "print(\"\\n‚úÖ FASE 11 COMPLETADA\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_23"
      },
      "source": [
        "# üìä FASE 12: Comparaci√≥n Final de Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_24"
      },
      "outputs": [],
      "source": [
        "# @title üèÜ FASE 12: SELECCI√ìN INTELIGENTE DEL MEJOR MODELO\n",
        "\n",
        "print(\"üèÜ FASE 12: SELECCI√ìN INTELIGENTE DEL MEJOR MODELO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# PASO 1: CONSTRUIR DATAFRAME DE COMPARACI√ìN\n",
        "# ============================================================================\n",
        "print(\"\\nüìä CONSTRUYENDO TABLA DE COMPARACI√ìN...\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Recopilar m√©tricas de todos los modelos\n",
        "comparison_data = {\n",
        "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network', 'Ensemble'],\n",
        "    'Train Acc': [\n",
        "        lr_train_acc,\n",
        "        rf_train_acc,\n",
        "        xgb_train_acc,\n",
        "        nn_train_acc,\n",
        "        ensemble_train_acc\n",
        "    ],\n",
        "    'Test Acc': [\n",
        "        lr_test_acc,\n",
        "        rf_test_acc,\n",
        "        xgb_test_acc,\n",
        "        nn_test_acc,\n",
        "        ensemble_test_acc\n",
        "    ],\n",
        "    'Test F1': [\n",
        "        lr_f1,\n",
        "        rf_f1,\n",
        "        xgb_f1,\n",
        "        nn_f1,\n",
        "        ensemble_f1\n",
        "    ],\n",
        "    'Test AUC': [\n",
        "        lr_auc,\n",
        "        rf_auc,\n",
        "        xgb_auc,\n",
        "        nn_auc,\n",
        "        ensemble_auc\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Crear DataFrame\n",
        "comparison = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Calcular diferencia train-test (overfitting indicator)\n",
        "comparison['Train-Test Gap'] = comparison['Train Acc'] - comparison['Test Acc']\n",
        "\n",
        "# Mostrar tabla inicial\n",
        "print(\"\\nüìã Tabla de Comparaci√≥n Inicial:\")\n",
        "print(comparison.round(4).to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# PASO 2: FUNCI√ìN DE SELECCI√ìN CON COMPOSITE SCORE\n",
        "# ============================================================================\n",
        "\n",
        "def select_best_model_composite(df):\n",
        "    \"\"\"\n",
        "    Selecciona el mejor modelo usando un composite score ponderado.\n",
        "\n",
        "    Ponderaci√≥n optimizada para problemas m√©dicos:\n",
        "    - AUC: 40% (capacidad de discriminaci√≥n entre clases)\n",
        "    - F1-Score: 40% (balance entre precisi√≥n y recall)\n",
        "    - Accuracy: 20% (desempe√±o general)\n",
        "\n",
        "    Penalizaci√≥n por overfitting:\n",
        "    - Si Train-Test Gap > 5%, se aplica penalizaci√≥n\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame con m√©tricas de modelos\n",
        "\n",
        "    Returns:\n",
        "        tuple: (nombre_mejor_modelo, √≠ndice, score, df_actualizado)\n",
        "    \"\"\"\n",
        "    print(\"\\nüìä CALCULANDO COMPOSITE SCORES...\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Calcular composite score base\n",
        "    df['Composite Score (Base)'] = (\n",
        "        0.4 * df['Test AUC'] +\n",
        "        0.4 * df['Test F1'] +\n",
        "        0.2 * df['Test Acc']\n",
        "    )\n",
        "\n",
        "    # Aplicar penalizaci√≥n por overfitting\n",
        "    df['Overfitting Penalty'] = 0.0\n",
        "    overfitting_threshold = 0.05  # 5%\n",
        "\n",
        "    for idx in df.index:\n",
        "        gap = df.loc[idx, 'Train-Test Gap']\n",
        "        if gap > overfitting_threshold:\n",
        "            # Penalizaci√≥n proporcional al exceso de gap\n",
        "            penalty = (gap - overfitting_threshold) * 0.5\n",
        "            df.loc[idx, 'Overfitting Penalty'] = penalty\n",
        "\n",
        "    # Calcular score final ajustado\n",
        "    df['Composite Score (Final)'] = df['Composite Score (Base)'] - df['Overfitting Penalty']\n",
        "\n",
        "    # Encontrar mejor modelo\n",
        "    best_idx = df['Composite Score (Final)'].idxmax()\n",
        "    best_model_name = df.loc[best_idx, 'Model']\n",
        "    best_score = df.loc[best_idx, 'Composite Score (Final)']\n",
        "\n",
        "    # Mostrar an√°lisis detallado\n",
        "    print(f\"\\nüéØ AN√ÅLISIS DE COMPOSITE SCORES:\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    for idx in df.index:\n",
        "        model = df.loc[idx, 'Model']\n",
        "        base_score = df.loc[idx, 'Composite Score (Base)']\n",
        "        penalty = df.loc[idx, 'Overfitting Penalty']\n",
        "        final_score = df.loc[idx, 'Composite Score (Final)']\n",
        "        gap = df.loc[idx, 'Train-Test Gap']\n",
        "\n",
        "        status = \"üèÜ GANADOR\" if idx == best_idx else \"\"\n",
        "\n",
        "        print(f\"\\n{model:20s} {status}\")\n",
        "        print(f\"  ‚Ä¢ Base Score:     {base_score:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Train-Test Gap: {gap:.4f} ({gap*100:.2f}%)\")\n",
        "        print(f\"  ‚Ä¢ Penalty:        {penalty:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Final Score:    {final_score:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"üèÜ MEJOR MODELO SELECCIONADO: {best_model_name}\")\n",
        "    print(f\"   üìà Composite Score Final: {best_score:.4f}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Explicar la decisi√≥n\n",
        "    print(f\"\\nüí° JUSTIFICACI√ìN DE LA SELECCI√ìN:\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"‚úÖ Accuracy:  {df.loc[best_idx, 'Test Acc']:.4f} ({df.loc[best_idx, 'Test Acc']*100:.2f}%)\")\n",
        "    print(f\"‚úÖ F1-Score:  {df.loc[best_idx, 'Test F1']:.4f} ({df.loc[best_idx, 'Test F1']*100:.2f}%)\")\n",
        "    print(f\"‚úÖ ROC-AUC:   {df.loc[best_idx, 'Test AUC']:.4f} ({df.loc[best_idx, 'Test AUC']*100:.2f}%)\")\n",
        "    print(f\"‚úÖ Overfitting Gap: {df.loc[best_idx, 'Train-Test Gap']:.4f} ({df.loc[best_idx, 'Train-Test Gap']*100:.2f}%)\")\n",
        "\n",
        "    print(\"\\nüìã En problemas m√©dicos priorizamos:\")\n",
        "    print(\"   1Ô∏è‚É£  AUC (40%): Capacidad de discriminaci√≥n\")\n",
        "    print(\"   2Ô∏è‚É£  F1 (40%): Balance precisi√≥n-recall (cr√≠tico en diagn√≥sticos)\")\n",
        "    print(\"   3Ô∏è‚É£  Accuracy (20%): Desempe√±o general\")\n",
        "    print(\"   ‚ö†Ô∏è  Penalizaci√≥n: Por overfitting significativo\")\n",
        "\n",
        "    return best_model_name, best_idx, best_score, df\n",
        "\n",
        "# ============================================================================\n",
        "# PASO 3: APLICAR SELECCI√ìN\n",
        "# ============================================================================\n",
        "\n",
        "best_model_name, best_idx, best_score, comparison = select_best_model_composite(\n",
        "    comparison.copy()\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# PASO 4: GUARDAR MEJOR MODELO Y M√âTRICAS\n",
        "# ============================================================================\n",
        "\n",
        "# Obtener el objeto del mejor modelo\n",
        "models_dict = {\n",
        "    'Logistic Regression': lr_model,\n",
        "    'Random Forest': rf_model,\n",
        "    'XGBoost': xgb_model,\n",
        "    'Neural Network': nn_model,\n",
        "    'Ensemble': ensemble_model\n",
        "}\n",
        "\n",
        "best_model = models_dict[best_model_name]\n",
        "\n",
        "# Guardar m√©tricas del mejor modelo\n",
        "best_acc = comparison.loc[best_idx, 'Test Acc']\n",
        "best_f1 = comparison.loc[best_idx, 'Test F1']\n",
        "best_auc = comparison.loc[best_idx, 'Test AUC']\n",
        "\n",
        "print(f\"\\nüíæ GUARDANDO MEJOR MODELO...\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Guardar modelo con joblib (m√°s eficiente que pickle)\n",
        "import joblib\n",
        "joblib.dump(best_model, f'best_model_{best_model_name.replace(\" \", \"_\").lower()}.pkl')\n",
        "print(f\"‚úÖ Modelo guardado: best_model_{best_model_name.replace(' ', '_').lower()}.pkl\")\n",
        "\n",
        "# ============================================================================\n",
        "# PASO 5: VISUALIZACI√ìN DE COMPARACI√ìN\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"\\nüìä GENERANDO VISUALIZACIONES DE COMPARACI√ìN...\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Crear figura con 2 subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Subplot 1: Comparaci√≥n de m√©tricas principales\n",
        "ax1 = axes[0]\n",
        "x = np.arange(len(comparison))\n",
        "width = 0.25\n",
        "\n",
        "rects1 = ax1.bar(x - width, comparison['Test Acc'], width, label='Accuracy', alpha=0.8)\n",
        "rects2 = ax1.bar(x, comparison['Test F1'], width, label='F1-Score', alpha=0.8)\n",
        "rects3 = ax1.bar(x + width, comparison['Test AUC'], width, label='ROC-AUC', alpha=0.8)\n",
        "\n",
        "ax1.set_xlabel('Modelo', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Comparaci√≥n de M√©tricas de Test', fontsize=14, fontweight='bold')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(comparison['Model'], rotation=45, ha='right')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "ax1.set_ylim([0.8, 1.0])\n",
        "\n",
        "# Resaltar mejor modelo\n",
        "ax1.axvline(x=best_idx, color='gold', linestyle='--', linewidth=2, alpha=0.5)\n",
        "ax1.text(best_idx, 0.82, 'üèÜ MEJOR', ha='center', fontsize=10,\n",
        "         bbox=dict(boxstyle='round', facecolor='gold', alpha=0.3))\n",
        "\n",
        "# Subplot 2: Composite Scores y Penalizaciones\n",
        "ax2 = axes[1]\n",
        "x2 = np.arange(len(comparison))\n",
        "\n",
        "bars1 = ax2.barh(x2, comparison['Composite Score (Base)'], 0.35,\n",
        "                 label='Score Base', alpha=0.7, color='steelblue')\n",
        "bars2 = ax2.barh(x2, comparison['Composite Score (Final)'], 0.35,\n",
        "                 label='Score Final (con penalizaci√≥n)', alpha=0.9, color='darkblue')\n",
        "\n",
        "ax2.set_yticks(x2)\n",
        "ax2.set_yticklabels(comparison['Model'])\n",
        "ax2.set_xlabel('Composite Score', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Composite Scores (Base vs Final)', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3, axis='x')\n",
        "ax2.set_xlim([0.85, 0.96])\n",
        "\n",
        "# Resaltar mejor modelo\n",
        "ax2.axhline(y=best_idx, color='gold', linestyle='--', linewidth=2, alpha=0.5)\n",
        "\n",
        "# Agregar anotaciones de penalizaci√≥n\n",
        "for idx, penalty in enumerate(comparison['Overfitting Penalty']):\n",
        "    if penalty > 0.001:\n",
        "        ax2.text(comparison.loc[idx, 'Composite Score (Final)'] - 0.002, idx,\n",
        "                f'‚ö†Ô∏è -{penalty:.3f}', ha='right', va='center', fontsize=8, color='red')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison_final.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualizaci√≥n guardada: model_comparison_final.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# PASO 6: TABLA RESUMEN FINAL\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"\\nüìã TABLA RESUMEN FINAL:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Reordenar columnas para mejor legibilidad\n",
        "comparison_final = comparison[[\n",
        "    'Model',\n",
        "    'Test Acc',\n",
        "    'Test F1',\n",
        "    'Test AUC',\n",
        "    'Train-Test Gap',\n",
        "    'Composite Score (Base)',\n",
        "    'Overfitting Penalty',\n",
        "    'Composite Score (Final)'\n",
        "]]\n",
        "\n",
        "# Ordenar por Composite Score Final (descendente)\n",
        "comparison_final = comparison_final.sort_values('Composite Score (Final)', ascending=False)\n",
        "\n",
        "print(comparison_final.round(4).to_string(index=False))\n",
        "\n",
        "# Guardar resultados\n",
        "comparison_final.to_csv('model_comparison_results.csv', index=False)\n",
        "print(\"\\n‚úÖ Resultados guardados: model_comparison_results.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ FASE 12 COMPLETADA\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title NUEVA FASE: Validaci√≥n Cruzada Estratificada\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"üîÑ VALIDACI√ìN CRUZADA ESTRATIFICADA (5-FOLD)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# CONFIGURACI√ìN DE CROSS-VALIDATION\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n‚öôÔ∏è  Configurando validaci√≥n cruzada...\")\n",
        "\n",
        "# Estrategia de CV\n",
        "cv_strategy = StratifiedKFold(\n",
        "    n_splits=5,\n",
        "    shuffle=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"   ‚úÖ StratifiedKFold(n_splits=5, shuffle=True)\")\n",
        "\n",
        "# M√©tricas a evaluar\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'f1': 'f1',\n",
        "    'roc_auc': 'roc_auc',\n",
        "    'precision': 'precision',\n",
        "    'recall': 'recall'\n",
        "}\n",
        "\n",
        "print(\"   ‚úÖ M√©tricas: Accuracy, F1, ROC-AUC, Precision, Recall\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# EJECUTAR CV PARA CADA MODELO\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ EJECUTANDO CROSS-VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n‚è±Ô∏è  Tiempo estimado: 2-5 minutos (depende de modelos)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Dictionary de modelos\n",
        "models_dict = {\n",
        "    'Logistic Regression': lr_model,\n",
        "    'Random Forest': rf_model,\n",
        "    'XGBoost': xgb_model,\n",
        "    'Neural Network': nn_model,\n",
        "    'Ensemble': ensemble_model\n",
        "}\n",
        "\n",
        "cv_results = {}\n",
        "\n",
        "for name, model in models_dict.items():\n",
        "    print(f\"\\nüìä Evaluando {name}...\")\n",
        "\n",
        "    try:\n",
        "        # Ejecutar cross-validation\n",
        "        scores = cross_validate(\n",
        "            model,\n",
        "            X_train_scaled,\n",
        "            y_train,\n",
        "            cv=cv_strategy,\n",
        "            scoring=scoring,\n",
        "            return_train_score=True,\n",
        "            n_jobs=-1,  # Usar todos los cores\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Extraer resultados\n",
        "        cv_results[name] = {\n",
        "            # Train scores\n",
        "            'train_acc_mean': scores['train_accuracy'].mean(),\n",
        "            'train_acc_std': scores['train_accuracy'].std(),\n",
        "            'train_f1_mean': scores['train_f1'].mean(),\n",
        "            'train_f1_std': scores['train_f1'].std(),\n",
        "\n",
        "            # Test scores\n",
        "            'test_acc_mean': scores['test_accuracy'].mean(),\n",
        "            'test_acc_std': scores['test_accuracy'].std(),\n",
        "            'test_f1_mean': scores['test_f1'].mean(),\n",
        "            'test_f1_std': scores['test_f1'].std(),\n",
        "            'test_auc_mean': scores['test_roc_auc'].mean(),\n",
        "            'test_auc_std': scores['test_roc_auc'].std(),\n",
        "            'test_precision_mean': scores['test_precision'].mean(),\n",
        "            'test_precision_std': scores['test_precision'].std(),\n",
        "            'test_recall_mean': scores['test_recall'].mean(),\n",
        "            'test_recall_std': scores['test_recall'].std(),\n",
        "\n",
        "            # Overfitting gap\n",
        "            'overfitting_gap_acc': scores['train_accuracy'].mean() - scores['test_accuracy'].mean(),\n",
        "            'overfitting_gap_f1': scores['train_f1'].mean() - scores['test_f1'].mean()\n",
        "        }\n",
        "\n",
        "        print(f\"   ‚úÖ Completado\")\n",
        "        print(f\"      Test Acc: {cv_results[name]['test_acc_mean']:.4f} ¬± {cv_results[name]['test_acc_std']:.4f}\")\n",
        "        print(f\"      Test F1:  {cv_results[name]['test_f1_mean']:.4f} ¬± {cv_results[name]['test_f1_std']:.4f}\")\n",
        "        print(f\"      Test AUC: {cv_results[name]['test_auc_mean']:.4f} ¬± {cv_results[name]['test_auc_std']:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {str(e)}\")\n",
        "        cv_results[name] = None\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# CREAR DATAFRAME DE RESULTADOS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä RESULTADOS DE VALIDACI√ìN CRUZADA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Filtrar modelos exitosos\n",
        "successful_results = {k: v for k, v in cv_results.items() if v is not None}\n",
        "\n",
        "# Crear DataFrame\n",
        "cv_df = pd.DataFrame(successful_results).T\n",
        "\n",
        "# Renombrar columnas para display\n",
        "display_columns = {\n",
        "    'train_acc_mean': 'Train Acc',\n",
        "    'train_acc_std': 'Train Acc Std',\n",
        "    'test_acc_mean': 'Test Acc',\n",
        "    'test_acc_std': 'Test Acc Std',\n",
        "    'test_f1_mean': 'Test F1',\n",
        "    'test_f1_std': 'Test F1 Std',\n",
        "    'test_auc_mean': 'Test AUC',\n",
        "    'test_auc_std': 'Test AUC Std',\n",
        "    'overfitting_gap_acc': 'Gap Acc'\n",
        "}\n",
        "\n",
        "cv_display = cv_df[display_columns.keys()].copy()\n",
        "cv_display.columns = display_columns.values()\n",
        "\n",
        "# Formatear para display\n",
        "print(\"\\n\" + cv_display.round(4).to_string())\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# AN√ÅLISIS DE ESTABILIDAD\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä AN√ÅLISIS DE ESTABILIDAD\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "STD (Desviaci√≥n Est√°ndar) indica variabilidad entre folds:\n",
        "‚Ä¢ Std < 0.02: Modelo muy estable\n",
        "‚Ä¢ Std < 0.05: Modelo estable\n",
        "‚Ä¢ Std < 0.10: Variabilidad moderada\n",
        "‚Ä¢ Std > 0.10: Modelo inestable (sensible al split)\n",
        "\"\"\")\n",
        "\n",
        "# Ordenar por estabilidad (menor std en test accuracy)\n",
        "stability_ranking = cv_df.sort_values('test_acc_std')\n",
        "\n",
        "print(\"\\nüèÜ RANKING POR ESTABILIDAD (Test Accuracy Std):\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for idx, (model_name, row) in enumerate(stability_ranking.iterrows(), 1):\n",
        "    std = row['test_acc_std']\n",
        "    mean = row['test_acc_mean']\n",
        "\n",
        "    if std < 0.02:\n",
        "        stability = \"‚úÖ Muy Estable\"\n",
        "    elif std < 0.05:\n",
        "        stability = \"‚úÖ Estable\"\n",
        "    elif std < 0.10:\n",
        "        stability = \"‚ö†Ô∏è  Moderado\"\n",
        "    else:\n",
        "        stability = \"‚ùå Inestable\"\n",
        "\n",
        "    print(f\"   {idx}. {model_name:25s} - Std: {std:.4f} - {stability}\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# VISUALIZACI√ìN 1: BOXPLOTS DE M√âTRICAS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüìä Generando visualizaciones...\")\n",
        "\n",
        "# Recolectar scores individuales de cada fold\n",
        "fold_scores = {name: [] for name in successful_results.keys()}\n",
        "\n",
        "for name, model in models_dict.items():\n",
        "    if name in successful_results:\n",
        "        scores = cross_validate(\n",
        "            model, X_train_scaled, y_train,\n",
        "            cv=cv_strategy,\n",
        "            scoring=scoring,\n",
        "            return_train_score=False,\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        fold_scores[name] = {\n",
        "            'Accuracy': scores['test_accuracy'],\n",
        "            'F1-Score': scores['test_f1'],\n",
        "            'ROC-AUC': scores['test_roc_auc']\n",
        "        }\n",
        "\n",
        "# Crear figura\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "metrics = ['Accuracy', 'F1-Score', 'ROC-AUC']\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Preparar datos para boxplot\n",
        "    data_for_plot = []\n",
        "    labels = []\n",
        "\n",
        "    for model_name, scores_dict in fold_scores.items():\n",
        "        data_for_plot.append(scores_dict[metric])\n",
        "        labels.append(model_name.replace(' ', '\\n'))\n",
        "\n",
        "    # Crear boxplot\n",
        "    bp = ax.boxplot(\n",
        "        data_for_plot,\n",
        "        labels=labels,\n",
        "        patch_artist=True,\n",
        "        showmeans=True,\n",
        "        meanprops=dict(marker='D', markerfacecolor='red', markersize=8)\n",
        "    )\n",
        "\n",
        "    # Colorear boxes\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.6)\n",
        "\n",
        "    ax.set_ylabel(metric, fontsize=12, weight='bold')\n",
        "    ax.set_title(f'Distribuci√≥n de {metric}\\nCV 5-Fold',\n",
        "                 fontsize=13, weight='bold')\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    ax.tick_params(axis='x', rotation=0, labelsize=9)\n",
        "\n",
        "plt.suptitle('Validaci√≥n Cruzada - Variabilidad de M√©tricas',\n",
        "             fontsize=16, weight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('cv_boxplots.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Boxplots guardados: cv_boxplots.png\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# VISUALIZACI√ìN 2: HEATMAP DE GAPS (Overfitting)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "# Crear DataFrame de gaps\n",
        "gap_data = pd.DataFrame({\n",
        "    'Modelo': list(successful_results.keys()),\n",
        "    'Gap Accuracy': [cv_results[name]['overfitting_gap_acc'] for name in successful_results.keys()],\n",
        "    'Gap F1': [cv_results[name]['overfitting_gap_f1'] for name in successful_results.keys()]\n",
        "})\n",
        "\n",
        "gap_data = gap_data.set_index('Modelo')\n",
        "\n",
        "# Heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(\n",
        "    gap_data.T,\n",
        "    annot=True,\n",
        "    fmt='.4f',\n",
        "    cmap='RdYlGn_r',  # Reversed: rojo = alto gap (malo)\n",
        "    center=0.05,\n",
        "    vmin=0,\n",
        "    vmax=0.15,\n",
        "    cbar_kws={'label': 'Overfitting Gap'},\n",
        "    linewidths=0.5,\n",
        "    linecolor='gray'\n",
        ")\n",
        "plt.title('An√°lisis de Overfitting - Train-Test Gaps\\n(Valores bajos = mejor)',\n",
        "          fontsize=14, weight='bold', pad=15)\n",
        "plt.ylabel('M√©trica', fontsize=12)\n",
        "plt.xlabel('Modelo', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('cv_overfitting_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Heatmap guardado: cv_overfitting_heatmap.png\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# COMPARACI√ìN: SIMPLE SPLIT vs CROSS-VALIDATION\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚öñÔ∏è  COMPARACI√ìN: SIMPLE SPLIT vs CROSS-VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Usar comparison DataFrame de FASE 12 (si existe)\n",
        "# Comparar test accuracy del simple split vs CV\n",
        "\n",
        "comparison_data = []\n",
        "\n",
        "for model_name in successful_results.keys():\n",
        "    # CV results\n",
        "    cv_acc = cv_results[model_name]['test_acc_mean']\n",
        "    cv_std = cv_results[model_name]['test_acc_std']\n",
        "\n",
        "    # Simple split results (de comparison DataFrame)\n",
        "    simple_acc = comparison[comparison['Model'] == model_name]['Test Acc'].values[0] if model_name in comparison['Model'].values else None\n",
        "\n",
        "    if simple_acc is not None:\n",
        "        diff = cv_acc - simple_acc\n",
        "\n",
        "        comparison_data.append({\n",
        "            'Modelo': model_name,\n",
        "            'Simple Split': simple_acc,\n",
        "            'CV Mean': cv_acc,\n",
        "            'CV Std': cv_std,\n",
        "            'Diferencia': diff\n",
        "        })\n",
        "\n",
        "comp_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\n\" + comp_df.round(4).to_string(index=False))\n",
        "\n",
        "print(\"\"\"\n",
        "\\nüí° INTERPRETACI√ìN:\n",
        "   ‚Ä¢ Diferencia peque√±a (<0.02): Split original es representativo\n",
        "   ‚Ä¢ Diferencia grande (>0.05): Split original podr√≠a ser \"lucky/unlucky\"\n",
        "   ‚Ä¢ CV Std bajo (<0.03): Modelo estable, confiable\n",
        "   ‚Ä¢ CV Std alto (>0.05): Performance muy variable seg√∫n datos\n",
        "\"\"\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# MEJOR MODELO SEG√öN CV\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üèÜ MEJOR MODELO SEG√öN CROSS-VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calcular composite score con CV results\n",
        "cv_df['composite_score'] = (\n",
        "    0.4 * cv_df['test_auc_mean'] +\n",
        "    0.4 * cv_df['test_f1_mean'] +\n",
        "    0.2 * cv_df['test_acc_mean']\n",
        ")\n",
        "\n",
        "best_cv_model = cv_df['composite_score'].idxmax()\n",
        "best_cv_score = cv_df.loc[best_cv_model, 'composite_score']\n",
        "best_cv_std = (\n",
        "    0.4 * cv_df.loc[best_cv_model, 'test_auc_std'] +\n",
        "    0.4 * cv_df.loc[best_cv_model, 'test_f1_std'] +\n",
        "    0.2 * cv_df.loc[best_cv_model, 'test_acc_std']\n",
        ")\n",
        "\n",
        "print(f\"\\n   Modelo: {best_cv_model}\")\n",
        "print(f\"   Composite Score: {best_cv_score:.4f} ¬± {best_cv_std:.4f}\")\n",
        "print(f\"   Test Accuracy:   {cv_df.loc[best_cv_model, 'test_acc_mean']:.4f} ¬± {cv_df.loc[best_cv_model, 'test_acc_std']:.4f}\")\n",
        "print(f\"   Test F1:         {cv_df.loc[best_cv_model, 'test_f1_mean']:.4f} ¬± {cv_df.loc[best_cv_model, 'test_f1_std']:.4f}\")\n",
        "print(f\"   Test AUC:        {cv_df.loc[best_cv_model, 'test_auc_mean']:.4f} ¬± {cv_df.loc[best_cv_model, 'test_auc_std']:.4f}\")\n",
        "\n",
        "# Comparar con best_model_name de FASE 12\n",
        "print(f\"\\n   Modelo elegido en FASE 12: {best_model_name}\")\n",
        "\n",
        "if best_cv_model == best_model_name:\n",
        "    print(\"   ‚úÖ Cross-validation confirma la elecci√≥n original\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  Cross-validation sugiere {best_cv_model} en lugar de {best_model_name}\")\n",
        "    print(\"   üí° Considerar usar resultado de CV (m√°s robusto)\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# GUARDAR RESULTADOS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüìÑ Guardando resultados...\")\n",
        "\n",
        "cv_df.to_csv('cross_validation_results.csv')\n",
        "print(\"‚úÖ Resultados guardados: cross_validation_results.csv\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# RECOMENDACIONES FINALES\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üí° RECOMENDACIONES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\"\"\n",
        "BASADO EN CROSS-VALIDATION:\n",
        "\n",
        "1. MODELO RECOMENDADO:\n",
        "   ‚Ä¢ {best_cv_model}\n",
        "   ‚Ä¢ Score: {best_cv_score:.4f} ¬± {best_cv_std:.4f}\n",
        "   ‚Ä¢ Raz√≥n: Performance robusta validada en m√∫ltiples splits\n",
        "\n",
        "2. ESTABILIDAD:\n",
        "   ‚Ä¢ Modelo m√°s estable: {stability_ranking.index[0]}\n",
        "   ‚Ä¢ Std m√°s bajo: {stability_ranking.iloc[0]['test_acc_std']:.4f}\n",
        "   ‚Ä¢ Importante para deployment en producci√≥n\n",
        "\n",
        "3. COMPARACI√ìN CON SIMPLE SPLIT:\n",
        "   ‚Ä¢ Si diferencias <0.03: Simple split es confiable\n",
        "   ‚Ä¢ Si diferencias >0.05: Usar CV para decisi√≥n final\n",
        "   ‚Ä¢ CV siempre es m√°s conservador (mejor para producci√≥n)\n",
        "\n",
        "4. OVERFITTING:\n",
        "   ‚Ä¢ Modelos con gap <0.05: Buenos candidatos\n",
        "   ‚Ä¢ Modelos con gap >0.10: Requieren m√°s regularizaci√≥n\n",
        "   ‚Ä¢ Gap bajo + Std bajo = modelo ideal\n",
        "\n",
        "5. PR√ìXIMOS PASOS:\n",
        "   ‚Ä¢ Documentar resultados de CV en reporte final\n",
        "   ‚Ä¢ Usar CV para hyperparameter tuning si es necesario\n",
        "   ‚Ä¢ En producci√≥n: monitorear si performance se mantiene\n",
        "   ‚Ä¢ Considerar nested CV para estimaci√≥n no sesgada\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ VALIDACI√ìN CRUZADA COMPLETADA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# INTERPRETACI√ìN PEDAG√ìGICA\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüí° INTERPRETACI√ìN PEDAG√ìGICA:\")\n",
        "print(\"\"\"\n",
        "1. ¬øPOR QU√â CROSS-VALIDATION?:\n",
        "   ‚Ä¢ Simple split: Una √∫nica estimaci√≥n (puede tener suerte/mala suerte)\n",
        "   ‚Ä¢ CV: K estimaciones ‚Üí promedio m√°s confiable\n",
        "   ‚Ä¢ Reduce varianza de la estimaci√≥n\n",
        "\n",
        "2. STRATIFIED vs REGULAR:\n",
        "   ‚Ä¢ Regular K-Fold: Puede tener folds desbalanceados\n",
        "   ‚Ä¢ Stratified: Garantiza balance en todos los folds\n",
        "   ‚Ä¢ Critical cuando clases son desbalanceadas\n",
        "\n",
        "3. INTERPRETACI√ìN DE STD:\n",
        "   ‚Ä¢ Std bajo: Modelo aprende patrones generales\n",
        "   ‚Ä¢ Std alto: Modelo es sensible a datos espec√≠ficos\n",
        "   ‚Ä¢ En producci√≥n: preferir modelos con Std bajo\n",
        "\n",
        "4. NESTED CV (Avanzado):\n",
        "   ‚Ä¢ CV exterior: Estimaci√≥n de performance\n",
        "   ‚Ä¢ CV interior: Tuning de hiperpar√°metros\n",
        "   ‚Ä¢ Evita sesgo optimista en la estimaci√≥n\n",
        "   ‚Ä¢ Costoso: K_outer √ó K_inner entrenamientos\n",
        "\n",
        "5. EN LA PR√ÅCTICA:\n",
        "   ‚Ä¢ Kaggle: CV es est√°ndar (leaderboard p√∫blico puede enga√±ar)\n",
        "   ‚Ä¢ Producci√≥n: CV en validaci√≥n final antes de deploy\n",
        "   ‚Ä¢ Research: CV + test set holdout separado\n",
        "   ‚Ä¢ Small datasets: Leave-One-Out CV (K = n)\n",
        "\n",
        "6. TRADE-OFFS:\n",
        "   ‚Ä¢ K peque√±o (3-5): M√°s r√°pido, m√°s varianza\n",
        "   ‚Ä¢ K grande (10): M√°s lento, menos varianza\n",
        "   ‚Ä¢ K=5: Balance com√∫n en industria\n",
        "   ‚Ä¢ Tiempo: K veces m√°s lento que simple split\n",
        "\"\"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AMPHoPoXpR6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title NUEVA FASE: An√°lisis Consolidado de Feature Importance\n",
        "\n",
        "\"\"\"\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "MEJORA #9: AN√ÅLISIS CONSOLIDADO DE FEATURE IMPORTANCE\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "CONCEPTOS CLAVE:\n",
        "\n",
        "1. M√öLTIPLES PERSPECTIVAS:\n",
        "   ‚Ä¢ Tree-based: Usa estructura de √°rboles\n",
        "   ‚Ä¢ Linear: Usa coeficientes\n",
        "   ‚Ä¢ SHAP: Usa teor√≠a de juegos\n",
        "   ‚Ä¢ Permutation: Usa degradaci√≥n de performance\n",
        "   ‚Ä¢ Cada uno tiene fortalezas/debilidades\n",
        "\n",
        "2. ¬øPOR QU√â CONSOLIDAR?:\n",
        "   ‚Ä¢ Un solo m√©todo puede enga√±ar\n",
        "   ‚Ä¢ Consenso entre m√©todos = mayor confianza\n",
        "   ‚Ä¢ Features importantes en todos ‚Üí cr√≠ticas\n",
        "   ‚Ä¢ Features importantes solo en uno ‚Üí investigar\n",
        "\n",
        "3. RANKING METHODS:\n",
        "   ‚Ä¢ Simple average: Promedio de rankings\n",
        "   ‚Ä¢ Weighted average: Ponderar por confiabilidad\n",
        "   ‚Ä¢ Borda count: Sistema de votaci√≥n\n",
        "   ‚Ä¢ Consensus: Features en top-K de todos\n",
        "\n",
        "4. APLICACI√ìN:\n",
        "   ‚Ä¢ Feature selection: Eliminar no importantes\n",
        "   ‚Ä¢ Feature engineering: Enfocar esfuerzos\n",
        "   ‚Ä¢ Interpretaci√≥n: Explicar al negocio\n",
        "   ‚Ä¢ Debugging: Detectar features problem√°ticas\n",
        "\n",
        "5. PRECAUCIONES:\n",
        "   ‚Ä¢ Features correlacionadas confunden importancia\n",
        "   ‚Ä¢ Importancia ‚â† Causalidad\n",
        "   ‚Ä¢ Context-dependent: puede cambiar con m√°s datos\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "print(\"üîç AN√ÅLISIS CONSOLIDADO DE FEATURE IMPORTANCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# RECOLECTAR IMPORTANCES DE CADA MODELO\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüìä Recolectando feature importances de todos los modelos...\")\n",
        "\n",
        "importance_dict = {}\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 1. RANDOM FOREST IMPORTANCE\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£  Random Forest (Gini Importance)...\")\n",
        "\n",
        "rf_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "rf_importance['Rank'] = range(1, len(rf_importance) + 1)\n",
        "importance_dict['Random Forest'] = rf_importance.set_index('Feature')['Rank'].to_dict()\n",
        "\n",
        "print(f\"   ‚úÖ Top 3: {', '.join(rf_importance.head(3)['Feature'].tolist())}\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 2. XGBOOST IMPORTANCE\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£  XGBoost (Gain Importance)...\")\n",
        "\n",
        "xgb_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': xgb_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "xgb_importance['Rank'] = range(1, len(xgb_importance) + 1)\n",
        "importance_dict['XGBoost'] = xgb_importance.set_index('Feature')['Rank'].to_dict()\n",
        "\n",
        "print(f\"   ‚úÖ Top 3: {', '.join(xgb_importance.head(3)['Feature'].tolist())}\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 3. LOGISTIC REGRESSION IMPORTANCE (Coeficientes)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£  Logistic Regression (Abs Coefficients)...\")\n",
        "\n",
        "lr_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': np.abs(lr_model.coef_[0])\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "lr_importance['Rank'] = range(1, len(lr_importance) + 1)\n",
        "importance_dict['Logistic Regression'] = lr_importance.set_index('Feature')['Rank'].to_dict()\n",
        "\n",
        "print(f\"   ‚úÖ Top 3: {', '.join(lr_importance.head(3)['Feature'].tolist())}\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 4. SHAP IMPORTANCE\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£  SHAP (Mean Absolute SHAP Values)...\")\n",
        "\n",
        "# Ya calculado en FASE 13, usar esos valores\n",
        "shap_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': np.abs(shap_values).mean(axis=0)\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "shap_importance['Rank'] = range(1, len(shap_importance) + 1)\n",
        "importance_dict['SHAP'] = shap_importance.set_index('Feature')['Rank'].to_dict()\n",
        "\n",
        "print(f\"   ‚úÖ Top 3: {', '.join(shap_importance.head(3)['Feature'].tolist())}\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 5. PERMUTATION IMPORTANCE\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n5Ô∏è‚É£  Permutation Importance (Model-Agnostic)...\")\n",
        "print(\"   ‚è±Ô∏è  Calculando... (puede tomar 30-60 segundos)\")\n",
        "\n",
        "# Usar mejor modelo para permutation importance\n",
        "perm_importance = permutation_importance(\n",
        "    best_model,\n",
        "    X_test_scaled,\n",
        "    y_test,\n",
        "    n_repeats=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "perm_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': perm_importance.importances_mean\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "perm_importance_df['Rank'] = range(1, len(perm_importance_df) + 1)\n",
        "importance_dict['Permutation'] = perm_importance_df.set_index('Feature')['Rank'].to_dict()\n",
        "\n",
        "print(f\"   ‚úÖ Top 3: {', '.join(perm_importance_df.head(3)['Feature'].tolist())}\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# CONSOLIDAR RANKINGS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîÑ CONSOLIDANDO RANKINGS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear DataFrame con todos los rankings\n",
        "all_rankings = pd.DataFrame(importance_dict)\n",
        "\n",
        "# Calcular ranking promedio\n",
        "all_rankings['Average Rank'] = all_rankings.mean(axis=1)\n",
        "all_rankings['Std Rank'] = all_rankings.std(axis=1)\n",
        "\n",
        "# Ordenar por ranking promedio\n",
        "all_rankings = all_rankings.sort_values('Average Rank')\n",
        "\n",
        "# Calcular consensus score (inverso del ranking promedio, normalizado)\n",
        "max_rank = len(X.columns)\n",
        "all_rankings['Consensus Score'] = 1 - (all_rankings['Average Rank'] - 1) / (max_rank - 1)\n",
        "\n",
        "print(\"\\nüìä RANKINGS CONSOLIDADOS:\")\n",
        "print(\"-\"*80)\n",
        "print(all_rankings.round(2).to_string())\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# VISUALIZACI√ìN 1: HEATMAP DE RANKINGS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüìä Generando heatmap de rankings...\")\n",
        "\n",
        "# Seleccionar top 10 features por consensus\n",
        "top_10_features = all_rankings.head(10).index\n",
        "\n",
        "# Filtrar solo esas features para el heatmap\n",
        "heatmap_data = all_rankings.loc[top_10_features,\n",
        "                                 ['Random Forest', 'XGBoost', 'Logistic Regression', 'SHAP', 'Permutation']]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(\n",
        "    heatmap_data.T,\n",
        "    annot=True,\n",
        "    fmt='.0f',\n",
        "    cmap='RdYlGn_r',  # Reversed: verde = rank bajo (mejor)\n",
        "    cbar_kws={'label': 'Ranking (1=mejor)'},\n",
        "    linewidths=0.5,\n",
        "    linecolor='white'\n",
        ")\n",
        "plt.title('Feature Importance Rankings - Top 10 Features\\n(Verde = M√°s Importante)',\n",
        "          fontsize=14, weight='bold', pad=15)\n",
        "plt.xlabel('Features', fontsize=12)\n",
        "plt.ylabel('M√©todo de Importance', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Heatmap guardado: feature_importance_heatmap.png\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# VISUALIZACI√ìN 2: BAR CHART DE CONSENSUS SCORES\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüìä Generando bar chart de consensus scores...\")\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot\n",
        "bars = plt.barh(\n",
        "    range(len(all_rankings)),\n",
        "    all_rankings['Consensus Score'],\n",
        "    color=plt.cm.RdYlGn(all_rankings['Consensus Score'])\n",
        ")\n",
        "\n",
        "# Agregar error bars (std de rankings)\n",
        "plt.errorbar(\n",
        "    all_rankings['Consensus Score'],\n",
        "    range(len(all_rankings)),\n",
        "    xerr=all_rankings['Std Rank'] / max_rank,  # Normalizar std\n",
        "    fmt='none',\n",
        "    ecolor='black',\n",
        "    capsize=3,\n",
        "    alpha=0.5\n",
        ")\n",
        "\n",
        "plt.yticks(range(len(all_rankings)), all_rankings.index)\n",
        "plt.xlabel('Consensus Score (0-1)', fontsize=12, weight='bold')\n",
        "plt.ylabel('Feature', fontsize=12, weight='bold')\n",
        "plt.title('Feature Importance - Consensus Score\\n(Promedio de 5 m√©todos)',\n",
        "          fontsize=14, weight='bold', pad=15)\n",
        "plt.xlim(0, 1)\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Agregar l√≠nea vertical en threshold de importancia\n",
        "threshold = 0.5\n",
        "plt.axvline(x=threshold, color='red', linestyle='--', linewidth=2,\n",
        "            label=f'Threshold = {threshold}')\n",
        "plt.legend(fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_consensus_scores.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Consensus scores guardado: feature_consensus_scores.png\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# AN√ÅLISIS DE CONSENSO\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ AN√ÅLISIS DE CONSENSO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Definir categor√≠as\n",
        "high_consensus = all_rankings[all_rankings['Consensus Score'] > 0.7]\n",
        "medium_consensus = all_rankings[(all_rankings['Consensus Score'] > 0.4) &\n",
        "                                 (all_rankings['Consensus Score'] <= 0.7)]\n",
        "low_consensus = all_rankings[all_rankings['Consensus Score'] <= 0.4]\n",
        "\n",
        "print(f\"\\nüìà FEATURES DE ALTO CONSENSO (Score > 0.7):\")\n",
        "print(f\"   Total: {len(high_consensus)}\")\n",
        "if len(high_consensus) > 0:\n",
        "    for feat in high_consensus.index:\n",
        "        score = high_consensus.loc[feat, 'Consensus Score']\n",
        "        std = high_consensus.loc[feat, 'Std Rank']\n",
        "        print(f\"   ‚Ä¢ {feat:15s} - Score: {score:.3f} - Std: {std:.2f}\")\n",
        "\n",
        "print(f\"\\nüìä FEATURES DE CONSENSO MEDIO (Score 0.4-0.7):\")\n",
        "print(f\"   Total: {len(medium_consensus)}\")\n",
        "if len(medium_consensus) > 0:\n",
        "    for feat in medium_consensus.index[:5]:  # Mostrar solo top 5\n",
        "        score = medium_consensus.loc[feat, 'Consensus Score']\n",
        "        std = medium_consensus.loc[feat, 'Std Rank']\n",
        "        print(f\"   ‚Ä¢ {feat:15s} - Score: {score:.3f} - Std: {std:.2f}\")\n",
        "    if len(medium_consensus) > 5:\n",
        "        print(f\"   ... y {len(medium_consensus) - 5} m√°s\")\n",
        "\n",
        "print(f\"\\nüìâ FEATURES DE BAJO CONSENSO (Score < 0.4):\")\n",
        "print(f\"   Total: {len(low_consensus)}\")\n",
        "if len(low_consensus) > 0:\n",
        "    print(\"   Estas features tienen importancia inconsistente entre m√©todos\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# FEATURES PARA FEATURE SELECTION\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÇÔ∏è  RECOMENDACIONES PARA FEATURE SELECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Features a mantener definitivamente\n",
        "keep_features = high_consensus.index.tolist()\n",
        "\n",
        "# Features a considerar\n",
        "consider_features = medium_consensus.index.tolist()\n",
        "\n",
        "# Features a eliminar potencialmente\n",
        "remove_features = low_consensus.index.tolist()\n",
        "\n",
        "print(f\"\"\"\n",
        "BASADO EN CONSENSO:\n",
        "\n",
        "‚úÖ MANTENER (Alto consenso - {len(keep_features)} features):\n",
        "   {', '.join(keep_features)}\n",
        "\n",
        "   Raz√≥n: Importantes en la mayor√≠a de m√©todos\n",
        "   Confianza: Alta\n",
        "\n",
        "‚ö†Ô∏è  REVISAR (Consenso medio - {len(consider_features)} features):\n",
        "   {', '.join(consider_features[:5])}{'...' if len(consider_features) > 5 else ''}\n",
        "\n",
        "   Raz√≥n: Importancia variable seg√∫n m√©todo\n",
        "   Confianza: Media\n",
        "   Acci√≥n: Mantener por ahora, monitorear\n",
        "\n",
        "‚ùå CONSIDERAR ELIMINAR (Bajo consenso - {len(remove_features)} features):\n",
        "   {', '.join(remove_features)}\n",
        "\n",
        "   Raz√≥n: Baja importancia en todos los m√©todos\n",
        "   Confianza: Alta\n",
        "   Acci√≥n: Eliminar para simplificar modelo\n",
        "\n",
        "üí° PROCESO RECOMENDADO:\n",
        "   1. Entrenar modelo solo con features de alto consenso\n",
        "   2. Comparar performance con modelo completo\n",
        "   3. Si performance similar ‚Üí usar modelo simplificado\n",
        "   4. Si performance cae ‚Üí investigar features de consenso medio\n",
        "\"\"\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# GUARDAR RESULTADOS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüìÑ Guardando resultados...\")\n",
        "\n",
        "all_rankings.to_csv('feature_importance_consolidated.csv')\n",
        "print(\"‚úÖ Rankings consolidados guardados: feature_importance_consolidated.csv\")\n",
        "\n",
        "# Crear archivo con recomendaciones\n",
        "recommendations = f\"\"\"# Feature Selection Recommendations\n",
        "Date: {pd.Timestamp.now()}\n",
        "\n",
        "## High Priority Features (Keep)\n",
        "{chr(10).join(f'- {feat}' for feat in keep_features)}\n",
        "\n",
        "## Medium Priority Features (Review)\n",
        "{chr(10).join(f'- {feat}' for feat in consider_features)}\n",
        "\n",
        "## Low Priority Features (Consider Removing)\n",
        "{chr(10).join(f'- {feat}' for feat in remove_features)}\n",
        "\n",
        "## Statistics\n",
        "- Total features: {len(X.columns)}\n",
        "- High consensus: {len(keep_features)} ({len(keep_features)/len(X.columns)*100:.1f}%)\n",
        "- Medium consensus: {len(consider_features)} ({len(consider_features)/len(X.columns)*100:.1f}%)\n",
        "- Low consensus: {len(remove_features)} ({len(remove_features)/len(X.columns)*100:.1f}%)\n",
        "\"\"\"\n",
        "\n",
        "with open('feature_selection_recommendations.md', 'w') as f:\n",
        "    f.write(recommendations)\n",
        "\n",
        "print(\"‚úÖ Recomendaciones guardadas: feature_selection_recommendations.md\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ AN√ÅLISIS CONSOLIDADO COMPLETADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# INTERPRETACI√ìN PEDAG√ìGICA\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüí° INTERPRETACI√ìN PEDAG√ìGICA:\")\n",
        "print(\"\"\"\n",
        "1. ¬øPOR QU√â M√öLTIPLES M√âTODOS?:\n",
        "   ‚Ä¢ Tree-based: Sesgado a features con muchas categor√≠as\n",
        "   ‚Ä¢ Linear: Asume independencia entre features\n",
        "   ‚Ä¢ SHAP: Computacionalmente costoso pero m√°s robusto\n",
        "   ‚Ä¢ Permutation: Considera interacciones pero lento\n",
        "   ‚Ä¢ Consenso: M√°s confiable que cualquier m√©todo individual\n",
        "\n",
        "2. INTERPRETACI√ìN DE STD RANK:\n",
        "   ‚Ä¢ Std bajo: Todos los m√©todos concuerdan\n",
        "   ‚Ä¢ Std alto: M√©todos no concuerdan\n",
        "   ‚Ä¢ Alto consensus + Std bajo = feature cr√≠tica\n",
        "   ‚Ä¢ Alto consensus + Std alto = investigar por qu√©\n",
        "\n",
        "3. CORRELACI√ìN ENTRE FEATURES:\n",
        "   ‚Ä¢ Features correlacionadas comparten importancia\n",
        "   ‚Ä¢ Un m√©todo puede asignar toda importancia a una\n",
        "   ‚Ä¢ Otro m√©todo puede repartir entre ambas\n",
        "   ‚Ä¢ Soluci√≥n: Analizar correlaciones antes de eliminar\n",
        "\n",
        "4. APLICACIONES:\n",
        "   ‚Ä¢ Reducci√≥n de dimensionalidad\n",
        "   ‚Ä¢ Feature engineering (enfocar esfuerzos)\n",
        "   ‚Ä¢ Interpretaci√≥n del modelo\n",
        "   ‚Ä¢ Debug (¬øpor qu√© esta feature es importante?)\n",
        "\n",
        "5. PRECAUCIONES:\n",
        "   ‚Ä¢ Importancia ‚â† Causalidad\n",
        "   ‚Ä¢ Context-dependent (cambia con datos/modelo)\n",
        "   ‚Ä¢ No eliminar sin validar impacto en performance\n",
        "   ‚Ä¢ En medicina: validar con expertos del dominio\n",
        "\n",
        "6. MEJORES PR√ÅCTICAS:\n",
        "   ‚Ä¢ Siempre usar m√∫ltiples m√©todos\n",
        "   ‚Ä¢ Documentar por qu√© se elimina una feature\n",
        "   ‚Ä¢ A/B test en producci√≥n antes de eliminar\n",
        "   ‚Ä¢ Monitorear si importancias cambian con tiempo\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "yGldXv7EqLUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title NUEVA FASE: An√°lisis de Calibraci√≥n de Probabilidades\n",
        "\n",
        "\"\"\"\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "MEJORA #7: AN√ÅLISIS DE CALIBRACI√ìN DE PROBABILIDADES\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "CONCEPTOS CLAVE:\n",
        "\n",
        "1. ¬øQU√â ES CALIBRACI√ìN?:\n",
        "   - Modelo calibrado: probabilidades reflejan frecuencias reales\n",
        "   - Ejemplo: Si dice 70%, deber√≠a acertar 7 de cada 10 veces\n",
        "   - Cr√≠tico en medicina para toma de decisiones\n",
        "\n",
        "2. ¬øPOR QU√â ES IMPORTANTE?:\n",
        "   - Accuracy alta NO garantiza buena calibraci√≥n\n",
        "   - Modelos pueden discriminar bien pero estar mal calibrados\n",
        "   - En medicina: Decisiones basadas en thresholds de probabilidad\n",
        "\n",
        "3. CURVA DE CALIBRACI√ìN:\n",
        "   - Perfecto: L√≠nea diagonal (predicho = observado)\n",
        "   - Por encima diagonal: Subestima probabilidades\n",
        "   - Por debajo diagonal: Sobreestima probabilidades\n",
        "\n",
        "4. BRIER SCORE:\n",
        "   - M√©trica de calibraci√≥n: (0 = perfecto, 1 = peor)\n",
        "   - Combina calibraci√≥n y discriminaci√≥n\n",
        "   - Penaliza probabilidades incorrectas\n",
        "\n",
        "5. M√âTODOS DE RECALIBRACI√ìN:\n",
        "   - Platt Scaling: Para modelos lineales y SVMs\n",
        "   - Isotonic Regression: Para cualquier modelo, m√°s flexible\n",
        "   - Temperature Scaling: Para redes neuronales\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
        "from sklearn.metrics import brier_score_loss\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"üìä AN√ÅLISIS DE CALIBRACI√ìN DE PROBABILIDADES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# PREPARAR DATOS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüîß Preparando an√°lisis de calibraci√≥n...\")\n",
        "\n",
        "# Dictionary de modelos\n",
        "models_dict = {\n",
        "    'Logistic Regression': lr_model,\n",
        "    'Random Forest': rf_model,\n",
        "    'XGBoost': xgb_model,\n",
        "    'Neural Network': nn_model,\n",
        "    'Ensemble': ensemble_model\n",
        "}\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# CALCULAR CURVAS DE CALIBRACI√ìN\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüìà Calculando curvas de calibraci√≥n...\")\n",
        "\n",
        "calibration_data = {}\n",
        "\n",
        "for name, model in models_dict.items():\n",
        "    print(f\"   ‚Ä¢ {name}...\")\n",
        "\n",
        "    # Obtener probabilidades\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    else:  # Neural Network\n",
        "        y_prob = model.predict(X_test_scaled).ravel()\n",
        "\n",
        "    # Calcular curva de calibraci√≥n\n",
        "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
        "        y_test,\n",
        "        y_prob,\n",
        "        n_bins=10,\n",
        "        strategy='uniform'\n",
        "    )\n",
        "\n",
        "    # Calcular Brier Score\n",
        "    brier = brier_score_loss(y_test, y_prob)\n",
        "\n",
        "    # Guardar datos\n",
        "    calibration_data[name] = {\n",
        "        'prob_pred': mean_predicted_value,\n",
        "        'prob_true': fraction_of_positives,\n",
        "        'brier_score': brier,\n",
        "        'probabilities': y_prob\n",
        "    }\n",
        "\n",
        "print(\"\\n‚úÖ Curvas calculadas para todos los modelos\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# VISUALIZACI√ìN: CALIBRATION CURVES\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüìä Generando visualizaci√≥n de curvas de calibraci√≥n...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
        "\n",
        "for idx, (name, data) in enumerate(calibration_data.items()):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Plot de referencia (perfectamente calibrado)\n",
        "    ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfecto', alpha=0.7)\n",
        "\n",
        "    # Plot de calibraci√≥n del modelo\n",
        "    ax.plot(\n",
        "        data['prob_pred'],\n",
        "        data['prob_true'],\n",
        "        's-',\n",
        "        linewidth=2,\n",
        "        markersize=8,\n",
        "        color=colors[idx],\n",
        "        label=f\"{name}\\n(Brier: {data['brier_score']:.4f})\"\n",
        "    )\n",
        "\n",
        "    # Configuraci√≥n\n",
        "    ax.set_xlabel('Probabilidad Predicha', fontsize=11)\n",
        "    ax.set_ylabel('Fracci√≥n de Positivos', fontsize=11)\n",
        "    ax.set_title(f'Calibration Curve - {name}', fontsize=12, weight='bold')\n",
        "    ax.legend(loc='upper left', fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xlim([-0.05, 1.05])\n",
        "    ax.set_ylim([-0.05, 1.05])\n",
        "\n",
        "# √öltimo subplot: Resumen\n",
        "axes[-1].axis('off')\n",
        "calibration_text = \"\"\"\n",
        "INTERPRETACI√ìN DE CALIBRATION CURVES:\n",
        "\n",
        "‚úì BIEN CALIBRADO:\n",
        "  ‚Ä¢ Puntos cerca de la diagonal\n",
        "  ‚Ä¢ Brier Score < 0.15\n",
        "\n",
        "‚ö†Ô∏è SUBESTIMACI√ìN:\n",
        "  ‚Ä¢ Puntos POR ENCIMA de diagonal\n",
        "  ‚Ä¢ Predice probabilidades MUY BAJAS\n",
        "  ‚Ä¢ Soluci√≥n: Ajustar threshold hacia arriba\n",
        "\n",
        "‚ö†Ô∏è SOBREESTIMACI√ìN:\n",
        "  ‚Ä¢ Puntos POR DEBAJO de diagonal\n",
        "  ‚Ä¢ Predice probabilidades MUY ALTAS\n",
        "  ‚Ä¢ Soluci√≥n: Ajustar threshold hacia abajo\n",
        "\n",
        "BRIER SCORE:\n",
        "  ‚Ä¢ 0.0 = Perfecto\n",
        "  ‚Ä¢ < 0.15 = Excelente\n",
        "  ‚Ä¢ 0.15-0.25 = Aceptable\n",
        "  ‚Ä¢ > 0.25 = Requiere recalibraci√≥n\n",
        "\"\"\"\n",
        "\n",
        "axes[-1].text(\n",
        "    0.05, 0.95,\n",
        "    calibration_text,\n",
        "    fontsize=10,\n",
        "    family='monospace',\n",
        "    verticalalignment='top',\n",
        "    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3)\n",
        ")\n",
        "\n",
        "plt.suptitle('An√°lisis de Calibraci√≥n - Todos los Modelos',\n",
        "             fontsize=16, weight='bold', y=0.995)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
        "plt.savefig('calibration_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualizaci√≥n guardada: calibration_analysis.png\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# TABLA DE M√âTRICAS DE CALIBRACI√ìN\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä M√âTRICAS DE CALIBRACI√ìN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "calibration_df = pd.DataFrame({\n",
        "    'Modelo': list(calibration_data.keys()),\n",
        "    'Brier Score': [data['brier_score'] for data in calibration_data.values()],\n",
        "})\n",
        "\n",
        "# Calcular Expected Calibration Error (ECE)\n",
        "def calculate_ece(y_true, y_prob, n_bins=10):\n",
        "    \"\"\"\n",
        "    Calculate Expected Calibration Error.\n",
        "\n",
        "    ECE mide la diferencia promedio entre probabilidades predichas\n",
        "    y frecuencias observadas.\n",
        "    \"\"\"\n",
        "    bins = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bins[:-1]\n",
        "    bin_uppers = bins[1:]\n",
        "\n",
        "    ece = 0.0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        # Encontrar muestras en este bin\n",
        "        in_bin = (y_prob >= bin_lower) & (y_prob < bin_upper)\n",
        "        prop_in_bin = np.mean(in_bin)\n",
        "\n",
        "        if prop_in_bin > 0:\n",
        "            accuracy_in_bin = np.mean(y_true[in_bin])\n",
        "            avg_confidence_in_bin = np.mean(y_prob[in_bin])\n",
        "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece\n",
        "\n",
        "# Calcular ECE para cada modelo\n",
        "ece_scores = []\n",
        "for name, data in calibration_data.items():\n",
        "    ece = calculate_ece(y_test, data['probabilities'])\n",
        "    ece_scores.append(ece)\n",
        "\n",
        "calibration_df['ECE'] = ece_scores\n",
        "\n",
        "# Agregar interpretaci√≥n\n",
        "def interpret_brier(score):\n",
        "    if score < 0.10:\n",
        "        return \"Excelente\"\n",
        "    elif score < 0.15:\n",
        "        return \"Muy Bueno\"\n",
        "    elif score < 0.25:\n",
        "        return \"Aceptable\"\n",
        "    else:\n",
        "        return \"Requiere Mejora\"\n",
        "\n",
        "calibration_df['Interpretaci√≥n'] = calibration_df['Brier Score'].apply(interpret_brier)\n",
        "\n",
        "# Ordenar por Brier Score\n",
        "calibration_df = calibration_df.sort_values('Brier Score')\n",
        "\n",
        "print(\"\\n\" + calibration_df.to_string(index=False))\n",
        "print(\"\\nüìå ECE (Expected Calibration Error): M√©trica complementaria\")\n",
        "print(\"   Valores menores son mejores (0 = perfecto)\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# AN√ÅLISIS DEL MEJOR MODELO\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "best_calibrated_model = calibration_df.iloc[0]['Modelo']\n",
        "best_brier = calibration_df.iloc[0]['Brier Score']\n",
        "best_ece = calibration_df.iloc[0]['ECE']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üèÜ MEJOR MODELO CALIBRADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n   Modelo: {best_calibrated_model}\")\n",
        "print(f\"   Brier Score: {best_brier:.4f}\")\n",
        "print(f\"   ECE: {best_ece:.4f}\")\n",
        "print(f\"   Status: {calibration_df.iloc[0]['Interpretaci√≥n']}\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# COMPARACI√ìN: Mejor Performance vs Mejor Calibraci√≥n\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚öñÔ∏è  COMPARACI√ìN: PERFORMANCE vs CALIBRACI√ìN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n   Mejor por Performance:  {best_model_name}\")\n",
        "print(f\"   Mejor por Calibraci√≥n:  {best_calibrated_model}\")\n",
        "\n",
        "if best_model_name == best_calibrated_model:\n",
        "    print(\"\\n   ‚úÖ ¬°Mismo modelo! Excelente balance.\")\n",
        "else:\n",
        "    print(\"\\n   ‚ö†Ô∏è  Modelos diferentes. Considerar:\")\n",
        "    print(f\"      ‚Ä¢ Si threshold fijo ‚Üí usar {best_calibrated_model}\")\n",
        "    print(f\"      ‚Ä¢ Si solo predicci√≥n ‚Üí usar {best_model_name}\")\n",
        "    print(f\"      ‚Ä¢ Si probabilidades cr√≠ticas ‚Üí recalibrar {best_model_name}\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# DEMOSTRACI√ìN DE RECALIBRACI√ìN (Platt Scaling)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîß DEMOSTRACI√ìN: RECALIBRACI√ìN CON PLATT SCALING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Recalibrar el mejor modelo (por performance)\n",
        "print(f\"\\n   Recalibrando {best_model_name} con Platt Scaling...\")\n",
        "\n",
        "calibrated_clf = CalibratedClassifierCV(\n",
        "    best_model,\n",
        "    method='sigmoid',  # Platt Scaling\n",
        "    cv='prefit'  # Modelo ya entrenado\n",
        ")\n",
        "\n",
        "# Fit en train\n",
        "calibrated_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predicciones calibradas\n",
        "y_prob_calibrated = calibrated_clf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# M√©tricas antes y despu√©s\n",
        "brier_before = brier_score_loss(y_test,\n",
        "    best_model.predict_proba(X_test_scaled)[:, 1] if hasattr(best_model, 'predict_proba')\n",
        "    else best_model.predict(X_test_scaled).ravel()\n",
        ")\n",
        "brier_after = brier_score_loss(y_test, y_prob_calibrated)\n",
        "\n",
        "print(f\"\\n   Brier Score ANTES:    {brier_before:.4f}\")\n",
        "print(f\"   Brier Score DESPU√âS:  {brier_after:.4f}\")\n",
        "print(f\"   Mejora:               {(brier_before - brier_after):.4f} \", end=\"\")\n",
        "\n",
        "if brier_after < brier_before:\n",
        "    print(\"‚úÖ\")\n",
        "else:\n",
        "    print(\"‚ùå (No mejor√≥)\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# VISUALIZACI√ìN: ANTES vs DESPU√âS DE RECALIBRACI√ìN\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüìä Generando comparaci√≥n antes/despu√©s de recalibraci√≥n...\")\n",
        "\n",
        "# Obtener probabilidades originales\n",
        "if hasattr(best_model, 'predict_proba'):\n",
        "    y_prob_original = best_model.predict_proba(X_test_scaled)[:, 1]\n",
        "else:\n",
        "    y_prob_original = best_model.predict(X_test_scaled).ravel()\n",
        "\n",
        "# Calcular curvas\n",
        "frac_pos_orig, mean_pred_orig = calibration_curve(y_test, y_prob_original, n_bins=10)\n",
        "frac_pos_cal, mean_pred_cal = calibration_curve(y_test, y_prob_calibrated, n_bins=10)\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Subplot 1: Calibration Curves\n",
        "ax1.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfecto', alpha=0.7)\n",
        "ax1.plot(mean_pred_orig, frac_pos_orig, 's-', linewidth=2, markersize=8,\n",
        "         color='red', label=f'Original (Brier: {brier_before:.4f})')\n",
        "ax1.plot(mean_pred_cal, frac_pos_cal, 'o-', linewidth=2, markersize=8,\n",
        "         color='green', label=f'Calibrado (Brier: {brier_after:.4f})')\n",
        "ax1.set_xlabel('Probabilidad Predicha', fontsize=12)\n",
        "ax1.set_ylabel('Fracci√≥n de Positivos', fontsize=12)\n",
        "ax1.set_title(f'Comparaci√≥n de Calibraci√≥n\\n{best_model_name}',\n",
        "              fontsize=13, weight='bold')\n",
        "ax1.legend(loc='upper left', fontsize=10)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: Distribuci√≥n de Probabilidades\n",
        "ax2.hist(y_prob_original, bins=20, alpha=0.6, label='Original', color='red', edgecolor='black')\n",
        "ax2.hist(y_prob_calibrated, bins=20, alpha=0.6, label='Calibrado', color='green', edgecolor='black')\n",
        "ax2.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Threshold 0.5')\n",
        "ax2.set_xlabel('Probabilidad Predicha', fontsize=12)\n",
        "ax2.set_ylabel('Frecuencia', fontsize=12)\n",
        "ax2.set_title('Distribuci√≥n de Probabilidades\\nAntes vs Despu√©s',\n",
        "              fontsize=13, weight='bold')\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('calibration_before_after.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Comparaci√≥n guardada: calibration_before_after.png\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# RECOMENDACIONES\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üí° RECOMENDACIONES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\"\"\n",
        "BASADO EN EL AN√ÅLISIS:\n",
        "\n",
        "1. MODELO MEJOR CALIBRADO:\n",
        "   ‚Ä¢ {best_calibrated_model}\n",
        "   ‚Ä¢ Brier Score: {best_brier:.4f}\n",
        "   ‚Ä¢ Usar si decisiones basadas en thresholds de probabilidad\n",
        "\n",
        "2. ¬øRECALIBRAR?:\n",
        "   ‚Ä¢ Brier antes: {brier_before:.4f}\n",
        "   ‚Ä¢ Brier despu√©s: {brier_after:.4f}\n",
        "   ‚Ä¢ Recomendaci√≥n: {'S√≠, aplicar Platt Scaling' if brier_after < brier_before - 0.01 else 'No necesario'}\n",
        "\n",
        "3. USO EN PRODUCCI√ìN:\n",
        "   ‚Ä¢ Para screening (threshold bajo): Usar modelo con mejor Recall\n",
        "   ‚Ä¢ Para confirmaci√≥n (threshold alto): Usar modelo calibrado\n",
        "   ‚Ä¢ Para probabilidades: SIEMPRE usar modelo calibrado\n",
        "\n",
        "4. VALIDACI√ìN:\n",
        "   ‚Ä¢ Validar calibraci√≥n en holdout set adicional\n",
        "   ‚Ä¢ Monitorear Brier Score en producci√≥n\n",
        "   ‚Ä¢ Recalibrar si Brier aumenta > 20%\n",
        "\n",
        "5. PR√ìXIMOS PASOS:\n",
        "   ‚Ä¢ Documentar threshold elegido y su justificaci√≥n\n",
        "   ‚Ä¢ Crear lookup table: probabilidad ‚Üí acci√≥n cl√≠nica\n",
        "   ‚Ä¢ Establecer alertas para monitoreo de calibraci√≥n\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ AN√ÅLISIS DE CALIBRACI√ìN COMPLETADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# INTERPRETACI√ìN PEDAG√ìGICA\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüí° INTERPRETACI√ìN PEDAG√ìGICA:\")\n",
        "print(\"\"\"\n",
        "1. CALIBRACI√ìN vs DISCRIMINACI√ìN:\n",
        "   ‚Ä¢ Discriminaci√≥n: Separar clases (medido por AUC)\n",
        "   ‚Ä¢ Calibraci√≥n: Probabilidades correctas (medido por Brier)\n",
        "   ‚Ä¢ Un modelo puede tener alto AUC pero mala calibraci√≥n\n",
        "\n",
        "2. ¬øPOR QU√â IMPORTA EN MEDICINA?:\n",
        "   ‚Ä¢ Decisi√≥n: \"Tratar si probabilidad > 70%\"\n",
        "   ‚Ä¢ Mal calibrado: 70% podr√≠a ser realmente 50% o 90%\n",
        "   ‚Ä¢ Consecuencias: Tratamiento incorrecto\n",
        "\n",
        "3. EJEMPLO PR√ÅCTICO:\n",
        "   ‚Ä¢ Modelo dice: 80% probabilidad de enfermedad\n",
        "   ‚Ä¢ Bien calibrado: 8 de cada 10 casos ser√°n enfermos\n",
        "   ‚Ä¢ Mal calibrado: Podr√≠a ser 6/10 o 9/10\n",
        "   ‚Ä¢ Diferencia cr√≠tica para toma de decisiones\n",
        "\n",
        "4. M√âTODOS DE RECALIBRACI√ìN:\n",
        "   ‚Ä¢ Platt Scaling: R√°pido, param√©trico\n",
        "   ‚Ä¢ Isotonic: M√°s flexible, requiere m√°s datos\n",
        "   ‚Ä¢ Temperature Scaling: Espec√≠fico para NNs\n",
        "\n",
        "5. EN LA INDUSTRIA:\n",
        "   ‚Ä¢ Google/Facebook: Recalibran modelos de ads\n",
        "   ‚Ä¢ Medicina: Regulaci√≥n requiere calibraci√≥n\n",
        "   ‚Ä¢ Finance: Critical para credit scoring\n",
        "   ‚Ä¢ Clima: Pron√≥sticos deben estar calibrados\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "Q5sJtbTXq6p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_25"
      },
      "source": [
        "# üîç FASE 13: Interpretabilidad con SHAP\n",
        "\n",
        "**SHAP** (SHapley Additive exPlanations) explica predicciones usando valores de Shapley de teor√≠a de juegos.\n",
        "\n",
        "## ¬øQU√â ES SHAP Y POR QU√â IMPORTA?                        \n",
        "----------------------------------\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) es una t√©cnica de interpretabilidad\n",
        "que responde: \"¬øPor qu√© el modelo hizo esta predicci√≥n?\"\n",
        "\n",
        "### üìå CONCEPTOS CLAVE:\n",
        "   * **SHAP Value**: Contribuci√≥n de cada feature a la predicci√≥n\n",
        "   * **Positivo** ‚Üí Empuja hacia clase 1 (Disease)\n",
        "   * **Negativo** ‚Üí Empuja hacia clase 0 (No Disease)\n",
        "   * **Base Value**: Predicci√≥n promedio del modelo\n",
        "\n",
        "### üéØ APLICACIONES:\n",
        "   * **Medicina**: Explicar diagn√≥sticos (requerido por ley en algunos pa√≠ses)\n",
        "   * **Finanzas**: Justificar decisiones de cr√©dito (Fair Lending)\n",
        "   * **Legal**: Auditor√≠a de modelos (evitar discriminaci√≥n)\n",
        "\n",
        "### üìä GR√ÅFICOS QUE GENERAREMOS:\n",
        "   1. Summary Plot ‚Üí Importancia global de features\n",
        "   2. Feature Importance ‚Üí Ranking de features\n",
        "   3. Waterfall Plot ‚Üí Explicaci√≥n de predicciones individuales\n",
        "   4. Dependence Plot ‚Üí Interacciones entre features\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_26"
      },
      "outputs": [],
      "source": [
        "# @title FASE 13: SHAP - An√°lisis Completo de Interpretabilidad (CORREGIDA)\n",
        "\n",
        "\"\"\"\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "FASE 13: SHAP ANALYSIS EXHAUSTIVO - VERSI√ìN CORREGIDA Y DEFINITIVA\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "CORRECCIONES APLICADAS:\n",
        "1. ‚úÖ Corregido: shap_values_to_use ‚Üí shap_values\n",
        "2. ‚úÖ Corregido: Manejo robusto de expected_value\n",
        "3. ‚úÖ Corregido: Waterfall plots con API estable\n",
        "4. ‚úÖ Corregido: Conversi√≥n correcta de Index a strings\n",
        "5. ‚úÖ Corregido: Manejo de arrays multidimensionales\n",
        "6. ‚úÖ Agregado: Validaciones y fallbacks\n",
        "\n",
        "CONCEPTOS CLAVE:\n",
        "- SHAP Values: Contribuci√≥n de cada feature a la predicci√≥n\n",
        "- Base Value: Predicci√≥n promedio del modelo\n",
        "- SHAP > 0: Empuja hacia clase positiva (enfermo)\n",
        "- SHAP < 0: Empuja hacia clase negativa (sano)\n",
        "\"\"\"\n",
        "\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"üéØ FASE 13: SHAP - AN√ÅLISIS COMPLETO DE INTERPRETABILIDAD\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# PASO 1: PREPARACI√ìN DEL EXPLAINER\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nüîß Preparando SHAP Explainer...\")\n",
        "\n",
        "# Determinar tipo de explainer seg√∫n modelo\n",
        "if best_model_name in ['Random Forest', 'XGBoost']:\n",
        "    # TreeExplainer para modelos basados en √°rboles\n",
        "    explainer = shap.TreeExplainer(best_model)\n",
        "    explainer_type = \"TreeExplainer\"\n",
        "\n",
        "elif best_model_name == 'Logistic Regression':\n",
        "    # LinearExplainer para modelos lineales\n",
        "    explainer = shap.LinearExplainer(best_model, X_train_final)\n",
        "    explainer_type = \"LinearExplainer\"\n",
        "\n",
        "else:  # Neural Network o Ensemble\n",
        "    # KernelExplainer (model-agnostic, m√°s lento)\n",
        "    explainer = shap.KernelExplainer(\n",
        "        best_model.predict_proba if hasattr(best_model, 'predict_proba') else best_model.predict,\n",
        "        shap.sample(X_train_final, 100)\n",
        "    )\n",
        "    explainer_type = \"KernelExplainer\"\n",
        "\n",
        "print(f\"‚úÖ {explainer_type} creado para {best_model_name}\")\n",
        "\n",
        "# ============================================================================\n",
        "# PASO 2: CALCULAR SHAP VALUES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nüìä Calculando SHAP values...\")\n",
        "print(\"   (Esto puede tomar 30-60 segundos)\")\n",
        "\n",
        "shap_values = explainer.shap_values(X_test_scaled)\n",
        "\n",
        "# Manejar diferentes formatos de salida\n",
        "if isinstance(shap_values, list):\n",
        "    # TreeExplainer devuelve lista [clase_0, clase_1]\n",
        "    shap_values = shap_values[1]  # Clase positiva (enfermo)\n",
        "    print(f\"‚úÖ SHAP values calculados (formato lista ‚Üí clase positiva)\")\n",
        "elif len(shap_values.shape) == 3:\n",
        "    # Formato 3D: (n_samples, n_features, n_classes)\n",
        "    shap_values = shap_values[:, :, 1]\n",
        "    print(f\"‚úÖ SHAP values calculados (formato 3D ‚Üí clase positiva)\")\n",
        "else:\n",
        "    # Formato 2D est√°ndar: (n_samples, n_features)\n",
        "    print(f\"‚úÖ SHAP values calculados (formato est√°ndar)\")\n",
        "\n",
        "print(f\"   Shape final: {shap_values.shape}\")\n",
        "\n",
        "# Validar shape\n",
        "assert shap_values.shape[0] == X_test_scaled.shape[0], \"Mismatch en n√∫mero de samples\"\n",
        "assert shap_values.shape[1] == X_test_scaled.shape[1], \"Mismatch en n√∫mero de features\"\n",
        "\n",
        "# Obtener expected value (base value)\n",
        "if isinstance(explainer.expected_value, np.ndarray):\n",
        "    base_value = explainer.expected_value[1] if len(explainer.expected_value) > 1 else explainer.expected_value[0]\n",
        "else:\n",
        "    base_value = explainer.expected_value\n",
        "\n",
        "print(f\"   Base value (E[f(x)]): {base_value:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZACI√ìN 1: SUMMARY PLOT (Beeswarm)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä VISUALIZACI√ìN 1: SUMMARY PLOT (Importancia Global)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(\n",
        "    shap_values,\n",
        "    X_test_scaled,\n",
        "    feature_names=X.columns.tolist(),  # ‚Üê Convertir a lista\n",
        "    show=False,\n",
        "    plot_size=(12, 8)\n",
        ")\n",
        "plt.title(\"SHAP Summary Plot - Importancia y Direcci√≥n de Features\",\n",
        "          fontsize=14, weight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('shap_1_summary_beeswarm.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úÖ Summary Plot guardado: shap_1_summary_beeswarm.png\")\n",
        "print(\"\"\"\n",
        "üí° INTERPRETACI√ìN:\n",
        "   ‚Ä¢ Eje Y: Features ordenadas por importancia\n",
        "   ‚Ä¢ Eje X: Impacto en la predicci√≥n (SHAP value)\n",
        "   ‚Ä¢ Color: Valor del feature (rojo=alto, azul=bajo)\n",
        "   ‚Ä¢ Dispersi√≥n: Rango de impactos para cada feature\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZACI√ìN 2: BAR PLOT (Feature Importance Ranking)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä VISUALIZACI√ìN 2: BAR PLOT (Ranking de Importancia)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "shap.summary_plot(\n",
        "    shap_values,\n",
        "    X_test_scaled,\n",
        "    feature_names=X.columns.tolist(),\n",
        "    plot_type=\"bar\",\n",
        "    show=False\n",
        ")\n",
        "plt.title(\"SHAP Feature Importance - Ranking Absoluto\",\n",
        "          fontsize=14, weight='bold', pad=15)\n",
        "plt.tight_layout()\n",
        "plt.savefig('shap_2_importance_bar.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úÖ Bar Plot guardado: shap_2_importance_bar.png\")\n",
        "print(\"\"\"\n",
        "üí° INTERPRETACI√ìN:\n",
        "   ‚Ä¢ Muestra |SHAP| promedio (magnitud, sin direcci√≥n)\n",
        "   ‚Ä¢ Features m√°s arriba = mayor impacto global\n",
        "   ‚Ä¢ √ötil para feature selection\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# TOP 3 FEATURES M√ÅS IMPORTANTES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üèÜ IDENTIFICANDO TOP 3 FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# CORRECCI√ìN CR√çTICA: Usar shap_values en lugar de shap_values_to_use\n",
        "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
        "top_3_indices = mean_abs_shap.argsort()[-3:][::-1]\n",
        "\n",
        "# Convertir a lista de strings expl√≠citamente\n",
        "feature_names_list = X.columns.tolist()\n",
        "top_3_features = [feature_names_list[idx] for idx in top_3_indices]\n",
        "\n",
        "print(\"\\nüèÜ TOP 3 FEATURES M√ÅS IMPORTANTES:\")\n",
        "for i, (idx, feat_name) in enumerate(zip(top_3_indices, top_3_features), 1):\n",
        "    shap_mean = mean_abs_shap[idx]\n",
        "    print(f\"   {i}. {feat_name:15s} (|SHAP| medio: {shap_mean:.4f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZACI√ìN 3: DEPENDENCE PLOTS (Top 3 Features)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä VISUALIZACI√ìN 3: DEPENDENCE PLOTS (Top 3 Features)\")\n",
        "print(\"=\"*80)\n",
        "print(\"üí° Muestra c√≥mo cada feature afecta las predicciones\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for i, (feature_idx, feature_name) in enumerate(zip(top_3_indices, top_3_features)):\n",
        "    print(f\"   ‚Ä¢ Generando plot para: {feature_name}\")\n",
        "\n",
        "    plt.sca(axes[i])\n",
        "\n",
        "    shap.dependence_plot(\n",
        "        feature_idx,  # √çndice num√©rico\n",
        "        shap_values,\n",
        "        X_test_scaled,\n",
        "        feature_names=feature_names_list,\n",
        "        show=False,\n",
        "        ax=axes[i]\n",
        "    )\n",
        "\n",
        "    axes[i].set_title(f'Dependence Plot\\n{feature_name}',\n",
        "                      fontsize=11, weight='bold')\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('shap_3_dependence_plots.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úÖ Dependence Plots guardados: shap_3_dependence_plots.png\")\n",
        "print(\"\"\"\n",
        "üí° INTERPRETACI√ìN:\n",
        "   ‚Ä¢ Eje X: Valor del feature\n",
        "   ‚Ä¢ Eje Y: SHAP value (impacto)\n",
        "   ‚Ä¢ Color: Feature con mayor interacci√≥n\n",
        "   ‚Ä¢ Tendencia: Relaci√≥n feature-output\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZACI√ìN 4: WATERFALL PLOT (Explicaciones Individuales)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä VISUALIZACI√ìN 4: WATERFALL PLOT (Explicaci√≥n Individual)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Encontrar casos interesantes\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "y_test_array = np.array(y_test)\n",
        "\n",
        "tp_indices = np.where((y_test_array == 1) & (y_pred == 1))[0]\n",
        "tn_indices = np.where((y_test_array == 0) & (y_pred == 0))[0]\n",
        "\n",
        "tp_idx = tp_indices[0] if len(tp_indices) > 0 else 0\n",
        "tn_idx = tn_indices[0] if len(tn_indices) > 0 else 1\n",
        "\n",
        "# Crear waterfall plots con API estable\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Waterfall 1: True Positive\n",
        "plt.sca(axes[0])\n",
        "shap.waterfall_plot(\n",
        "    shap.Explanation(\n",
        "        values=shap_values[tp_idx],\n",
        "        base_values=base_value,\n",
        "        data=X_test_scaled[tp_idx],\n",
        "        feature_names=feature_names_list\n",
        "    ),\n",
        "    max_display=10,\n",
        "    show=False\n",
        ")\n",
        "axes[0].set_title(f'Caso 1: Predicci√≥n Correcta - Enfermo (TP)\\n' +\n",
        "                  f'Real: {y_test_array[tp_idx]}, Predicho: {y_pred[tp_idx]}',\n",
        "                  fontsize=11, weight='bold')\n",
        "\n",
        "# Waterfall 2: True Negative\n",
        "plt.sca(axes[1])\n",
        "shap.waterfall_plot(\n",
        "    shap.Explanation(\n",
        "        values=shap_values[tn_idx],\n",
        "        base_values=base_value,\n",
        "        data=X_test_scaled[tn_idx],\n",
        "        feature_names=feature_names_list\n",
        "    ),\n",
        "    max_display=10,\n",
        "    show=False\n",
        ")\n",
        "axes[1].set_title(f'Caso 2: Predicci√≥n Correcta - Sano (TN)\\n' +\n",
        "                  f'Real: {y_test_array[tn_idx]}, Predicho: {y_pred[tn_idx]}',\n",
        "                  fontsize=11, weight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('shap_4_waterfall_plots.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úÖ Waterfall Plots guardados: shap_4_waterfall_plots.png\")\n",
        "print(\"\"\"\n",
        "üí° INTERPRETACI√ìN:\n",
        "   ‚Ä¢ Base value (E[f(x)]): Predicci√≥n promedio\n",
        "   ‚Ä¢ Cada barra: Contribuci√≥n de un feature\n",
        "   ‚Ä¢ Rojo ‚Üë: Empuja hacia enfermo\n",
        "   ‚Ä¢ Azul ‚Üì: Empuja hacia sano\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZACI√ìN 5: FORCE PLOT (M√∫ltiples Casos)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä VISUALIZACI√ìN 5: FORCE PLOT (M√∫ltiples Casos)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Inicializar JS para force plots\n",
        "shap.initjs()\n",
        "\n",
        "# Seleccionar subset de casos\n",
        "n_samples = min(20, len(shap_values))\n",
        "\n",
        "try:\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    shap.force_plot(\n",
        "        base_value,\n",
        "        shap_values[:n_samples],\n",
        "        X_test_scaled[:n_samples],\n",
        "        feature_names=feature_names_list,\n",
        "        matplotlib=True,\n",
        "        show=False\n",
        "    )\n",
        "    plt.title(f'Force Plot - Primeros {n_samples} Casos del Test Set',\n",
        "              fontsize=14, weight='bold', pad=15)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('shap_5_force_plot.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"‚úÖ Force Plot guardado: shap_5_force_plot.png\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Force plot no disponible: {str(e)}\")\n",
        "    print(\"   (No cr√≠tico - continuando con otros gr√°ficos)\")\n",
        "\n",
        "print(\"\"\"\n",
        "üí° INTERPRETACI√ìN:\n",
        "   ‚Ä¢ Cada fila: Una predicci√≥n\n",
        "   ‚Ä¢ Rojo: Features hacia \"enfermo\"\n",
        "   ‚Ä¢ Azul: Features hacia \"sano\"\n",
        "   ‚Ä¢ Ancho: Magnitud del impacto\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZACI√ìN 6: DECISION PLOT (Trayectorias)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä VISUALIZACI√ìN 6: DECISION PLOT (Trayectorias de Decisi√≥n)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Seleccionar casos interesantes\n",
        "interesting_indices = np.concatenate([\n",
        "    tp_indices[:5] if len(tp_indices) >= 5 else tp_indices,\n",
        "    tn_indices[:5] if len(tn_indices) >= 5 else tn_indices\n",
        "])\n",
        "\n",
        "if len(interesting_indices) > 0:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    try:\n",
        "        shap.decision_plot(\n",
        "            base_value,\n",
        "            shap_values[interesting_indices],\n",
        "            feature_names=feature_names_list,\n",
        "            show=False,\n",
        "            highlight=[list(range(min(5, len(tp_indices)))),\n",
        "                      list(range(min(5, len(tp_indices)), len(interesting_indices)))],\n",
        "            legend_labels=['True Positives', 'True Negatives'],\n",
        "            legend_location='lower right'\n",
        "        )\n",
        "\n",
        "        plt.title('Decision Plot - Trayectorias de Predicci√≥n\\n(Verde: TP, Morado: TN)',\n",
        "                  fontsize=14, weight='bold', pad=20)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('shap_6_decision_plot.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(\"‚úÖ Decision Plot guardado: shap_6_decision_plot.png\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Decision plot no disponible: {str(e)}\")\n",
        "        print(\"   (No cr√≠tico - an√°lisis SHAP completado exitosamente)\")\n",
        "\n",
        "print(\"\"\"\n",
        "üí° INTERPRETACI√ìN:\n",
        "   ‚Ä¢ Eje Y: Features (menos a m√°s importante)\n",
        "   ‚Ä¢ Eje X: Output del modelo\n",
        "   ‚Ä¢ Cada l√≠nea: Una predicci√≥n\n",
        "   ‚Ä¢ Trayectoria: C√≥mo cada feature ajusta\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# AN√ÅLISIS CUANTITATIVO\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìà AN√ÅLISIS CUANTITATIVO DE SHAP VALUES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear DataFrame con estad√≠sticas\n",
        "shap_stats = pd.DataFrame({\n",
        "    'Feature': feature_names_list,\n",
        "    'Mean |SHAP|': np.abs(shap_values).mean(axis=0),\n",
        "    'Std SHAP': shap_values.std(axis=0),\n",
        "    'Mean SHAP': shap_values.mean(axis=0),\n",
        "    'Max |SHAP|': np.abs(shap_values).max(axis=0)\n",
        "})\n",
        "\n",
        "# Ordenar por importancia\n",
        "shap_stats = shap_stats.sort_values('Mean |SHAP|', ascending=False)\n",
        "\n",
        "print(\"\\nüìä ESTAD√çSTICAS DE SHAP VALUES:\")\n",
        "print(\"-\"*80)\n",
        "print(shap_stats.to_string(index=False))\n",
        "\n",
        "# Guardar a CSV\n",
        "shap_stats.to_csv('shap_statistics.csv', index=False)\n",
        "print(\"\\n‚úÖ Estad√≠sticas guardadas: shap_statistics.csv\")\n",
        "\n",
        "# ============================================================================\n",
        "# AN√ÅLISIS DE COHERENCIA CL√çNICA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üè• AN√ÅLISIS DE COHERENCIA CL√çNICA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "VALIDACI√ìN CON CONOCIMIENTO M√âDICO:\n",
        "\n",
        "ESPERADO (Factores de riesgo conocidos):\n",
        "‚úì ca (n√∫mero de vasos): Indicador directo\n",
        "‚úì thal (talasemia): Relacionado con oxigenaci√≥n\n",
        "‚úì cp (dolor de pecho): S√≠ntoma primario\n",
        "‚úì oldpeak (depresi√≥n ST): Marcador ECG\n",
        "‚úì thalach (freq. card√≠aca m√°x): Capacidad\n",
        "\n",
        "COMPARAR con Top 3 identificados:\n",
        "\"\"\")\n",
        "\n",
        "for i, (feat, idx) in enumerate(zip(top_3_features, top_3_indices), 1):\n",
        "    mean_shap_positive = shap_values[y_test_array == 1, idx].mean()\n",
        "    mean_shap_negative = shap_values[y_test_array == 0, idx].mean()\n",
        "\n",
        "    print(f\"\\n{i}. {feat}:\")\n",
        "    print(f\"   ‚Ä¢ SHAP medio (enfermos): {mean_shap_positive:+.4f}\")\n",
        "    print(f\"   ‚Ä¢ SHAP medio (sanos):    {mean_shap_negative:+.4f}\")\n",
        "    print(f\"   ‚Ä¢ Diferencia:            {mean_shap_positive - mean_shap_negative:+.4f}\")\n",
        "\n",
        "print(\"\"\"\n",
        "\\nüí° INTERPRETACI√ìN:\n",
        "   ‚Ä¢ Si SHAP(enfermos) > SHAP(sanos): Correlaci√≥n positiva\n",
        "   ‚Ä¢ Validar contra literatura m√©dica\n",
        "   ‚Ä¢ Features contraintuitivas pueden indicar confounders\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# RESUMEN EJECUTIVO\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìã RESUMEN EJECUTIVO - SHAP ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\"\"\n",
        "MODELO ANALIZADO: {best_model_name}\n",
        "SAMPLES EXPLICADOS: {len(shap_values)} predicciones del test set\n",
        "BASE VALUE: {base_value:.4f}\n",
        "\n",
        "TOP 3 FEATURES:\n",
        "1. {top_3_features[0]:15s} (|SHAP| medio: {mean_abs_shap[top_3_indices[0]]:.4f})\n",
        "2. {top_3_features[1]:15s} (|SHAP| medio: {mean_abs_shap[top_3_indices[1]]:.4f})\n",
        "3. {top_3_features[2]:15s} (|SHAP| medio: {mean_abs_shap[top_3_indices[2]]:.4f})\n",
        "\n",
        "ARCHIVOS GENERADOS:\n",
        "‚úì shap_1_summary_beeswarm.png    - Vista global\n",
        "‚úì shap_2_importance_bar.png      - Ranking\n",
        "‚úì shap_3_dependence_plots.png    - Relaciones feature-output\n",
        "‚úì shap_4_waterfall_plots.png     - Explicaciones individuales\n",
        "‚úì shap_5_force_plot.png          - M√∫ltiples casos\n",
        "‚úì shap_6_decision_plot.png       - Trayectorias\n",
        "‚úì shap_statistics.csv            - Estad√≠sticas\n",
        "\n",
        "PR√ìXIMOS PASOS:\n",
        "- Revisar coherencia m√©dica\n",
        "- Explicar predicciones a stakeholders\n",
        "- Validar con expertos del dominio\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ FASE 13 COMPLETADA - SHAP Analysis Completo\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "üí° PUNTOS CLAVE:\n",
        "   1. SHAP explica predicciones usando teor√≠a de juegos\n",
        "   2. 6 visualizaciones cubren diferentes perspectivas\n",
        "   3. Validaci√≥n cl√≠nica es esencial\n",
        "   4. √ötil para debugging y explicabilidad\n",
        "   5. Requerido en muchos contextos regulados\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_27"
      },
      "source": [
        "# üìÑ FASE 14: Reportes con Evidently AI\n",
        "\n",
        "Genera reporte HTML autom√°tico con an√°lisis completo de clasificaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install evidently --upgrade --quiet\n",
        "import evidently\n",
        "print(f\"Versi√≥n: {evidently.__version__}\")  # Debe ser ‚â•0.4"
      ],
      "metadata": {
        "id": "OjmTDuQkIvlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_28"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# FASE 15: MONITOREO CON EVIDENTLY AI\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìä FASE 15: Monitoreo de Modelos con Evidently AI\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë                    EVIDENTLY AI - MONITOREO DE MODELOS                     ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "üéØ EVIDENTLY AI - AN√ÅLISIS COMPLETO:\n",
        "\n",
        "   ‚Ä¢ Classification Performance: M√©tricas detalladas del modelo\n",
        "   ‚Ä¢ Data Summary: Resumen estad√≠stico de los datos\n",
        "   ‚Ä¢ Data Drift: Detecci√≥n de cambios en distribuciones\n",
        "   ‚Ä¢ ROC-AUC, Precision-Recall, Calibration curves\n",
        "   ‚Ä¢ Comparaci√≥n Train vs Test\n",
        "\n",
        "üìä REPORTES INTEGRADOS:\n",
        "   1. Classification Performance (ROC, PR, Confusion Matrix)\n",
        "   2. Data Summary (estad√≠sticas descriptivas)\n",
        "   3. Data Drift (cambios en features)\n",
        "\n",
        "‚è±Ô∏è Tiempo estimado: 1-2 minutos\n",
        "\"\"\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# SELECCIONAR MEJOR MODELO\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüèÜ Seleccionando mejor modelo para an√°lisis...\")\n",
        "\n",
        "best_model_obj = best_model\n",
        "best_model_name = best_model\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# GENERAR REPORTE EVIDENTLY\n",
        "# ==========================================\n",
        "best_model_obj = best_model\n",
        "if best_model_obj is not None:\n",
        "    try:\n",
        "        print(f\"\\nüìä Generando reporte completo para: {best_model_name}\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "        # ==========================================\n",
        "        # PREPARAR DATOS - TRAIN\n",
        "        # ==========================================\n",
        "\n",
        "        print(\"\\n1Ô∏è‚É£ Preparando datos de entrenamiento...\")\n",
        "\n",
        "        # Copiar datos y resetear √≠ndices\n",
        "        if isinstance(X_train_final, pd.DataFrame):\n",
        "            train_data_ev = X_train_final.copy().reset_index(drop=True)\n",
        "        else:\n",
        "            train_data_ev = pd.DataFrame(X_train_final, columns=feature_columns).reset_index(drop=True)\n",
        "\n",
        "        # Agregar target\n",
        "        if isinstance(y_train_final, pd.Series):\n",
        "            train_data_ev['target'] = y_train_final.reset_index(drop=True).values\n",
        "        else:\n",
        "            train_data_ev['target'] = y_train_final\n",
        "\n",
        "        # Generar predicciones\n",
        "        train_data_ev['prediction'] = best_model_obj.predict(X_train_final)\n",
        "\n",
        "        # Generar probabilidades (score para clase positiva)\n",
        "        train_probas = best_model_obj.predict_proba(X_train_final)\n",
        "        train_data_ev['score'] = train_probas[:, 1]  # Probabilidad de clase 1 (Disease)\n",
        "\n",
        "        print(f\"   ‚úÖ Train data preparado: {train_data_ev.shape}\")\n",
        "        print(f\"      ‚Ä¢ Target: {train_data_ev['target'].nunique()} clases\")\n",
        "        print(f\"      ‚Ä¢ Predictions: {train_data_ev['prediction'].nunique()} valores √∫nicos\")\n",
        "        print(f\"      ‚Ä¢ Score range: [{train_data_ev['score'].min():.3f}, {train_data_ev['score'].max():.3f}]\")\n",
        "\n",
        "        # ==========================================\n",
        "        # PREPARAR DATOS - TEST\n",
        "        # ==========================================\n",
        "\n",
        "        print(\"\\n2Ô∏è‚É£ Preparando datos de test...\")\n",
        "\n",
        "        # Copiar datos y resetear √≠ndices\n",
        "        if isinstance(X_test_final, pd.DataFrame):\n",
        "            test_data_ev = X_test_final.copy().reset_index(drop=True)\n",
        "        else:\n",
        "            test_data_ev = pd.DataFrame(X_test_final, columns=feature_columns).reset_index(drop=True)\n",
        "\n",
        "        # Agregar target\n",
        "        if isinstance(y_test_final, pd.Series):\n",
        "            test_data_ev['target'] = y_test_final.reset_index(drop=True).values\n",
        "        else:\n",
        "            test_data_ev['target'] = y_test_final\n",
        "\n",
        "        # Generar predicciones\n",
        "        test_data_ev['prediction'] = best_model_obj.predict(X_test_final)\n",
        "\n",
        "        # Generar probabilidades (score para clase positiva)\n",
        "        test_probas = best_model_obj.predict_proba(X_test_final)\n",
        "        test_data_ev['score'] = test_probas[:, 1]  # Probabilidad de clase 1 (Disease)\n",
        "\n",
        "        print(f\"   ‚úÖ Test data preparado: {test_data_ev.shape}\")\n",
        "        print(f\"      ‚Ä¢ Target: {test_data_ev['target'].nunique()} clases\")\n",
        "        print(f\"      ‚Ä¢ Predictions: {test_data_ev['prediction'].nunique()} valores √∫nicos\")\n",
        "        print(f\"      ‚Ä¢ Score range: [{test_data_ev['score'].min():.3f}, {test_data_ev['score'].max():.3f}]\")\n",
        "\n",
        "        # ==========================================\n",
        "        # CREAR DATA DEFINITION\n",
        "        # ==========================================\n",
        "\n",
        "        print(\"\\n3Ô∏è‚É£ Creando Data Definition para clasificaci√≥n binaria...\")\n",
        "\n",
        "        from evidently import Dataset, DataDefinition, BinaryClassification\n",
        "        from evidently import Report\n",
        "        from evidently.presets import ClassificationPreset, DataSummaryPreset, DataDriftPreset\n",
        "\n",
        "        # Definir estructura de datos para clasificaci√≥n binaria\n",
        "        data_definition = DataDefinition(\n",
        "            classification=[\n",
        "                BinaryClassification(\n",
        "                    target=\"target\",                    # Columna con valores reales\n",
        "                    prediction_labels=\"prediction\",     # Columna con predicciones\n",
        "                    prediction_probas=\"score\",          # Columna con probabilidades\n",
        "                    pos_label=1                         # Clase positiva (Disease)\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        print(\"   ‚úÖ Data Definition creado\")\n",
        "        print(\"      ‚Ä¢ Target column: 'target'\")\n",
        "        print(\"      ‚Ä¢ Prediction column: 'prediction'\")\n",
        "        print(\"      ‚Ä¢ Probability column: 'score'\")\n",
        "        print(\"      ‚Ä¢ Positive label: 1 (Disease)\")\n",
        "\n",
        "        # ==========================================\n",
        "        # CREAR DATASETS\n",
        "        # ==========================================\n",
        "\n",
        "        print(\"\\n4Ô∏è‚É£ Creando Evidently Datasets...\")\n",
        "\n",
        "        # Crear Dataset para train (reference)\n",
        "        train_dataset = Dataset.from_pandas(\n",
        "            train_data_ev,\n",
        "            data_definition=data_definition\n",
        "        )\n",
        "\n",
        "        # Crear Dataset para test (current)\n",
        "        test_dataset = Dataset.from_pandas(\n",
        "            test_data_ev,\n",
        "            data_definition=data_definition\n",
        "        )\n",
        "\n",
        "        print(\"   ‚úÖ Datasets creados:\")\n",
        "        print(\"      ‚Ä¢ Reference (train): Dataset con definici√≥n de clasificaci√≥n\")\n",
        "        print(\"      ‚Ä¢ Current (test): Dataset con definici√≥n de clasificaci√≥n\")\n",
        "\n",
        "        # ==========================================\n",
        "        # CREAR Y EJECUTAR REPORTE\n",
        "        # ==========================================\n",
        "\n",
        "        print(\"\\n5Ô∏è‚É£ Generando reporte completo con Evidently AI...\")\n",
        "        print(\"   ‚è±Ô∏è Esto puede tomar 30-60 segundos...\")\n",
        "\n",
        "        # Crear reporte con todos los presets\n",
        "        report = Report([\n",
        "            ClassificationPreset(),    # M√©tricas de clasificaci√≥n\n",
        "            DataSummaryPreset(),       # Resumen de datos\n",
        "            DataDriftPreset()          # Detecci√≥n de drift\n",
        "        ])\n",
        "\n",
        "        # Ejecutar reporte\n",
        "        my_report = report.run(\n",
        "            reference_data=train_dataset,\n",
        "            current_data=test_dataset\n",
        "        )\n",
        "\n",
        "        print(\"   ‚úÖ Reporte generado exitosamente\")\n",
        "\n",
        "        # ==========================================\n",
        "        # GUARDAR REPORTE HTML\n",
        "        # ==========================================\n",
        "\n",
        "        print(\"\\n6Ô∏è‚É£ Guardando reporte HTML...\")\n",
        "\n",
        "        output_file = 'evidently_report.html'\n",
        "        my_report.save_html(output_file)\n",
        "\n",
        "        print(f\"   ‚úÖ Reporte guardado: {output_file}\")\n",
        "\n",
        "        # ==========================================\n",
        "        # MOSTRAR EN NOTEBOOK (OPCIONAL)\n",
        "        # ==========================================\n",
        "\n",
        "        try:\n",
        "            print(\"\\n7Ô∏è‚É£ Mostrando reporte en notebook...\")\n",
        "            my_report.show()\n",
        "        except Exception as show_error:\n",
        "            print(f\"   ‚ö†Ô∏è No se pudo mostrar en notebook: {type(show_error).__name__}\")\n",
        "            print(\"   üí° Descarga el archivo HTML para visualizarlo\")\n",
        "\n",
        "        # ==========================================\n",
        "        # RESUMEN DE CONTENIDOS\n",
        "        # ==========================================\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"‚úÖ REPORTE EVIDENTLY GENERADO EXITOSAMENTE!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"\\nüìÑ Archivo: {output_file}\")\n",
        "        print(f\"üéØ Modelo analizado: {best_model_name}\")\n",
        "\n",
        "        print(\"\\nüìä CONTENIDO DEL REPORTE:\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "        print(\"\\nüéØ CLASSIFICATION PRESET:\")\n",
        "        print(\"   ‚úÖ ROC-AUC Score y curva completa\")\n",
        "        print(\"   ‚úÖ Curva Precision-Recall\")\n",
        "        print(\"   ‚úÖ Distribuci√≥n de probabilidades (score)\")\n",
        "        print(\"   ‚úÖ Calibration curve (qu√© tan calibrado est√° el modelo)\")\n",
        "        print(\"   ‚úÖ An√°lisis de umbrales √≥ptimos\")\n",
        "        print(\"   ‚úÖ Matriz de confusi√≥n detallada\")\n",
        "        print(\"   ‚úÖ M√©tricas por clase (0: No Disease, 1: Disease)\")\n",
        "        print(\"   ‚úÖ Classification quality metrics\")\n",
        "        print(\"   ‚úÖ Class separation analysis\")\n",
        "\n",
        "        print(\"\\nüìà DATA SUMMARY PRESET:\")\n",
        "        print(\"   ‚úÖ Estad√≠sticas descriptivas (train vs test)\")\n",
        "        print(\"   ‚úÖ Distribuci√≥n de features\")\n",
        "        print(\"   ‚úÖ Missing values analysis\")\n",
        "        print(\"   ‚úÖ Correlaciones entre features\")\n",
        "        print(\"   ‚úÖ Target distribution\")\n",
        "\n",
        "        print(\"\\nüîç DATA DRIFT PRESET:\")\n",
        "        print(\"   ‚úÖ Feature drift detection (cambios en distribuciones)\")\n",
        "        print(\"   ‚úÖ Drift score por feature\")\n",
        "        print(\"   ‚úÖ Visualizaci√≥n de distribuciones (train vs test)\")\n",
        "        print(\"   ‚úÖ Target drift analysis\")\n",
        "        print(\"   ‚úÖ Prediction drift\")\n",
        "\n",
        "        print(\"\\nüí° C√ìMO USAR EL REPORTE:\")\n",
        "        print(\"-\"*80)\n",
        "        print(\"   1. Descarga el archivo HTML:\")\n",
        "        print(\"      ‚Ä¢ Click en üìÅ (Files) en sidebar izquierdo\")\n",
        "        print(\"      ‚Ä¢ Busca 'evidently_report.html'\")\n",
        "        print(\"      ‚Ä¢ Click derecho ‚Üí Download\")\n",
        "        print()\n",
        "        print(\"   2. Abre en tu navegador:\")\n",
        "        print(\"      ‚Ä¢ Doble click en el archivo descargado\")\n",
        "        print(\"      ‚Ä¢ El reporte es interactivo y completo\")\n",
        "        print(\"      ‚Ä¢ No requiere Python instalado\")\n",
        "        print()\n",
        "        print(\"   3. Navega por las secciones:\")\n",
        "        print(\"      ‚Ä¢ Classification: Analiza performance del modelo\")\n",
        "        print(\"      ‚Ä¢ Data Summary: Explora estad√≠sticas de datos\")\n",
        "        print(\"      ‚Ä¢ Data Drift: Identifica cambios significativos\")\n",
        "\n",
        "        print(\"\\nüéØ QU√â BUSCAR EN EL REPORTE:\")\n",
        "        print(\"-\"*80)\n",
        "        print(\"   üìä ROC-AUC: ¬øEst√° cerca de 1.0? (mejor performance)\")\n",
        "        print(\"   üìä Precision-Recall: Balance seg√∫n objetivos del negocio\")\n",
        "        print(\"   üìä Calibration: ¬øLas probabilidades est√°n bien calibradas?\")\n",
        "        print(\"   üìä Data Drift: ¬øHay features con drift significativo?\")\n",
        "        print(\"   üìä Performance: ¬øTrain y Test similares? (no overfitting)\")\n",
        "\n",
        "        print(\"\\nüîî ALERTAS A REVISAR:\")\n",
        "        print(\"-\"*80)\n",
        "        print(\"   ‚ö†Ô∏è Drift score > 0.5 en features cr√≠ticas\")\n",
        "        print(\"   ‚ö†Ô∏è Gran diferencia entre m√©tricas train vs test\")\n",
        "        print(\"   ‚ö†Ô∏è Calibration curve alejada de la diagonal\")\n",
        "        print(\"   ‚ö†Ô∏è Cambios en distribuci√≥n del target\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"‚ö†Ô∏è ERROR EN EVIDENTLY\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"   Tipo: {type(e).__name__}\")\n",
        "        print(f\"   Detalles: {str(e)[:500]}\")\n",
        "\n",
        "        print(\"\\nüí° POSIBLES CAUSAS:\")\n",
        "        print(\"   ‚Ä¢ Incompatibilidad de versi√≥n de Evidently\")\n",
        "        print(\"   ‚Ä¢ Problemas con estructura de datos\")\n",
        "        print(\"   ‚Ä¢ Conflictos de dependencias\")\n",
        "\n",
        "        print(\"\\nüí° ALTERNATIVAS DISPONIBLES:\")\n",
        "        print(\"   ‚úÖ Yellowbrick: Confusion Matrix, ROC, PR curves (ya ejecutado)\")\n",
        "        print(\"   ‚úÖ MLflow: Todas las m√©tricas trackeadas (disponible)\")\n",
        "        print(\"   ‚úÖ SHAP: Interpretabilidad completa (ya ejecutado)\")\n",
        "        print(\"   ‚úÖ Deepchecks: Validaci√≥n del modelo (ya ejecutado)\")\n",
        "\n",
        "        print(\"\\n   El laboratorio est√° completo sin Evidently.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚è≠Ô∏è EVIDENTLY OMITIDO - No hay modelo disponible\")\n",
        "    print(\"   Por favor entrena al menos un modelo antes de ejecutar esta fase\")\n",
        "\n",
        "# ==========================================\n",
        "# CONCLUSI√ìN DEL LABORATORIO\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ FASE 15 COMPLETADA\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "my_report"
      ],
      "metadata": {
        "id": "RZF2a6IqyZKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîç FASE 15: Validaci√≥n Avanzada con Deepchecks\n",
        "\n",
        "## ¬øQu√© es Deepchecks?\n",
        "\n",
        "**Deepchecks** es una librer√≠a profesional de Python para **validaci√≥n y testing de modelos de Machine Learning**. Es utilizada por equipos de ML en empresas como Microsoft, Intel y startups de MLOps.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ ¬øPor qu√© es Cr√≠tico en Producci√≥n?\n",
        "\n",
        "En el mundo real, los modelos pueden fallar silenciosamente por:\n",
        "- ‚úÖ **Data Integrity Issues**: Valores faltantes, duplicados, data leakage\n",
        "- ‚úÖ **Distribution Shifts**: Train vs Test diferentes\n",
        "- ‚úÖ **Performance Issues**: Overfitting, underfitting, bias por clase\n",
        "- ‚úÖ **Model Degradation**: Rendimiento deterior√°ndose con el tiempo\n",
        "\n",
        "**Deepchecks detecta estos problemas ANTES de deployar a producci√≥n.**\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Tipos de Checks\n",
        "\n",
        "### 1. **Data Integrity Checks**\n",
        "- Mixed data types\n",
        "- String mismatch\n",
        "- Data duplicates\n",
        "- Feature-label correlation\n",
        "- Conflicting labels\n",
        "\n",
        "### 2. **Train-Test Validation**\n",
        "- Feature drift\n",
        "- Label drift\n",
        "- Train-test feature drift\n",
        "- Dataset size comparison\n",
        "\n",
        "### 3. **Model Performance**\n",
        "- Confusion matrix analysis\n",
        "- Performance report\n",
        "- Class performance\n",
        "- Segment performance\n",
        "- Simple model comparison\n",
        "\n",
        "### 4. **Model Robustness**\n",
        "- Weak segments performance\n",
        "- Performance bias\n",
        "- Feature importance\n",
        "- Unused features\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Valor Educativo\n",
        "\n",
        "**Para Data Scientists profesionales:**\n",
        "- ‚úÖ Simula revisi√≥n de calidad pre-producci√≥n\n",
        "- ‚úÖ Genera reportes autom√°ticos para stakeholders\n",
        "- ‚úÖ Detecta problemas que an√°lisis manual puede omitir\n",
        "- ‚úÖ Est√°ndar en MLOps pipelines\n",
        "\n",
        "**En este laboratorio:**\n",
        "- Validaremos el **mejor modelo** identificado (Random Forest / XGBoost)\n",
        "- Generaremos **3 reportes completos**:\n",
        "  1. **Data Integrity Suite**: Validaci√≥n de datos\n",
        "  2. **Train-Test Validation Suite**: Comparaci√≥n train vs test\n",
        "  3. **Full Suite**: An√°lisis completo del modelo\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Nota T√©cnica\n",
        "\n",
        "Deepchecks tiene incompatibilidad con NumPy 2.0. Hemos implementado un **workaround** que permite su uso en Google Colab actual.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Resultado Esperado\n",
        "\n",
        "Al final de esta fase tendr√°s:\n",
        "- ‚úÖ 3 reportes HTML interactivos\n",
        "- ‚úÖ Validaci√≥n completa de calidad de datos\n",
        "- ‚úÖ An√°lisis de performance por segmentos\n",
        "- ‚úÖ Detecci√≥n autom√°tica de problemas potenciales\n",
        "- ‚úÖ Recomendaciones accionables"
      ],
      "metadata": {
        "id": "HTMUG9C35ksQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from deepchecks.tabular import Dataset\n",
        "from deepchecks.tabular.suites import full_suite\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"üîç FASE 14: DEEPCHECKS - VALIDACI√ìN EXHAUSTIVA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# PREPARACI√ìN DE DATASETS CON METADATA\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüìä Preparando datasets con metadata completa...\")\n",
        "\n",
        "# Identificar features categ√≥ricas\n",
        "# (En UCI Heart Disease, estas son categ√≥ricas aunque est√©n como n√∫meros)\n",
        "categorical_features = [\n",
        "    'sex',      # 0=female, 1=male\n",
        "    'cp',       # Chest pain type (0-3)\n",
        "    'fbs',      # Fasting blood sugar (0/1)\n",
        "    'restecg',  # Resting ECG (0-2)\n",
        "    'exang',    # Exercise induced angina (0/1)\n",
        "    'slope',    # Slope of peak exercise ST (0-2)\n",
        "    'ca',       # Number of major vessels (0-3)\n",
        "    'thal'      # Thalassemia (0-3)\n",
        "]\n",
        "\n",
        "print(f\"   Features categ√≥ricas identificadas: {len(categorical_features)}\")\n",
        "for feat in categorical_features:\n",
        "    print(f\"      ‚Ä¢ {feat}\")\n",
        "\n",
        "# Crear Deepchecks Datasets\n",
        "# Nota: Usamos DataFrames originales antes de scaling para interpretabilidad\n",
        "train_ds = Dataset(\n",
        "    pd.DataFrame(X_train_scaled, columns=X.columns, index=y_train.index), # Fix: Align index\n",
        "    label=pd.Series(y_train, name='target'),\n",
        "    cat_features=categorical_features,\n",
        "    features=X.columns.tolist()\n",
        ")\n",
        "\n",
        "test_ds = Dataset(\n",
        "    pd.DataFrame(X_test_scaled, columns=X.columns, index=y_test.index),   # Fix: Align index\n",
        "    label=pd.Series(y_test, name='target'),\n",
        "    cat_features=categorical_features,\n",
        "    features=X.columns.tolist()\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Datasets preparados con metadata completa\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# CONFIGURAR Y EJECUTAR FULL SUITE\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ EJECUTANDO DEEPCHECKS FULL SUITE\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nEsta suite incluye 40+ checks organizados en:\")\n",
        "print(\"   1. Data Integrity (integridad de datos)\")\n",
        "print(\"   2. Train-Test Validation (comparaci√≥n distribuciones)\")\n",
        "print(\"   3. Model Evaluation (performance del modelo)\")\n",
        "print(\"\\n‚è±Ô∏è  Tiempo estimado: 30-60 segundos\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Crear suite\n",
        "suite = full_suite()\n",
        "\n",
        "# Ejecutar validaci√≥n\n",
        "try:\n",
        "    result = suite.run(\n",
        "        train_dataset=train_ds,\n",
        "        test_dataset=test_ds,\n",
        "        model=best_model\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚úÖ Full Suite ejecutada exitosamente\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è  Error en ejecuci√≥n: {str(e)}\")\n",
        "    print(\"   Intentando con suite simplificada...\")\n",
        "\n",
        "    # Fallback: Suite m√°s simple\n",
        "    from deepchecks.tabular.suites import model_evaluation\n",
        "    suite = model_evaluation()\n",
        "    result = suite.run(\n",
        "        train_dataset=train_ds,\n",
        "        test_dataset=test_ds,\n",
        "        model=best_model\n",
        "    )\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# AN√ÅLISIS DE RESULTADOS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä RESUMEN DE VALIDACI√ìN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Extraer checks por prioridad\n",
        "checks_by_priority = {\n",
        "    'Critical (P1)': [],\n",
        "    'High (P2)': [],\n",
        "    'Medium (P3)': [],\n",
        "    'Low (P4)': [],\n",
        "    'Passed': []\n",
        "}\n",
        "\n",
        "for check_result in result.results:\n",
        "    # Obtener prioridad del check\n",
        "    if hasattr(check_result, 'priority'):\n",
        "        priority = check_result.priority\n",
        "\n",
        "        # Clasificar seg√∫n prioridad\n",
        "        if priority == 1:\n",
        "            checks_by_priority['Critical (P1)'].append(check_result.header)\n",
        "        elif priority == 2:\n",
        "            checks_by_priority['High (P2)'].append(check_result.header)\n",
        "        elif priority == 3:\n",
        "            checks_by_priority['Medium (P3)'].append(check_result.header)\n",
        "        elif priority == 4:\n",
        "            checks_by_priority['Low (P4)'].append(check_result.header)\n",
        "    else:\n",
        "        # Si no tiene prioridad, asumimos que pas√≥\n",
        "        checks_by_priority['Passed'].append(check_result.header)\n",
        "\n",
        "# Mostrar resumen\n",
        "total_checks = len(result.results)\n",
        "critical_issues = len(checks_by_priority['Critical (P1)'])\n",
        "high_issues = len(checks_by_priority['High (P2)'])\n",
        "medium_issues = len(checks_by_priority['Medium (P3)'])\n",
        "low_issues = len(checks_by_priority['Low (P4)'])\n",
        "passed = len(checks_by_priority['Passed'])\n",
        "\n",
        "print(f\"\\nüìã Total de checks ejecutados: {total_checks}\")\n",
        "print(f\"\\n   ‚úÖ Pasados:          {passed}\")\n",
        "print(f\"   üî¥ Cr√≠ticos (P1):    {critical_issues}\")\n",
        "print(f\"   üü† Altos (P2):       {high_issues}\")\n",
        "print(f\"   üü° Medios (P3):      {medium_issues}\")\n",
        "print(f\"   ‚ö™ Bajos (P4):       {low_issues}\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# DETALLES DE CHECKS CR√çTICOS Y ALTOS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "if critical_issues > 0 or high_issues > 0:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚ö†Ô∏è  ATENCI√ìN: ISSUES QUE REQUIEREN REVISI√ìN\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if critical_issues > 0:\n",
        "        print(\"\\nüî¥ CR√çTICOS (Prioridad 1) - DEBEN ARREGLARSE:\")\n",
        "        for i, check_name in enumerate(checks_by_priority['Critical (P1)'], 1):\n",
        "            print(f\"   {i}. {check_name}\")\n",
        "\n",
        "    if high_issues > 0:\n",
        "        print(\"\\nüü† ALTOS (Prioridad 2) - DEBER√çAN ARREGLARSE:\")\n",
        "        for i, check_name in enumerate(checks_by_priority['High (P2)'], 1):\n",
        "            print(f\"   {i}. {check_name}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üéâ ¬°EXCELENTE! NO HAY ISSUES CR√çTICOS NI ALTOS\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nEl modelo pas√≥ todas las validaciones importantes.\")\n",
        "    print(\"Puede proceder a deployment con confianza.\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# GUARDAR REPORTE HTML\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüìÑ Guardando reporte HTML completo...\")\n",
        "\n",
        "output_file = 'deepchecks_full_validation_report.html'\n",
        "result.save_as_html(output_file)\n",
        "\n",
        "print(f\"‚úÖ Reporte guardado: {output_file}\")\n",
        "print(f\"\\n   Abrir el archivo HTML para ver:\")\n",
        "print(f\"   ‚Ä¢ Gr√°ficos interactivos de cada check\")\n",
        "print(f\"   ‚Ä¢ Explicaciones detalladas de cada issue\")\n",
        "print(f\"   ‚Ä¢ Recomendaciones de soluci√≥n\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# AN√ÅLISIS DE CHECKS ESPEC√çFICOS IMPORTANTES\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîç AN√ÅLISIS DE CHECKS CLAVE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Buscar checks espec√≠ficos importantes\n",
        "key_checks = {\n",
        "    'Train Test Feature Drift': 'Compara distribuciones entre train y test',\n",
        "    'Model Error Analysis': 'Analiza patrones en errores del modelo',\n",
        "    'Confusion Matrix Report': 'Matriz de confusi√≥n detallada',\n",
        "    'Feature Label Correlation': 'Correlaci√≥n features con target'\n",
        "}\n",
        "\n",
        "for check_name, description in key_checks.items():\n",
        "    found = False\n",
        "    for check_result in result.results:\n",
        "        if check_name.lower() in check_result.header.lower():\n",
        "            print(f\"\\nüìä {check_name}\")\n",
        "            print(f\"   {description}\")\n",
        "\n",
        "            # Extraer valor si est√° disponible\n",
        "            if hasattr(check_result, 'value'):\n",
        "                print(f\"   Resultado: {check_result.value}\")\n",
        "\n",
        "            found = True\n",
        "            break\n",
        "\n",
        "    if not found:\n",
        "        print(f\"\\n‚ö†Ô∏è  {check_name}: No encontrado en suite\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# RECOMENDACIONES BASADAS EN RESULTADOS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üí° RECOMENDACIONES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if critical_issues == 0 and high_issues == 0:\n",
        "    print(\"\"\"\n",
        "‚úÖ ESTADO: LISTO PARA DEPLOYMENT\n",
        "\n",
        "El modelo ha pasado todas las validaciones cr√≠ticas.\n",
        "\n",
        "PR√ìXIMOS PASOS:\n",
        "1. Revisar issues de prioridad media si los hay\n",
        "2. Documentar decisi√≥n de deployment\n",
        "3. Configurar monitoreo en producci√≥n\n",
        "4. Establecer thresholds de alerta\n",
        "    \"\"\")\n",
        "\n",
        "elif critical_issues > 0:\n",
        "    print(\"\"\"\n",
        "üî¥ ESTADO: NO LISTO PARA DEPLOYMENT\n",
        "\n",
        "Se detectaron issues CR√çTICOS que deben resolverse.\n",
        "\n",
        "ACCIONES INMEDIATAS:\n",
        "1. Revisar cada issue cr√≠tico en el reporte HTML\n",
        "2. Corregir problemas identificados\n",
        "3. Re-ejecutar validaci√≥n\n",
        "4. No proceder a deployment hasta resolver\n",
        "\n",
        "ISSUES COMUNES Y SOLUCIONES:\n",
        "- Data Leakage: Revisar feature engineering\n",
        "- Label Ambiguity: Validar calidad de labels\n",
        "- Feature Drift: Verificar distribuciones train/test\n",
        "- Overfitting: Aplicar m√°s regularizaci√≥n\n",
        "    \"\"\")\n",
        "\n",
        "else:  # high_issues > 0\n",
        "    print(\"\"\"\n",
        "üü† ESTADO: DEPLOYMENT CONDICIONAL\n",
        "\n",
        "Se detectaron issues de ALTA prioridad.\n",
        "\n",
        "RECOMENDACIONES:\n",
        "1. Revisar issues en detalle (ver reporte HTML)\n",
        "2. Evaluar impacto en producci√≥n\n",
        "3. Si es posible, corregir antes de deploy\n",
        "4. Si se deploya: monitoreo cercano\n",
        "\n",
        "CONSIDERACIONES:\n",
        "- Documentar issues conocidos\n",
        "- Establecer plan de mejora continua\n",
        "- Monitorear m√©tricas afectadas\n",
        "    \"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ FASE 14 COMPLETADA - Deepchecks Full Suite\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# INTERPRETACI√ìN PEDAG√ìGICA\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "print(\"\\nüí° INTERPRETACI√ìN PEDAG√ìGICA:\")\n",
        "print(\"\"\"\n",
        "1. IMPORTANCIA DE DEEPCHECKS:\n",
        "   - MLflow rastrea experimentos\n",
        "   - Deepchecks valida calidad del modelo\n",
        "   - Ambos son complementarios y necesarios\n",
        "\n",
        "2. PRIORIDADES:\n",
        "   - P1 (Critical): Bloquean deployment\n",
        "   - P2 (High): Requieren atenci√≥n\n",
        "   - P3-P4: Mejoras incrementales\n",
        "\n",
        "3. VALIDACI√ìN vs TESTING:\n",
        "   - Testing: ¬øEl modelo funciona?\n",
        "   - Validaci√≥n: ¬øEl modelo es confiable?\n",
        "   - Deepchecks responde la segunda pregunta\n",
        "\n",
        "4. EN LA INDUSTRIA:\n",
        "   - Empresas FAANG usan validaci√≥n exhaustiva\n",
        "   - Healthcare/Finance: Regulaci√≥n requiere validaci√≥n\n",
        "   - Startups: A menudo omiten esto (error costoso)\n",
        "\n",
        "5. CHECKS M√ÅS IMPORTANTES PARA MEDICINA:\n",
        "   - Feature Drift: Datos cambiaron desde train?\n",
        "   - Confusion Matrix: Balance FP vs FN\n",
        "   - Calibration: Probabilidades confiables?\n",
        "   - Error Analysis: Sesgos en subgrupos?\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "C13CCbMu5nZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_29"
      },
      "source": [
        "# ‚úÖ FASE 16: Resumen Ejecutivo\n",
        "\n",
        "üéì Resumen Ejecutivo - Medical Diagnosis Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_30"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print(\"üìÑ GENERANDO RESUMEN EJECUTIVO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear resumen en Markdown\n",
        "summary = f\"\"\"# üéì Resumen Ejecutivo - Medical Diagnosis Classification\n",
        "\n",
        "## üìä Resultados del Laboratorio\n",
        "\n",
        "**Dataset:** UCI Heart Disease (303 pacientes, 13 features)\n",
        "**Fecha:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "### üèÜ Mejor Modelo\n",
        "\n",
        "**Modelo Seleccionado:** {best_model_name}\n",
        "\n",
        "| M√©trica | Valor |\n",
        "|---------|-------|\n",
        "| Accuracy | {best_acc:.4f} ({best_acc*100:.2f}%) |\n",
        "| F1-Score | {best_f1:.4f} ({best_f1*100:.2f}%) |\n",
        "| ROC-AUC | {best_auc:.4f} ({best_auc*100:.2f}%) |\n",
        "\n",
        "### üìà Comparaci√≥n de Modelos\n",
        "\n",
        "{comparison.to_markdown(index=False)}\n",
        "\n",
        "### ‚úÖ Herramientas Utilizadas\n",
        "\n",
        "- ‚úÖ MLflow - Experiment tracking\n",
        "- ‚úÖ Optuna - HPO (60 trials total)\n",
        "- ‚úÖ Yellowbrick - Visualizaciones\n",
        "- ‚úÖ SHAP - Interpretabilidad\n",
        "- ‚úÖ Evidently AI - Reportes\n",
        "- ‚úÖ Deepchecks - An√°lisis a profundidad del modelo\n",
        "\n",
        "### üí° Recomendaciones\n",
        "\n",
        "1. **Deployment:** Usar modelo '{best_model_name}' en producci√≥n\n",
        "2. **Monitoreo:** Implementar tracking de performance en tiempo real\n",
        "3. **Reentrenamiento:** Evaluar cada 3-6 meses o cuando F1 < 0.85\n",
        "\n",
        "### üîç An√°lisis Adicionales Disponibles\n",
        "\n",
        "- üìä MLflow UI: Comparaci√≥n interactiva de experimentos\n",
        "- üìà Evidently Report: An√°lisis exhaustivo de clasificaci√≥n\n",
        "- üéØ SHAP Values: Interpretaci√≥n de predicciones individuales\n",
        "\n",
        "---\n",
        "\n",
        "**Curso:** Miner√≠a de Datos (IEI-097)\n",
        "**Docente:** Eduardo Far√≠as Reyes\n",
        "**Instituci√≥n:** Instituto Profesional Santo Tom√°s\n",
        "\"\"\"\n",
        "\n",
        "# Guardar resumen\n",
        "with open('resumen_ejecutivo.md', 'w', encoding='utf-8') as f:\n",
        "    f.write(summary)\n",
        "\n",
        "print(\"\\n‚úÖ Resumen ejecutivo guardado: resumen_ejecutivo.md\")\n",
        "print(\"\\n\" + summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_31"
      },
      "source": [
        "# üéì CONCLUSIONES Y LOGROS\n",
        "\n",
        "## ‚úÖ Lo que hemos completado:\n",
        "\n",
        "1. **EDA exhaustivo** - An√°lisis completo de datos m√©dicos\n",
        "2. **Preprocesamiento riguroso** - Sin data leakage, con estratificaci√≥n\n",
        "3. **5 modelos entrenados**:\n",
        "   - Logistic Regression (baseline)\n",
        "   - Random Forest (optimizado con Optuna)\n",
        "   - XGBoost (optimizado con Optuna)\n",
        "   - Neural Network (Deep Learning)\n",
        "   - Ensemble (Voting)\n",
        "4. **Optimizaci√≥n autom√°tica** - 60 trials Optuna total\n",
        "5. **Experiment tracking** - MLflow con UI v√≠a ngrok\n",
        "6. **Visualizaciones profesionales** - Yellowbrick\n",
        "7. **Interpretabilidad** - SHAP para explicar predicciones\n",
        "8. **Reportes autom√°ticos** - Evidently AI\n",
        "9. **Validaci√≥n exhaustiva** - Deepchecks (15+ checks)\n",
        "\n",
        "## üèÜ Mejor Modelo\n",
        "[Determinado en FASE 12]\n",
        "\n",
        "## üíº Aplicabilidad Profesional\n",
        "Este stack se usa en:\n",
        "- ‚úÖ Startups de ML/AI\n",
        "- ‚úÖ FAANG (Google, Meta, Amazon, etc.)\n",
        "- ‚úÖ Healthcare & Finance (reguladas)\n",
        "- ‚úÖ Consultor√≠as de Data Science\n",
        "\n",
        "## üöÄ Pr√≥ximos Pasos\n",
        "1. Experimentar con otros datasets\n",
        "2. Probar m√°s modelos (LightGBM, CatBoost)\n",
        "3. Feature engineering avanzado\n",
        "4. Deployment con FastAPI + Docker\n",
        "5. Monitoreo en producci√≥n\n",
        "\n",
        "## üìö Recursos\n",
        "- MLflow: https://mlflow.org\n",
        "- Optuna: https://optuna.readthedocs.io\n",
        "- SHAP: https://shap.readthedocs.io\n",
        "- Evidently: https://docs.evidentlyai.com\n",
        "- Deepchecks: https://docs.deepchecks.com\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Felicitaciones! üéâ**\n",
        "\n",
        "Has completado un laboratorio de nivel profesional que te prepara para roles de ML Engineer y Data Scientist en la industria.\n",
        "\n",
        "---\n",
        "\n",
        "**Docente:** Eduardo Far√≠as Reyes  \n",
        "**Instituci√≥n:** Instituto Profesional Santo Tom√°s  \n",
        "**Curso:** Miner√≠a de Datos (IEI-097)"
      ]
    }
  ]
}